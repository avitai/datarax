{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# IMDB Sentiment Analysis Quick Reference\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Beginner |\n",
    "| **Runtime** | ~5 min |\n",
    "| **Prerequisites** | Basic Datarax pipeline, HuggingFace datasets |\n",
    "| **Format** | Python + Jupyter |\n",
    "| **Memory** | ~500 MB RAM |\n",
    "\n",
    "## Overview\n",
    "\n",
    "This quick reference demonstrates loading the IMDB movie review dataset from\n",
    "HuggingFace Hub for sentiment analysis. You'll learn to handle text data\n",
    "in Datarax pipelines, which differs from image data handling.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this example, you will be able to:\n",
    "\n",
    "1. Load IMDB dataset using `HFEagerSource` with streaming\n",
    "2. Handle text data in Datarax pipelines\n",
    "3. Apply text preprocessing transformations\n",
    "4. Understand differences between text and image pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "# Install datarax with HuggingFace support\n",
    "uv pip install \"datarax[data]\"\n",
    "```\n",
    "\n",
    "**Note**: First run may download dataset files from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "from datarax import from_source\n",
    "from datarax.dag.nodes import OperatorNode\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.sources import HFEagerConfig, HFEagerSource\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## IMDB Dataset Overview\n",
    "\n",
    "The IMDB dataset contains 50,000 movie reviews labeled for sentiment analysis:\n",
    "\n",
    "| Split | Samples | Labels |\n",
    "|-------|---------|--------|\n",
    "| train | 25,000 | 0 (negative), 1 (positive) |\n",
    "| test | 25,000 | 0 (negative), 1 (positive) |\n",
    "\n",
    "Each sample contains:\n",
    "- `text`: The movie review text (string)\n",
    "- `label`: Sentiment label (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB in streaming mode\n",
    "config = HFEagerConfig(\n",
    "    name=\"stanfordnlp/imdb\",  # Use full dataset path for reliability\n",
    "    split=\"train\",\n",
    "    streaming=True,  # Stream to avoid downloading 84MB\n",
    "    download_kwargs={\"trust_remote_code\": True},\n",
    ")\n",
    "\n",
    "source = HFEagerSource(config, rngs=nnx.Rngs(0))\n",
    "print(f\"Loaded HuggingFace dataset: {config.name}\")\n",
    "print(f\"Split: {config.split}\")\n",
    "print(\"Mode: Streaming (no full download)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Inspect Data Structure\n",
    "\n",
    "Unlike image datasets, IMDB returns text strings. Let's examine the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text datasets, iterate element-by-element since strings can't be\n",
    "# batched as JAX arrays (text needs tokenization first for batching)\n",
    "print(\"Sample reviews from IMDB:\")\n",
    "\n",
    "for i, element in enumerate(source):\n",
    "    if i >= 3:\n",
    "        break\n",
    "\n",
    "    print(f\"\\nExample {i + 1}:\")\n",
    "    print(f\"  Keys: {list(element.keys())}\")\n",
    "\n",
    "    # Show label\n",
    "    label = element.get(\"label\")\n",
    "    sentiment = \"positive\" if label == 1 else \"negative\"\n",
    "    print(f\"  Label: {label} ({sentiment})\")\n",
    "\n",
    "    # Show text preview\n",
    "    text = element.get(\"text\", \"\")\n",
    "    if isinstance(text, (list, tuple)):\n",
    "        text = text[0] if text else \"\"\n",
    "    text_preview = str(text)[:100] + \"...\" if len(str(text)) > 100 else str(text)\n",
    "    print(f\"  Text preview: {text_preview}\")\n",
    "\n",
    "# Reset source for further use\n",
    "source.reset()\n",
    "\n",
    "# Expected output:\n",
    "# Example 1:\n",
    "#   Keys: ['text', 'label']\n",
    "#   Label: 0 (negative)\n",
    "#   Text preview: I rented I AM CURIOUS-YELLOW from my video store because of..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Text Preprocessing\n",
    "\n",
    "For NLP tasks, you typically need to:\n",
    "1. Tokenize text (convert to token IDs)\n",
    "2. Truncate/pad to fixed length\n",
    "3. Create attention masks\n",
    "\n",
    "Here's a simple length-based transform for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(element, key=None):  # noqa: ARG001\n",
    "    \"\"\"Normalize sentiment label to JAX array.\"\"\"\n",
    "    del key  # Unused - deterministic operator\n",
    "\n",
    "    # IMDB labels: 0=negative, 1=positive\n",
    "    # Convert to proper JAX array for batching\n",
    "    label = element.data.get(\"label\", 0)\n",
    "    return element.update_data({\"label\": jnp.array(label, dtype=jnp.int32)})\n",
    "\n",
    "\n",
    "text_stats_op = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False),\n",
    "    fn=normalize_label,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "print(\"Created label normalization operator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Build Pipeline with Preprocessing\n",
    "\n",
    "Chain the source with our preprocessing operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh source for the full pipeline\n",
    "# Note: We exclude 'text' field because strings can't be batched as JAX arrays.\n",
    "# For text processing, you would typically tokenize first or process element-by-element.\n",
    "source2 = HFEagerSource(\n",
    "    HFEagerConfig(\n",
    "        name=\"stanfordnlp/imdb\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        download_kwargs={\"trust_remote_code\": True},\n",
    "        exclude_keys={\"text\"},  # Exclude text field - can't batch strings\n",
    "    ),\n",
    "    rngs=nnx.Rngs(1),\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = from_source(source2, batch_size=8).add(OperatorNode(text_stats_op))\n",
    "\n",
    "print(\"Pipeline: HFEagerSource(IMDB) -> TextStats -> Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Process and Analyze\n",
    "\n",
    "Collect statistics about review lengths and sentiment distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process batches and collect sentiment statistics\n",
    "print(\"\\nAnalyzing IMDB review sentiment:\")\n",
    "\n",
    "total_reviews = 0\n",
    "total_positive = 0\n",
    "\n",
    "num_batches = 20  # Process 20 batches for analysis\n",
    "\n",
    "for i, batch in enumerate(pipeline):\n",
    "    if i >= num_batches:\n",
    "        break\n",
    "\n",
    "    data = batch.get_data()\n",
    "\n",
    "    batch_size = len(data[\"label\"]) if hasattr(data[\"label\"], \"__len__\") else 1\n",
    "    total_reviews += batch_size\n",
    "\n",
    "    # Count positives (label=1 is positive)\n",
    "    labels = data[\"label\"]\n",
    "    if hasattr(labels, \"__iter__\"):\n",
    "        total_positive += sum(1 for l in labels if l == 1)\n",
    "    else:\n",
    "        total_positive += 1 if labels == 1 else 0\n",
    "\n",
    "    if i < 3:  # Show first 3 batches\n",
    "        print(f\"Batch {i}: {batch_size} samples, labels={labels[:5]}...\")\n",
    "\n",
    "print(f\"\\nSentiment Summary ({total_reviews} reviews analyzed):\")\n",
    "print(f\"  Positive: {total_positive} ({100 * total_positive / total_reviews:.1f}%)\")\n",
    "total_negative = total_reviews - total_positive\n",
    "print(f\"  Negative: {total_negative} ({100 * total_negative / total_reviews:.1f}%)\")\n",
    "\n",
    "# Expected output:\n",
    "# Sentiment Summary (160 reviews analyzed):\n",
    "#   Positive: ~50%\n",
    "#   Negative: ~50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Dataset** | IMDB (25k train reviews) |\n",
    "| **Format** | Text + binary label |\n",
    "| **Mode** | Streaming (no full download) |\n",
    "| **Preprocessing** | Label normalization (text excluded for batching) |\n",
    "\n",
    "### Text vs Image Pipelines\n",
    "\n",
    "| Aspect | Image | Text |\n",
    "|--------|-------|------|\n",
    "| Data type | Arrays (H×W×C) | Strings (can't batch directly) |\n",
    "| Batching | Stack arrays | Tokenize first, then batch |\n",
    "| Normalization | Pixel scaling | Tokenization to IDs |\n",
    "| Augmentation | Spatial transforms | Synonym replacement, etc. |\n",
    "\n",
    "**Note:** Text strings cannot be batched as JAX arrays. For NLP tasks:\n",
    "1. Use `exclude_keys` to skip text fields when batching numeric fields\n",
    "2. Process text element-by-element, or\n",
    "3. Tokenize text to numeric IDs before batching\n",
    "\n",
    "### Integration Notes\n",
    "\n",
    "For full NLP pipelines, you would typically:\n",
    "\n",
    "1. Use a tokenizer (HuggingFace tokenizers, SentencePiece)\n",
    "2. Convert tokens to fixed-length sequences\n",
    "3. Add attention masks for padding\n",
    "4. Store as JAX arrays for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Full tutorial**: [HuggingFace Tutorial](02_hf_tutorial.ipynb) for advanced usage\n",
    "- **Image datasets**: [CIFAR-10](../../core/04_cifar10_quickref.ipynb)\n",
    "- **TFDS alternative**: [TFDS](../tfds/01_tfds_quickref.ipynb)\n",
    "- **API Reference**: [HFEagerSource](https://datarax.readthedocs.io/sources/hf/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the IMDB quick reference example.\"\"\"\n",
    "    print(\"IMDB Sentiment Analysis Quick Reference\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load dataset (exclude text field - strings can't be batched)\n",
    "    config = HFEagerConfig(\n",
    "        name=\"stanfordnlp/imdb\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        download_kwargs={\"trust_remote_code\": True},\n",
    "        exclude_keys={\"text\"},\n",
    "    )\n",
    "    source = HFEagerSource(config, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create pipeline with label normalization\n",
    "    pipeline = from_source(source, batch_size=8).add(OperatorNode(text_stats_op))\n",
    "\n",
    "    # Process batches\n",
    "    total_reviews = 0\n",
    "    total_positive = 0\n",
    "\n",
    "    for i, batch in enumerate(pipeline):\n",
    "        if i >= 10:  # Process 10 batches\n",
    "            break\n",
    "\n",
    "        data = batch.get_data()\n",
    "        batch_size = len(data[\"label\"]) if hasattr(data[\"label\"], \"__len__\") else 1\n",
    "        total_reviews += batch_size\n",
    "\n",
    "        labels = data[\"label\"]\n",
    "        if hasattr(labels, \"__iter__\"):\n",
    "            total_positive += sum(1 for l in labels if l == 1)\n",
    "        else:\n",
    "            total_positive += 1 if labels == 1 else 0\n",
    "\n",
    "    print(f\"Processed {total_reviews} IMDB reviews\")\n",
    "    negative = total_reviews - total_positive\n",
    "    print(f\"Sentiment distribution: {total_positive} positive, {negative} negative\")\n",
    "    print(\"Example completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
