{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# HuggingFace Datasets Quick Reference\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Beginner |\n",
    "| **Runtime** | ~5 min |\n",
    "| **Prerequisites** | Basic Datarax pipeline knowledge |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Load and process datasets from [HuggingFace Hub](https://huggingface.co/datasets)\n",
    "using Datarax's `HFEagerSource`. This enables access to thousands of pre-built datasets\n",
    "with seamless integration into your data pipelines.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this example, you will be able to:\n",
    "\n",
    "1. Configure `HFEagerSource` for HuggingFace datasets\n",
    "2. Use streaming mode for large datasets\n",
    "3. Inspect dataset structure and contents\n",
    "4. Apply transformations to HuggingFace data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "# Install datarax with data dependencies\n",
    "uv pip install \"datarax[data]\"\n",
    "```\n",
    "\n",
    "**Note**: First run may download dataset files from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "from datarax import from_source\n",
    "from datarax.dag.nodes import OperatorNode\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.sources import HFEagerConfig, HFEagerSource\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Configure HuggingFace Source\n",
    "\n",
    "`HFEagerConfig` specifies which dataset to load.\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "- `name`: Dataset identifier (e.g., \"mnist\", \"imdb\", \"squad\")\n",
    "- `split`: Which split to use (\"train\", \"test\", \"validation\")\n",
    "- `streaming`: Enable for large datasets to avoid full download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset in streaming mode\n",
    "config = HFEagerConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"train\",\n",
    "    streaming=True,  # Stream data instead of downloading all\n",
    ")\n",
    "\n",
    "source = HFEagerSource(config, rngs=nnx.Rngs(0))\n",
    "print(f\"Loaded HuggingFace dataset: {config.name}\")\n",
    "\n",
    "# Check dataset size (may not be available in streaming mode)\n",
    "try:\n",
    "    print(f\"Dataset size: {len(source)}\")\n",
    "except (NotImplementedError, TypeError):\n",
    "    print(\"Dataset size: N/A (streaming mode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Create Pipeline and Inspect Data\n",
    "\n",
    "Build a pipeline and examine what data the dataset provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with batch_size=1 for inspection\n",
    "pipeline = from_source(source, batch_size=1)\n",
    "\n",
    "# Get first few examples\n",
    "print(\"First 3 examples:\")\n",
    "example_iter = iter(pipeline)\n",
    "\n",
    "for i in range(3):\n",
    "    batch = next(example_iter)\n",
    "    data = batch.get_data()\n",
    "\n",
    "    print(f\"\\nExample {i + 1}:\")\n",
    "    print(f\"  Keys: {list(data.keys())}\")\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if hasattr(value, \"shape\"):\n",
    "            print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value).__name__} = {value}\")\n",
    "\n",
    "# Expected output (MNIST):\n",
    "# Example 1:\n",
    "#   Keys: ['image', 'label']\n",
    "#   image: shape=(28, 28), dtype=uint8\n",
    "#   label: int = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Apply Transformations\n",
    "\n",
    "Add operators to transform the HuggingFace data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a normalization transform\n",
    "def normalize_image(element, key=None):\n",
    "    \"\"\"Normalize image to [0, 1] range and add channel dimension.\"\"\"\n",
    "    image = element.data.get(\"image\")\n",
    "    if image is not None and hasattr(image, \"dtype\"):\n",
    "        # Normalize to [0, 1]\n",
    "        normalized = image.astype(jnp.float32) / 255.0\n",
    "        # Add channel dimension if needed\n",
    "        if normalized.ndim == 2:\n",
    "            normalized = normalized[..., None]\n",
    "        return element.update_data({\"image\": normalized})\n",
    "    return element\n",
    "\n",
    "\n",
    "# Create operator\n",
    "normalizer = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False),\n",
    "    fn=normalize_image,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "# Build transformed pipeline (need fresh source for new iteration)\n",
    "source2 = HFEagerSource(config, rngs=nnx.Rngs(1))\n",
    "transformed_pipeline = from_source(source2, batch_size=32).add(OperatorNode(normalizer))\n",
    "\n",
    "# Process a batch\n",
    "batch = next(iter(transformed_pipeline))\n",
    "image_batch = batch[\"image\"]\n",
    "\n",
    "print(\"Transformed batch:\")\n",
    "print(f\"  Image shape: {image_batch.shape}\")\n",
    "print(f\"  Image range: [{image_batch.min():.3f}, {image_batch.max():.3f}]\")\n",
    "\n",
    "# Expected output:\n",
    "# Transformed batch:\n",
    "#   Image shape: (32, 28, 28, 1)\n",
    "#   Image range: [0.000, 1.000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Feature | Value |\n",
    "|---------|-------|\n",
    "| Dataset | MNIST from HuggingFace Hub |\n",
    "| Mode | Streaming (no full download) |\n",
    "| Batch Size | 32 |\n",
    "| Output Shape | (32, 28, 28, 1) |\n",
    "| Normalization | [0, 255] â†’ [0, 1] |\n",
    "\n",
    "HuggingFace integration provides:\n",
    "\n",
    "- Access to 100,000+ datasets\n",
    "- Automatic caching and versioning\n",
    "- Streaming for large datasets\n",
    "- Seamless Datarax pipeline integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- **More datasets**: Try `\"imdb\"`, `\"squad\"`, `\"cifar10\"` - change the `name` parameter\n",
    "- **Custom configs**: Use `HFEagerConfig(subset=\"...\")` for dataset variants\n",
    "- **TFDS alternative**: [TFDS](../tfds/01_tfds_quickref.ipynb)\n",
    "- **Full tutorial**: [HuggingFace Tutorial](02_hf_tutorial.py) for advanced usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the HuggingFace integration example.\"\"\"\n",
    "    print(\"HuggingFace Datasets Integration Example\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load dataset\n",
    "    config = HFEagerConfig(name=\"mnist\", split=\"train\", streaming=True)\n",
    "    source = HFEagerSource(config, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create pipeline with normalization\n",
    "    def normalize(element, key=None):\n",
    "        image = element.data.get(\"image\")\n",
    "        if image is not None:\n",
    "            normalized = image.astype(jnp.float32) / 255.0\n",
    "            if normalized.ndim == 2:\n",
    "                normalized = normalized[..., None]\n",
    "            return element.update_data({\"image\": normalized})\n",
    "        return element\n",
    "\n",
    "    normalizer = ElementOperator(\n",
    "        ElementOperatorConfig(stochastic=False), fn=normalize, rngs=nnx.Rngs(0)\n",
    "    )\n",
    "\n",
    "    pipeline = from_source(source, batch_size=32).add(OperatorNode(normalizer))\n",
    "\n",
    "    # Process batches\n",
    "    total_samples = 0\n",
    "    for i, batch in enumerate(pipeline):\n",
    "        if i >= 10:  # Process 10 batches\n",
    "            break\n",
    "        total_samples += batch[\"image\"].shape[0]\n",
    "\n",
    "    print(f\"Processed {total_samples} samples from HuggingFace MNIST\")\n",
    "    print(\"Example completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
