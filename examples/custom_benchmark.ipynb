{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26949d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Custom benchmark script for Datarax.\n",
    "\n",
    "This script demonstrates how to use Datarax's benchmark utilities directly\n",
    "in Python code, without using the CLI.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datarax import from_source\n",
    "from datarax.core.nodes import OperatorNode\n",
    "from datarax.dag import DAGExecutor\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.sources import MemorySource, MemorySourceConfig\n",
    "from datarax.benchmarking.pipeline_throughput import (\n",
    "    BatchSizeBenchmark,\n",
    "    PipelineBenchmark,\n",
    "    ProfileReport,\n",
    "    benchmark_comparison,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_image_data(num_samples: int = 1000, image_size: int = 32) -> dict:\n",
    "    \"\"\"Generate sample image data for benchmarking.\n",
    "\n",
    "    Args:\n",
    "        num_samples: Number of samples to generate.\n",
    "        image_size: Size of each image (image_size x image_size x 3).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with 'image' and 'label' arrays.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(42)\n",
    "    return {\n",
    "        \"image\": rng.rand(num_samples, image_size, image_size, 3).astype(np.float32),\n",
    "        \"label\": rng.randint(0, 10, (num_samples,)).astype(np.int32),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_transform(element, key=None):\n",
    "    \"\"\"Normalize image values to [0, 1] range.\n",
    "\n",
    "    Args:\n",
    "        element: Element containing 'image' and 'label' data.\n",
    "        key: Unused PRNG key (for API compatibility).\n",
    "\n",
    "    Returns:\n",
    "        Element with normalized image.\n",
    "    \"\"\"\n",
    "    return element.update_data({\"image\": element.data[\"image\"] / 255.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip_transform(element, key):\n",
    "    \"\"\"Apply random horizontal flip to image.\n",
    "\n",
    "    Args:\n",
    "        element: Element containing 'image' and 'label' data.\n",
    "        key: JAX PRNG key for randomness.\n",
    "\n",
    "    Returns:\n",
    "        Element with potentially flipped image.\n",
    "    \"\"\"\n",
    "    image = element.data[\"image\"]\n",
    "    flip = jax.random.bernoulli(key, 0.5)\n",
    "    flipped_image = jnp.where(flip, jnp.fliplr(image), image)\n",
    "    return element.update_data({\"image\": flipped_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffeb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_heavy_transform(element, key):\n",
    "    \"\"\"Simulate a compute-intensive operation.\n",
    "\n",
    "    Args:\n",
    "        element: Element containing 'image' and 'label' data.\n",
    "        key: JAX PRNG key (unused in this transform).\n",
    "\n",
    "    Returns:\n",
    "        Element with slightly modified image.\n",
    "    \"\"\"\n",
    "    image = element.data[\"image\"]\n",
    "    # Simulate a compute-intensive operation\n",
    "    for _ in range(10):\n",
    "        image = image * 0.99\n",
    "    return element.update_data({\"image\": image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_pipeline(batch_size: int = 32) -> DAGExecutor:\n",
    "    \"\"\"Create a basic image pipeline with minimal processing.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Number of samples per batch.\n",
    "\n",
    "    Returns:\n",
    "        DAGExecutor configured with source and normalizer.\n",
    "    \"\"\"\n",
    "    # Generate sample data\n",
    "    data = generate_sample_image_data()\n",
    "\n",
    "    # Create data source using config-based API\n",
    "    source_config = MemorySourceConfig()\n",
    "    source = MemorySource(source_config, data=data, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create normalizer operator (deterministic)\n",
    "    normalizer_config = ElementOperatorConfig(stochastic=False)\n",
    "    normalizer = ElementOperator(normalizer_config, fn=normalize_transform, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Build pipeline using DAG-based API\n",
    "    pipeline = from_source(source, batch_size=batch_size).add(OperatorNode(normalizer))\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_pipeline(batch_size: int = 32) -> DAGExecutor:\n",
    "    \"\"\"Create a more complex image pipeline with augmentation.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Number of samples per batch.\n",
    "\n",
    "    Returns:\n",
    "        DAGExecutor configured with source, normalizer, and augmenters.\n",
    "    \"\"\"\n",
    "    # Generate sample data\n",
    "    data = generate_sample_image_data()\n",
    "\n",
    "    # Create data source using config-based API\n",
    "    source_config = MemorySourceConfig()\n",
    "    source = MemorySource(source_config, data=data, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create normalizer operator (deterministic)\n",
    "    normalizer_config = ElementOperatorConfig(stochastic=False)\n",
    "    normalizer = ElementOperator(normalizer_config, fn=normalize_transform, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create flip augmenter (stochastic)\n",
    "    flip_config = ElementOperatorConfig(stochastic=True, stream_name=\"flip\")\n",
    "    flip_augmenter = ElementOperator(flip_config, fn=random_flip_transform, rngs=nnx.Rngs(flip=42))\n",
    "\n",
    "    # Create heavy transform (stochastic for API consistency)\n",
    "    heavy_config = ElementOperatorConfig(stochastic=True, stream_name=\"heavy\")\n",
    "    heavy_transform = ElementOperator(\n",
    "        heavy_config, fn=simulated_heavy_transform, rngs=nnx.Rngs(heavy=43)\n",
    "    )\n",
    "\n",
    "    # Build pipeline using DAG-based API\n",
    "    pipeline = (\n",
    "        from_source(source, batch_size=batch_size)\n",
    "        .add(OperatorNode(normalizer))\n",
    "        .add(OperatorNode(flip_augmenter))\n",
    "        .add(OperatorNode(heavy_transform))\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c291f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unbatched_pipeline(batch_size: int = 32) -> DAGExecutor:\n",
    "    \"\"\"Create a pipeline for batch size benchmarks.\n",
    "\n",
    "    This factory creates a complete pipeline with the specified batch size.\n",
    "    BatchSizeBenchmark calls this with different batch sizes to compare performance.\n",
    "\n",
    "    Args:\n",
    "        batch_size: The batch size to use for this pipeline instance.\n",
    "\n",
    "    Returns:\n",
    "        DAGExecutor configured with the specified batch size.\n",
    "    \"\"\"\n",
    "    # Generate sample data\n",
    "    data = generate_sample_image_data()\n",
    "\n",
    "    # Create data source using config-based API\n",
    "    source_config = MemorySourceConfig()\n",
    "    source = MemorySource(source_config, data=data, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create normalizer operator (deterministic)\n",
    "    normalizer_config = ElementOperatorConfig(stochastic=False)\n",
    "    normalizer = ElementOperator(normalizer_config, fn=normalize_transform, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Build complete pipeline WITH batching at the specified size\n",
    "    # Operators are added AFTER BatchNode so they receive Batch objects\n",
    "    pipeline = from_source(source, batch_size=batch_size, enforce_batch=True).add(\n",
    "        OperatorNode(normalizer)\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c616303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_benchmark():\n",
    "    \"\"\"Run a basic pipeline benchmark.\"\"\"\n",
    "    print(\"\\n=== Running Pipeline Benchmark ===\")\n",
    "    pipeline = create_basic_pipeline(batch_size=32)\n",
    "\n",
    "    benchmark = PipelineBenchmark(\n",
    "        pipeline,\n",
    "        num_batches=50,\n",
    "        warmup_batches=5,\n",
    "    )\n",
    "\n",
    "    print(\"Running benchmark...\")\n",
    "    results = benchmark.run(pipeline_seed=42)\n",
    "    benchmark.print_results()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison_benchmark():\n",
    "    \"\"\"Run a comparison benchmark between different pipelines.\"\"\"\n",
    "    print(\"\\n=== Running Comparison Benchmark ===\")\n",
    "\n",
    "    configurations = {\n",
    "        \"basic\": create_basic_pipeline(batch_size=32),\n",
    "        \"advanced\": create_advanced_pipeline(batch_size=32),\n",
    "    }\n",
    "\n",
    "    print(\"Comparing pipelines...\")\n",
    "    results = benchmark_comparison(\n",
    "        configurations,\n",
    "        num_batches=30,\n",
    "        warmup_batches=5,\n",
    "    )\n",
    "\n",
    "    print(\"\\nComparison Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Configuration |   Examples/s    |    Batches/s    |  Duration (s)  \")\n",
    "    print(\"-\" * 80)\n",
    "    for name, metrics in results.items():\n",
    "        print(\n",
    "            f\"{name:^13} | {metrics['examples_per_second']:^15.2f} | \"\n",
    "            f\"{metrics['batches_per_second']:^15.2f} | \"\n",
    "            f\"{metrics['duration_seconds']:^14.4f}\"\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_profile_report():\n",
    "    \"\"\"Run a profile report.\"\"\"\n",
    "    print(\"\\n=== Running Profile Report ===\")\n",
    "\n",
    "    pipeline = create_advanced_pipeline(batch_size=32)\n",
    "\n",
    "    profile = ProfileReport(pipeline)\n",
    "\n",
    "    print(\"Running profile...\")\n",
    "    profile.run(num_batches=10, pipeline_seed=42)\n",
    "    profile.print_report()\n",
    "\n",
    "    return profile.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_size_benchmark():\n",
    "    \"\"\"Run a batch size benchmark.\"\"\"\n",
    "    print(\"\\n=== Running Batch Size Benchmark ===\")\n",
    "\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "\n",
    "    benchmark = BatchSizeBenchmark(\n",
    "        data_stream_factory=create_unbatched_pipeline,\n",
    "        batch_sizes=batch_sizes,\n",
    "        num_batches=30,\n",
    "        warmup_batches=5,\n",
    "    )\n",
    "\n",
    "    print(f\"Running batch size benchmark with sizes {batch_sizes}...\")\n",
    "    results = benchmark.run(pipeline_seed=42)\n",
    "\n",
    "    print(\"\\nBatch Size Benchmark Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Batch Size |   Examples/s    |    Batches/s    |  Duration (s)  \")\n",
    "    print(\"-\" * 80)\n",
    "    for batch_size, metrics in results.items():\n",
    "        print(\n",
    "            f\"{batch_size:^10} | {metrics['examples_per_second']:^15.2f} | \"\n",
    "            f\"{metrics['batches_per_second']:^15.2f} | \"\n",
    "            f\"{metrics['duration_seconds']:^14.4f}\"\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run all benchmarks.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run various benchmarks\n",
    "    pipeline_results = run_pipeline_benchmark()\n",
    "    comparison_results = run_comparison_benchmark()\n",
    "    batch_size_results = run_batch_size_benchmark()\n",
    "    profile_results = run_profile_report()\n",
    "\n",
    "    # Save all results\n",
    "    all_results = {\n",
    "        \"pipeline_benchmark\": pipeline_results,\n",
    "        \"comparison_benchmark\": comparison_results,\n",
    "        \"batch_size_benchmark\": batch_size_results,\n",
    "        \"profile_report\": profile_results,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nAll benchmarks completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d13c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
