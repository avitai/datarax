{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Custom benchmark script for Datarax.\n",
    "\n",
    "This script demonstrates how to use Datarax's benchmark utilities directly\n",
    "in Python code, without using the CLI.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datarax import from_source\n",
    "from datarax.core.nodes import OperatorNode\n",
    "from datarax.dag import DAGExecutor\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.sources import MemorySource, MemorySourceConfig\n",
    "from datarax.benchmarking.timing import TimingCollector, TimingSample\n",
    "from datarax.benchmarking.comparative import BenchmarkComparison\n",
    "from datarax.benchmarking.results import BenchmarkResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_image_data(num_samples: int = 1000, image_size: int = 32) -> dict:\n",
    "    \"\"\"Generate sample image data for benchmarking.\n",
    "\n",
    "    Args:\n",
    "        num_samples: Number of samples to generate.\n",
    "        image_size: Size of each image (image_size x image_size x 3).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with 'image' and 'label' arrays.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(42)\n",
    "    return {\n",
    "        \"image\": rng.rand(num_samples, image_size, image_size, 3).astype(np.float32),\n",
    "        \"label\": rng.randint(0, 10, (num_samples,)).astype(np.int32),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_transform(element, key=None):\n",
    "    \"\"\"Normalize image values to [0, 1] range.\n",
    "\n",
    "    Args:\n",
    "        element: Element containing 'image' and 'label' data.\n",
    "        key: Unused PRNG key (for API compatibility).\n",
    "\n",
    "    Returns:\n",
    "        Element with normalized image.\n",
    "    \"\"\"\n",
    "    return element.update_data({\"image\": element.data[\"image\"] / 255.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip_transform(element, key):\n",
    "    \"\"\"Apply random horizontal flip to image.\n",
    "\n",
    "    Args:\n",
    "        element: Element containing 'image' and 'label' data.\n",
    "        key: JAX PRNG key for randomness.\n",
    "\n",
    "    Returns:\n",
    "        Element with potentially flipped image.\n",
    "    \"\"\"\n",
    "    image = element.data[\"image\"]\n",
    "    flip = jax.random.bernoulli(key, 0.5)\n",
    "    flipped_image = jnp.where(flip, jnp.fliplr(image), image)\n",
    "    return element.update_data({\"image\": flipped_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_heavy_transform(element, key):\n",
    "    \"\"\"Simulate a compute-intensive operation.\n",
    "\n",
    "    Args:\n",
    "        element: Element containing 'image' and 'label' data.\n",
    "        key: JAX PRNG key (unused in this transform).\n",
    "\n",
    "    Returns:\n",
    "        Element with slightly modified image.\n",
    "    \"\"\"\n",
    "    image = element.data[\"image\"]\n",
    "    # Simulate a compute-intensive operation\n",
    "    for _ in range(10):\n",
    "        image = image * 0.99\n",
    "    return element.update_data({\"image\": image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_pipeline(batch_size: int = 32) -> DAGExecutor:\n",
    "    \"\"\"Create a basic image pipeline with minimal processing.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Number of samples per batch.\n",
    "\n",
    "    Returns:\n",
    "        DAGExecutor configured with source and normalizer.\n",
    "    \"\"\"\n",
    "    # Generate sample data\n",
    "    data = generate_sample_image_data()\n",
    "\n",
    "    # Create data source using config-based API\n",
    "    source_config = MemorySourceConfig()\n",
    "    source = MemorySource(source_config, data=data, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create normalizer operator (deterministic)\n",
    "    normalizer_config = ElementOperatorConfig(stochastic=False)\n",
    "    normalizer = ElementOperator(normalizer_config, fn=normalize_transform, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Build pipeline using DAG-based API\n",
    "    pipeline = from_source(source, batch_size=batch_size).add(OperatorNode(normalizer))\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_pipeline(batch_size: int = 32) -> DAGExecutor:\n",
    "    \"\"\"Create a more complex image pipeline with augmentation.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Number of samples per batch.\n",
    "\n",
    "    Returns:\n",
    "        DAGExecutor configured with source, normalizer, and augmenters.\n",
    "    \"\"\"\n",
    "    # Generate sample data\n",
    "    data = generate_sample_image_data()\n",
    "\n",
    "    # Create data source using config-based API\n",
    "    source_config = MemorySourceConfig()\n",
    "    source = MemorySource(source_config, data=data, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create normalizer operator (deterministic)\n",
    "    normalizer_config = ElementOperatorConfig(stochastic=False)\n",
    "    normalizer = ElementOperator(normalizer_config, fn=normalize_transform, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create flip augmenter (stochastic)\n",
    "    flip_config = ElementOperatorConfig(stochastic=True, stream_name=\"flip\")\n",
    "    flip_augmenter = ElementOperator(flip_config, fn=random_flip_transform, rngs=nnx.Rngs(flip=42))\n",
    "\n",
    "    # Create heavy transform (stochastic for API consistency)\n",
    "    heavy_config = ElementOperatorConfig(stochastic=True, stream_name=\"heavy\")\n",
    "    heavy_transform = ElementOperator(\n",
    "        heavy_config, fn=simulated_heavy_transform, rngs=nnx.Rngs(heavy=43)\n",
    "    )\n",
    "\n",
    "    # Build pipeline using DAG-based API\n",
    "    pipeline = (\n",
    "        from_source(source, batch_size=batch_size)\n",
    "        .add(OperatorNode(normalizer))\n",
    "        .add(OperatorNode(flip_augmenter))\n",
    "        .add(OperatorNode(heavy_transform))\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sync_fn():\n",
    "    \"\"\"JAX device sync for accurate GPU timing.\"\"\"\n",
    "    jnp.array(0.0).block_until_ready()\n",
    "\n",
    "\n",
    "def measure_pipeline(pipeline, num_batches: int = 50, warmup: int = 5) -> TimingSample:\n",
    "    \"\"\"Measure pipeline throughput with warmup.\"\"\"\n",
    "    for i, _ in enumerate(pipeline):\n",
    "        if i >= warmup - 1:\n",
    "            break\n",
    "\n",
    "    collector = TimingCollector(sync_fn=_sync_fn)\n",
    "    return collector.measure_iteration(iter(pipeline), num_batches=num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_benchmark():\n",
    "    \"\"\"Run a basic pipeline benchmark.\"\"\"\n",
    "    print(\"\\n=== Running Pipeline Benchmark ===\")\n",
    "    pipeline = create_basic_pipeline(batch_size=32)\n",
    "    sample = measure_pipeline(pipeline, num_batches=50)\n",
    "\n",
    "    bps = sample.num_batches / sample.wall_clock_sec if sample.wall_clock_sec > 0 else 0\n",
    "    eps = sample.num_elements / sample.wall_clock_sec if sample.wall_clock_sec > 0 else 0\n",
    "    print(f\"  Wall clock:   {sample.wall_clock_sec:.4f} s\")\n",
    "    print(f\"  Batches/sec:  {bps:.2f}\")\n",
    "    print(f\"  Elements/sec: {eps:.2f}\")\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison_benchmark():\n",
    "    \"\"\"Run a comparison benchmark between different pipelines.\"\"\"\n",
    "    print(\"\\n=== Running Comparison Benchmark ===\")\n",
    "\n",
    "    configs = {\n",
    "        \"basic\": create_basic_pipeline(batch_size=32),\n",
    "        \"advanced\": create_advanced_pipeline(batch_size=32),\n",
    "    }\n",
    "\n",
    "    comparison = BenchmarkComparison()\n",
    "    for name, pipeline in configs.items():\n",
    "        sample = measure_pipeline(pipeline, num_batches=30)\n",
    "        result = BenchmarkResult(\n",
    "            framework=\"Datarax\",\n",
    "            scenario_id=\"custom\",\n",
    "            variant=name,\n",
    "            timing=sample,\n",
    "            resources=None,\n",
    "            environment={},\n",
    "            config={\"batch_size\": 32},\n",
    "        )\n",
    "        comparison.add_result(name, result)\n",
    "\n",
    "    print(f\"\\n  Best config:  {comparison.best_config}\")\n",
    "    print(f\"  Worst config: {comparison.worst_config}\")\n",
    "    ratios = comparison.get_performance_ratio()\n",
    "    for name, ratio in ratios.items():\n",
    "        print(f\"  {name}: {ratio:.2f}x relative throughput\")\n",
    "\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_profile_report():\n",
    "    \"\"\"Run a profile report using TimingCollector.\"\"\"\n",
    "    print(\"\\n=== Running Profile Report ===\")\n",
    "\n",
    "    pipeline = create_advanced_pipeline(batch_size=32)\n",
    "    sample = measure_pipeline(pipeline, num_batches=10)\n",
    "\n",
    "    bps = sample.num_batches / sample.wall_clock_sec if sample.wall_clock_sec > 0 else 0\n",
    "    eps = sample.num_elements / sample.wall_clock_sec if sample.wall_clock_sec > 0 else 0\n",
    "    print(f\"  Wall clock:     {sample.wall_clock_sec:.4f} s\")\n",
    "    print(f\"  First batch:    {sample.first_batch_time * 1000:.2f} ms\")\n",
    "    print(f\"  Batches/sec:    {bps:.2f}\")\n",
    "    print(f\"  Elements/sec:   {eps:.2f}\")\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_size_benchmark():\n",
    "    \"\"\"Run a batch size benchmark.\"\"\"\n",
    "    print(\"\\n=== Running Batch Size Benchmark ===\")\n",
    "\n",
    "    batch_sizes = [8, 16, 32, 64, 128]\n",
    "    print(f\"{'Batch Size':>10} | {'Elements/sec':>15} | {'Wall Clock (s)':>15}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        pipeline = create_basic_pipeline(batch_size=batch_size)\n",
    "        sample = measure_pipeline(pipeline, num_batches=30)\n",
    "        eps = sample.num_elements / sample.wall_clock_sec if sample.wall_clock_sec > 0 else 0\n",
    "        print(f\"{batch_size:>10} | {eps:>15.2f} | {sample.wall_clock_sec:>15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run all benchmarks.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run various benchmarks\n",
    "    pipeline_results = run_pipeline_benchmark()\n",
    "    comparison_results = run_comparison_benchmark()\n",
    "    batch_size_results = run_batch_size_benchmark()\n",
    "    profile_results = run_profile_report()\n",
    "\n",
    "    # Save all results\n",
    "    all_results = {\n",
    "        \"pipeline_benchmark\": pipeline_results,\n",
    "        \"comparison_benchmark\": comparison_results,\n",
    "        \"batch_size_benchmark\": batch_size_results,\n",
    "        \"profile_report\": profile_results,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nAll benchmarks completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
