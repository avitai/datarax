{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# CIFAR-10 Pipeline Quick Reference\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Beginner |\n",
    "| **Runtime** | ~5 min |\n",
    "| **Prerequisites** | Basic Datarax pipeline, TFDS setup |\n",
    "| **Format** | Python + Jupyter |\n",
    "| **Memory** | ~500 MB RAM |\n",
    "\n",
    "## Overview\n",
    "\n",
    "This quick reference demonstrates loading and processing CIFAR-10 from TensorFlow\n",
    "Datasets (TFDS). CIFAR-10 is a classic benchmark dataset containing 60,000 32x32\n",
    "color images in 10 classes, making it ideal for learning image classification pipelines.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this example, you will be able to:\n",
    "\n",
    "1. Load CIFAR-10 using `TFDSEagerSource` with proper configuration\n",
    "2. Apply standard CIFAR-10 normalization (ImageNet-style)\n",
    "3. Build a batched pipeline ready for training\n",
    "4. Understand the data shapes and preprocessing workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "# Install datarax with TFDS support\n",
    "uv pip install \"datarax[tfds]\"\n",
    "```\n",
    "\n",
    "**Note**: First run downloads CIFAR-10 (~170 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Memory Configuration - prevent TensorFlow from using GPU\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES_FOR_TF\"] = \"\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "# Now import JAX and Datarax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "from datarax import from_source\n",
    "from datarax.dag.nodes import OperatorNode\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.sources import TFDSEagerConfig, TFDSEagerSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## CIFAR-10 Preprocessing Constants\n",
    "\n",
    "Standard normalization values for CIFAR-10, computed from the training set.\n",
    "Using these values ensures compatibility with pretrained models and published results.\n",
    "\n",
    "| Statistic | R | G | B |\n",
    "|-----------|---|---|---|\n",
    "| **Mean** | 0.4914 | 0.4822 | 0.4465 |\n",
    "| **Std** | 0.2470 | 0.2435 | 0.2616 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 normalization constants\n",
    "CIFAR10_MEAN = jnp.array([0.4914, 0.4822, 0.4465])\n",
    "CIFAR10_STD = jnp.array([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "# Class names for reference\n",
    "CIFAR10_CLASSES = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "print(\"CIFAR-10 classes:\", CIFAR10_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Create TFDS Data Source\n",
    "\n",
    "Configure `TFDSEagerSource` to load CIFAR-10 training split. We use a subset\n",
    "for this quick reference to keep runtime short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 training data (subset for quick demo)\n",
    "config = TFDSEagerConfig(\n",
    "    name=\"cifar10\",\n",
    "    split=\"train[:1000]\",  # First 1000 samples for demo\n",
    "    shuffle=True,\n",
    "    seed=42,  # Integer seed for Grain's index_shuffle\n",
    "    exclude_keys={\"id\"},  # Exclude non-numeric fields\n",
    ")\n",
    "\n",
    "source = TFDSEagerSource(config, rngs=nnx.Rngs(42))\n",
    "\n",
    "print(\"Dataset: CIFAR-10\")\n",
    "print(f\"Samples: {len(source)}\")\n",
    "print(f\"Classes: {len(CIFAR10_CLASSES)}\")\n",
    "\n",
    "# Expected output:\n",
    "# Dataset: CIFAR-10\n",
    "# Samples: 1000\n",
    "# Classes: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Define Preprocessing\n",
    "\n",
    "Standard CIFAR-10 preprocessing:\n",
    "1. Convert uint8 [0, 255] to float32 [0, 1]\n",
    "2. Apply channel-wise normalization with CIFAR-10 statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cifar10(element, key=None):  # noqa: ARG001\n",
    "    \"\"\"Normalize CIFAR-10 images to standard statistics.\"\"\"\n",
    "    del key  # Unused - deterministic operator\n",
    "    image = element.data[\"image\"]\n",
    "\n",
    "    # Convert to float32 and scale to [0, 1]\n",
    "    image = image.astype(jnp.float32) / 255.0\n",
    "\n",
    "    # Apply CIFAR-10 normalization: (x - mean) / std\n",
    "    image = (image - CIFAR10_MEAN) / CIFAR10_STD\n",
    "\n",
    "    return element.update_data({\"image\": image})\n",
    "\n",
    "\n",
    "normalizer = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False),\n",
    "    fn=preprocess_cifar10,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "print(\"Created CIFAR-10 normalizer with standard statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Build Pipeline\n",
    "\n",
    "Chain source and preprocessing into a batched pipeline.\n",
    "Batch size of 32 is standard for CIFAR-10 training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training pipeline\n",
    "batch_size = 32\n",
    "pipeline = from_source(source, batch_size=batch_size).add(OperatorNode(normalizer))\n",
    "\n",
    "print(\"Pipeline: TFDSEagerSource(CIFAR-10) -> Normalize -> Output\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Batches per epoch: {len(source) // batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Iterate Through Data\n",
    "\n",
    "Process batches and verify the preprocessing is correct.\n",
    "Normalized data should have approximately zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and verify batches\n",
    "print(\"\\nProcessing batches:\")\n",
    "all_means = []\n",
    "all_stds = []\n",
    "\n",
    "for i, batch in enumerate(pipeline):\n",
    "    if i >= 5:  # Show first 5 batches\n",
    "        break\n",
    "\n",
    "    image_batch = batch[\"image\"]\n",
    "    label_batch = batch[\"label\"]\n",
    "\n",
    "    # Compute per-channel statistics\n",
    "    batch_mean = image_batch.mean(axis=(0, 1, 2))\n",
    "    batch_std = image_batch.std(axis=(0, 1, 2))\n",
    "    all_means.append(batch_mean)\n",
    "    all_stds.append(batch_std)\n",
    "\n",
    "    if i < 3:  # Print details for first 3 batches\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  Image: shape={image_batch.shape}, dtype={image_batch.dtype}\")\n",
    "        print(f\"  Labels: {label_batch[:8]}... (first 8)\")\n",
    "        print(\n",
    "            f\"  Per-channel mean: [{batch_mean[0]:.3f}, {batch_mean[1]:.3f}, {batch_mean[2]:.3f}]\"\n",
    "        )\n",
    "\n",
    "# Expected output:\n",
    "# Batch 0:\n",
    "#   Image: shape=(32, 32, 32, 3), dtype=float32\n",
    "#   Labels: [6 9 9 4 1 1 2 7]... (first 8)\n",
    "#   Per-channel mean: [-0.012, 0.034, -0.089]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate statistics across batches\n",
    "import jax.numpy as jnp\n",
    "\n",
    "mean_of_means = jnp.stack(all_means).mean(axis=0)\n",
    "mean_of_stds = jnp.stack(all_stds).mean(axis=0)\n",
    "\n",
    "print(\"\\nAggregate Statistics (should be ~0 mean, ~1 std):\")\n",
    "print(\n",
    "    f\"  Mean across batches: [{mean_of_means[0]:.3f}, {mean_of_means[1]:.3f}, \"\n",
    "    f\"{mean_of_means[2]:.3f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"  Std across batches:  [{mean_of_stds[0]:.3f}, {mean_of_stds[1]:.3f}, {mean_of_stds[2]:.3f}]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Dataset** | CIFAR-10 (1000 samples for demo) |\n",
    "| **Image Shape** | (32, 32, 3) RGB |\n",
    "| **Batch Size** | 32 |\n",
    "| **Normalization** | Channel-wise with CIFAR-10 statistics |\n",
    "| **Output Range** | Approximately N(0, 1) per channel |\n",
    "\n",
    "### Data Format\n",
    "\n",
    "```\n",
    "batch = {\n",
    "    \"image\": Array[32, 32, 32, 3],  # (batch, height, width, channels)\n",
    "    \"label\": Array[32]               # (batch,) integer labels 0-9\n",
    "}\n",
    "```\n",
    "\n",
    "### Why Normalize?\n",
    "\n",
    "1. **Faster convergence**: Normalized inputs improve gradient flow\n",
    "2. **Compatibility**: Matches pretrained model expectations\n",
    "3. **Numerical stability**: Prevents overflow/underflow in deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Augmentation**: Add [image operators](05_augmentation_quickref.ipynb) for training\n",
    "- **Full training**: See [MNIST Tutorial](06_mnist_tutorial.ipynb) for complete workflow\n",
    "- **Advanced**: [MixUp/CutMix](../advanced/augmentation/01_mixup_cutmix_tutorial.ipynb)\n",
    "- **API Reference**: [TFDSEagerSource](https://datarax.readthedocs.io/sources/tfds/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the CIFAR-10 pipeline example.\"\"\"\n",
    "    print(\"CIFAR-10 Pipeline Quick Reference\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create source\n",
    "    config = TFDSEagerConfig(\n",
    "        name=\"cifar10\",\n",
    "        split=\"train[:500]\",\n",
    "        shuffle=True,\n",
    "        seed=42,  # Integer seed for Grain's index_shuffle\n",
    "        exclude_keys={\"id\"},  # Exclude non-numeric fields\n",
    "    )\n",
    "    source = TFDSEagerSource(config, rngs=nnx.Rngs(42))\n",
    "\n",
    "    # Create normalizer\n",
    "    normalizer = ElementOperator(\n",
    "        ElementOperatorConfig(stochastic=False),\n",
    "        fn=preprocess_cifar10,\n",
    "        rngs=nnx.Rngs(0),\n",
    "    )\n",
    "\n",
    "    # Build and run pipeline\n",
    "    pipeline = from_source(source, batch_size=32).add(OperatorNode(normalizer))\n",
    "\n",
    "    total_samples = 0\n",
    "    for batch in pipeline:\n",
    "        total_samples += batch[\"image\"].shape[0]\n",
    "        # Verify shape\n",
    "        assert batch[\"image\"].shape[1:] == (32, 32, 3), \"Unexpected image shape\"\n",
    "\n",
    "    print(f\"Processed {total_samples} CIFAR-10 samples\")\n",
    "    print(\"Image shape: (batch, 32, 32, 3)\")\n",
    "    print(\"Example completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
