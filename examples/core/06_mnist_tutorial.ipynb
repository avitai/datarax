{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# MNIST Classification Pipeline Tutorial\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Intermediate |\n",
    "| **Runtime** | ~30 min (CPU) / ~10 min (GPU) |\n",
    "| **Prerequisites** | Simple Pipeline, Operators Tutorial |\n",
    "| **Format** | Python + Jupyter |\n",
    "| **Memory** | ~1 GB RAM |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Build a complete MNIST classification pipeline from data loading to training.\n",
    "This tutorial demonstrates the full Datarax workflow with a Flax NNX model,\n",
    "covering data preprocessing, augmentation, training loop integration, and\n",
    "performance analysis.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. Create a complete training pipeline with TFDSSource\n",
    "2. Apply standard MNIST preprocessing and augmentation\n",
    "3. Integrate Datarax with Flax NNX training loops\n",
    "4. Handle epochs and shuffling correctly\n",
    "5. Generate visualizations of samples and training metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "# Install datarax with TFDS and Flax support\n",
    "uv pip install \"datarax[tfds]\" flax optax matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Memory Configuration\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES_FOR_TF\"] = \"\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "# Core imports\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "# Datarax imports\n",
    "from datarax import from_source\n",
    "from datarax.dag.nodes import OperatorNode\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.operators.modality.image import (\n",
    "    BrightnessOperator,\n",
    "    BrightnessOperatorConfig,\n",
    "    NoiseOperator,\n",
    "    NoiseOperatorConfig,\n",
    ")\n",
    "from datarax.sources import TFDSEagerConfig, TFDSEagerSource\n",
    "\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## MNIST Dataset Overview\n",
    "\n",
    "MNIST is the \"Hello World\" of machine learning - 70,000 grayscale images of\n",
    "handwritten digits (0-9).\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Image size | 28×28×1 (grayscale) |\n",
    "| Train samples | 60,000 |\n",
    "| Test samples | 10,000 |\n",
    "| Classes | 10 (digits 0-9) |\n",
    "| Pixel range | 0-255 (uint8) |\n",
    "\n",
    "### Standard Normalization\n",
    "\n",
    "| Statistic | Value |\n",
    "|-----------|-------|\n",
    "| Mean | 0.1307 |\n",
    "| Std | 0.3081 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST constants\n",
    "MNIST_MEAN = 0.1307\n",
    "MNIST_STD = 0.3081\n",
    "NUM_CLASSES = 10\n",
    "IMAGE_SHAPE = (28, 28, 1)\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 3  # Reduced for tutorial\n",
    "TRAIN_SAMPLES = 10000  # Subset for faster demo\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Training samples: {TRAIN_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 1: Data Loading and Preprocessing\n",
    "\n",
    "Create the MNIST data source and preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create MNIST training source\n",
    "train_config = TFDSEagerConfig(\n",
    "    name=\"mnist\",\n",
    "    split=f\"train[:{TRAIN_SAMPLES}]\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "train_source = TFDSEagerSource(train_config, rngs=nnx.Rngs(42))\n",
    "\n",
    "# Create test source (no shuffle)\n",
    "test_config = TFDSEagerConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"test[:2000]\",  # Subset for faster evaluation\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_source = TFDSEagerSource(test_config, rngs=nnx.Rngs(0))\n",
    "\n",
    "print(f\"Training samples: {len(train_source)}\")\n",
    "print(f\"Test samples: {len(test_source)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Preprocessing Function\n",
    "\n",
    "Standard MNIST preprocessing:\n",
    "1. Convert uint8 [0-255] to float32 [0-1]\n",
    "2. Normalize with MNIST statistics\n",
    "3. Ensure correct shape (add channel dim if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mnist(element, key=None):  # noqa: ARG001\n",
    "    \"\"\"Preprocess MNIST images with standard normalization.\"\"\"\n",
    "    del key  # Unused - deterministic operator\n",
    "    image = element.data[\"image\"]\n",
    "\n",
    "    # Convert to float32 and scale to [0, 1]\n",
    "    image = image.astype(jnp.float32) / 255.0\n",
    "\n",
    "    # Ensure channel dimension\n",
    "    if image.ndim == 2:\n",
    "        image = image[..., None]\n",
    "\n",
    "    # Apply MNIST normalization\n",
    "    image = (image - MNIST_MEAN) / MNIST_STD\n",
    "\n",
    "    # One-hot encode labels for cross-entropy\n",
    "    label = element.data[\"label\"]\n",
    "    label_onehot = jax.nn.one_hot(label, NUM_CLASSES)\n",
    "\n",
    "    return element.update_data({\"image\": image, \"label\": label, \"label_onehot\": label_onehot})\n",
    "\n",
    "\n",
    "preprocessor = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False),\n",
    "    fn=preprocess_mnist,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "print(\"Created MNIST preprocessor with one-hot encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Training Augmentation\n",
    "\n",
    "Light augmentation for training: brightness and noise.\n",
    "Test pipeline has no augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training augmentation operators\n",
    "brightness_aug = BrightnessOperator(\n",
    "    BrightnessOperatorConfig(\n",
    "        field_key=\"image\",\n",
    "        brightness_range=(-0.1, 0.1),\n",
    "        stochastic=True,\n",
    "        stream_name=\"brightness\",\n",
    "    ),\n",
    "    rngs=nnx.Rngs(brightness=100),\n",
    ")\n",
    "\n",
    "noise_aug = NoiseOperator(\n",
    "    NoiseOperatorConfig(\n",
    "        field_key=\"image\",\n",
    "        mode=\"gaussian\",\n",
    "        noise_std=0.1,\n",
    "        stochastic=True,\n",
    "        stream_name=\"noise\",\n",
    "    ),\n",
    "    rngs=nnx.Rngs(noise=200),\n",
    ")\n",
    "\n",
    "print(\"Created augmentation operators:\")\n",
    "print(\"  - Brightness: ±0.1\")\n",
    "print(\"  - Gaussian noise: std=0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Build Pipelines\n",
    "\n",
    "Training pipeline with augmentation, test pipeline without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training pipeline with augmentation\n",
    "train_pipeline = (\n",
    "    from_source(train_source, batch_size=BATCH_SIZE)\n",
    "    .add(OperatorNode(preprocessor))\n",
    "    .add(OperatorNode(brightness_aug))\n",
    "    .add(OperatorNode(noise_aug))\n",
    ")\n",
    "\n",
    "# Test pipeline without augmentation (create fresh sources for actual use)\n",
    "test_preprocessor = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False),\n",
    "    fn=preprocess_mnist,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "test_pipeline = from_source(test_source, batch_size=BATCH_SIZE).add(OperatorNode(test_preprocessor))\n",
    "\n",
    "print(\"Pipelines created:\")\n",
    "print(\"  Train: Source -> Preprocess -> Brightness -> Noise\")\n",
    "print(\"  Test:  Source -> Preprocess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 2: Visualize Sample Data\n",
    "\n",
    "Generate visualization of MNIST samples before and after augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample batch for visualization\n",
    "sample_batch = next(iter(train_pipeline))\n",
    "images = sample_batch[\"image\"]\n",
    "labels = sample_batch[\"label\"]\n",
    "\n",
    "print(f\"Sample batch shape: {images.shape}\")\n",
    "print(f\"Sample labels: {labels[:16]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_grid(images, labels, title, filename=None, nrows=4, ncols=4):\n",
    "    \"\"\"Plot a grid of MNIST images with labels.\"\"\"\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(8, 8))\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            # Denormalize for display\n",
    "            img = images[i] * MNIST_STD + MNIST_MEAN\n",
    "            img = np.clip(img, 0, 1)\n",
    "\n",
    "            # Remove channel dim for display\n",
    "            if img.ndim == 3:\n",
    "                img = img.squeeze(-1)\n",
    "\n",
    "            ax.imshow(img, cmap=\"gray\")\n",
    "            ax.set_title(f\"Label: {int(labels[i])}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        plt.savefig(filename, dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Generate sample grid\n",
    "output_dir = Path(\"docs/assets/images/examples\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plot_mnist_grid(\n",
    "    np.array(images[:16]),\n",
    "    np.array(labels[:16]),\n",
    "    \"MNIST Training Samples (with augmentation)\",\n",
    "    output_dir / \"cv-mnist-sample-grid.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 3: Define the Model\n",
    "\n",
    "Simple CNN for MNIST classification using Flax NNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nnx.Module):\n",
    "    \"\"\"Simple CNN for MNIST classification.\"\"\"\n",
    "\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), padding=\"SAME\", rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), padding=\"SAME\", rngs=rngs)\n",
    "\n",
    "        # Dense layers\n",
    "        self.dense1 = nnx.Linear(64 * 7 * 7, 128, rngs=rngs)\n",
    "        self.dense2 = nnx.Linear(128, NUM_CLASSES, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        # Conv block 1: Conv -> ReLU -> MaxPool\n",
    "        x = self.conv1(x)\n",
    "        x = nnx.relu(x)\n",
    "        x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "        # Conv block 2: Conv -> ReLU -> MaxPool\n",
    "        x = self.conv2(x)\n",
    "        x = nnx.relu(x)\n",
    "        x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "\n",
    "        # Flatten and dense layers\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dense1(x)\n",
    "        x = nnx.relu(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = MNISTClassifier(rngs=nnx.Rngs(0))\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = jnp.ones((1, 28, 28, 1))\n",
    "dummy_output = model(dummy_input)\n",
    "print(f\"Model output shape: {dummy_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 4: Training Loop\n",
    "\n",
    "Implement training with Datarax pipeline integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = nnx.Optimizer(model, optax.adam(LEARNING_RATE), wrt=nnx.Param)\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: MNISTClassifier, optimizer: nnx.Optimizer, batch: dict) -> jax.Array:\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label_onehot\"]\n",
    "\n",
    "    def loss_fn(model):\n",
    "        logits = model(images)\n",
    "        loss = optax.softmax_cross_entropy(logits, labels).mean()\n",
    "        return loss\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(model, grads)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model: MNISTClassifier, batch: dict) -> tuple[jax.Array, jax.Array]:\n",
    "    \"\"\"Single evaluation step.\"\"\"\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "\n",
    "    logits = model(images)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    correct = (predictions == labels).sum()\n",
    "\n",
    "    return correct, len(labels)\n",
    "\n",
    "\n",
    "print(\"Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Training Loop with Metrics Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics storage\n",
    "train_losses = []\n",
    "train_times = []\n",
    "batch_throughputs = []\n",
    "\n",
    "\n",
    "def create_train_pipeline():\n",
    "    \"\"\"Create a fresh training pipeline for each epoch.\"\"\"\n",
    "    source = TFDSEagerSource(train_config, rngs=nnx.Rngs(42))\n",
    "\n",
    "    preprocessor = ElementOperator(\n",
    "        ElementOperatorConfig(stochastic=False),\n",
    "        fn=preprocess_mnist,\n",
    "        rngs=nnx.Rngs(0),\n",
    "    )\n",
    "\n",
    "    brightness = BrightnessOperator(\n",
    "        BrightnessOperatorConfig(\n",
    "            field_key=\"image\",\n",
    "            brightness_range=(-0.1, 0.1),\n",
    "            stochastic=True,\n",
    "            stream_name=\"brightness\",\n",
    "        ),\n",
    "        rngs=nnx.Rngs(brightness=100),\n",
    "    )\n",
    "\n",
    "    noise = NoiseOperator(\n",
    "        NoiseOperatorConfig(\n",
    "            field_key=\"image\",\n",
    "            mode=\"gaussian\",\n",
    "            noise_std=0.1,\n",
    "            stochastic=True,\n",
    "            stream_name=\"noise\",\n",
    "        ),\n",
    "        rngs=nnx.Rngs(noise=200),\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        from_source(source, batch_size=BATCH_SIZE)\n",
    "        .add(OperatorNode(preprocessor))\n",
    "        .add(OperatorNode(brightness))\n",
    "        .add(OperatorNode(noise))\n",
    "    )\n",
    "\n",
    "\n",
    "def create_test_pipeline():\n",
    "    \"\"\"Create a fresh test pipeline.\"\"\"\n",
    "    source = TFDSEagerSource(test_config, rngs=nnx.Rngs(0))\n",
    "\n",
    "    preprocessor = ElementOperator(\n",
    "        ElementOperatorConfig(stochastic=False),\n",
    "        fn=preprocess_mnist,\n",
    "        rngs=nnx.Rngs(0),\n",
    "    )\n",
    "\n",
    "    return from_source(source, batch_size=BATCH_SIZE).add(OperatorNode(preprocessor))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Create fresh pipeline for this epoch\n",
    "    pipeline = create_train_pipeline()\n",
    "\n",
    "    for batch_idx, batch in enumerate(pipeline):\n",
    "        batch_start = time.time()\n",
    "\n",
    "        # Training step\n",
    "        loss = train_step(model, optimizer, batch)\n",
    "        epoch_losses.append(float(loss))\n",
    "\n",
    "        # Track throughput\n",
    "        batch_time = time.time() - batch_start\n",
    "        throughput = BATCH_SIZE / batch_time if batch_time > 0 else 0\n",
    "        batch_throughputs.append(throughput)\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  Epoch {epoch + 1}, Batch {batch_idx}: loss={float(loss):.4f}\")\n",
    "\n",
    "    # Epoch summary\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    train_losses.extend(epoch_losses)\n",
    "    train_times.append(epoch_time)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_pipeline = create_test_pipeline()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in test_pipeline:\n",
    "        correct, n = eval_step(model, batch)\n",
    "        total_correct += int(correct)\n",
    "        total_samples += int(n)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Train loss: {epoch_loss:.4f}\")\n",
    "    print(f\"  Test accuracy: {accuracy:.2%}\")\n",
    "    print(f\"  Time: {epoch_time:.1f}s\")\n",
    "    print()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 5: Generate Visualizations\n",
    "\n",
    "Create plots for training metrics and pipeline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Training Loss Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, alpha=0.7, linewidth=0.5)\n",
    "\n",
    "# Add smoothed line\n",
    "window = min(20, len(train_losses) // 5)\n",
    "if window > 1:\n",
    "    smoothed = np.convolve(train_losses, np.ones(window) / window, mode=\"valid\")\n",
    "    plt.plot(range(window - 1, len(train_losses)), smoothed, linewidth=2, label=\"Smoothed\")\n",
    "\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"MNIST Training Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\n",
    "    output_dir / \"cv-mnist-training-loss.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\"\n",
    ")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'cv-mnist-training-loss.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Throughput Analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batch_throughputs, alpha=0.5)\n",
    "avg_throughput = np.mean(batch_throughputs)\n",
    "plt.axhline(\n",
    "    y=avg_throughput, color=\"r\", linestyle=\"--\", label=f\"Average: {avg_throughput:.0f} samples/s\"\n",
    ")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Throughput (samples/second)\")\n",
    "plt.title(\"Pipeline Throughput During Training\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(output_dir / \"cv-mnist-throughput.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'cv-mnist-throughput.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Augmentation Comparison\n",
    "# Get samples without augmentation for comparison\n",
    "plain_source = TFDSEagerSource(\n",
    "    TFDSEagerConfig(name=\"mnist\", split=\"train[:128]\", shuffle=False),\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "plain_preprocessor = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False),\n",
    "    fn=preprocess_mnist,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "plain_pipeline = from_source(plain_source, batch_size=128).add(OperatorNode(plain_preprocessor))\n",
    "plain_batch = next(iter(plain_pipeline))\n",
    "\n",
    "# Get augmented samples\n",
    "aug_pipeline = create_train_pipeline()\n",
    "aug_batch = next(iter(aug_pipeline))\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle(\"Original vs Augmented MNIST Samples\", fontsize=14)\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    img_orig = np.array(plain_batch[\"image\"][i]) * MNIST_STD + MNIST_MEAN\n",
    "    img_orig = np.clip(img_orig, 0, 1).squeeze()\n",
    "    axes[0, i].imshow(img_orig, cmap=\"gray\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel(\"Original\", fontsize=12)\n",
    "\n",
    "    # Augmented\n",
    "    img_aug = np.array(aug_batch[\"image\"][i]) * MNIST_STD + MNIST_MEAN\n",
    "    img_aug = np.clip(img_aug, 0, 1).squeeze()\n",
    "    axes[1, i].imshow(img_aug, cmap=\"gray\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel(\"Augmented\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    output_dir / \"cv-mnist-augmentation-samples.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"white\",\n",
    ")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'cv-mnist-augmentation-samples.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Final Test Accuracy | ~95%+ |\n",
    "| Average Throughput | ~5000 samples/s (CPU) |\n",
    "| Training Time | ~30s per epoch |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Pipeline Integration**: Datarax integrates seamlessly with Flax NNX training loops\n",
    "2. **Fresh Pipelines**: Create new pipeline instances for each epoch to reset iteration\n",
    "3. **Augmentation**: Light augmentation (brightness, noise) improves generalization\n",
    "4. **Preprocessing**: Always normalize with dataset-specific statistics\n",
    "5. **Batching**: `from_source(batch_size=N)` handles batching automatically\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "Training:\n",
    "TFDSEagerSource -> Preprocess -> Brightness -> Noise -> Model\n",
    "\n",
    "Testing:\n",
    "TFDSEagerSource -> Preprocess -> Model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Stronger augmentation**: [Fashion-MNIST tutorial](07_fashion_augmentation_tutorial.ipynb)\n",
    "- **MixUp/CutMix**: [Batch augmentation](../advanced/augmentation/01_mixup_cutmix_tutorial.ipynb)\n",
    "- **Distributed training**: [Sharding guide](../advanced/distributed/02_sharding_guide.ipynb)\n",
    "- **Checkpointing**: See advanced/checkpointing for resumable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the complete MNIST tutorial.\"\"\"\n",
    "    print(\"MNIST Classification Pipeline Tutorial\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Quick training run\n",
    "    model = MNISTClassifier(rngs=nnx.Rngs(0))\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n",
    "\n",
    "    # Single epoch\n",
    "    pipeline = create_train_pipeline()\n",
    "    for batch_idx, batch in enumerate(pipeline):\n",
    "        _ = train_step(model, optimizer, batch)\n",
    "        if batch_idx >= 10:\n",
    "            break\n",
    "\n",
    "    # Evaluate\n",
    "    test_pipeline = create_test_pipeline()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in test_pipeline:\n",
    "        correct, n = eval_step(model, batch)\n",
    "        total_correct += int(correct)\n",
    "        total_samples += int(n)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test accuracy after quick training: {accuracy:.2%}\")\n",
    "    print(\"Tutorial completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
