{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Comparison: Stateful vs Stateless Transformation Pipelines.\n",
    "\n",
    "This example demonstrates the fundamental differences in transformation\n",
    "handling between datarax's stateful NNX approach and Grain's\n",
    "stateless design, with real performance measurements.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import flax.nnx as nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "============================================================================\n",
    "GRAIN (STATELESS) TRANSFORMATIONS - Following Best Practices\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GrainNormalizeTransform:\n",
    "    \"\"\"Grain-style stateless normalization.\"\"\"\n",
    "\n",
    "    mean: float = 0.0\n",
    "    std: float = 1.0\n",
    "\n",
    "    def __call__(self, features: dict, rng: np.random.RandomState) -> dict:\n",
    "        \"\"\"Apply normalization without state.\"\"\"\n",
    "        if \"data\" in features:\n",
    "            # Stateless - can't track running statistics\n",
    "            features[\"data\"] = (features[\"data\"] - self.mean) / self.std\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GrainAugmentationTransform:\n",
    "    \"\"\"Grain-style random augmentation.\"\"\"\n",
    "\n",
    "    noise_scale: float = 0.1\n",
    "\n",
    "    def __call__(self, features: dict, rng: np.random.RandomState) -> dict:\n",
    "        \"\"\"Apply augmentation with external RNG.\"\"\"\n",
    "        if \"data\" in features:\n",
    "            noise = rng.randn(*features[\"data\"].shape) * self.noise_scale\n",
    "            features[\"data\"] = features[\"data\"] + noise\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GrainBatchTransform:\n",
    "    \"\"\"Grain-style batch processing.\"\"\"\n",
    "\n",
    "    batch_size: int = 32\n",
    "\n",
    "    def __call__(self, batch: list[dict]) -> np.ndarray:\n",
    "        \"\"\"Process a batch of samples.\"\"\"\n",
    "        # Stack features\n",
    "        stacked = np.stack([item[\"data\"] for item in batch])\n",
    "        return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class GrainTransformPipeline:\n",
    "    \"\"\"Grain-style transformation pipeline with external state.\"\"\"\n",
    "\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "        # External state dictionary for any stateful operations\n",
    "        self.state = {\n",
    "            \"samples_processed\": 0,\n",
    "            \"batch_count\": 0,\n",
    "            \"running_mean\": 0.0,\n",
    "            \"running_var\": 1.0,\n",
    "            \"rng_seed\": 42,\n",
    "        }\n",
    "\n",
    "    def apply_transforms(self, data: dict, state: dict) -> tuple[dict, dict]:\n",
    "        \"\"\"Apply transforms with external state management.\"\"\"\n",
    "        # Create RNG from state\n",
    "        rng = np.random.RandomState(state[\"rng_seed\"])\n",
    "\n",
    "        # Apply each transform (skip batch transforms for individual samples)\n",
    "        for transform in self.transforms:\n",
    "            if hasattr(transform, \"__call__\") and not isinstance(transform, GrainBatchTransform):\n",
    "                data = transform(data, rng)\n",
    "\n",
    "        # Manual state update\n",
    "        new_state = state.copy()\n",
    "        new_state[\"samples_processed\"] += 1\n",
    "        new_state[\"rng_seed\"] += 1\n",
    "\n",
    "        # Manual running statistics (complex!)\n",
    "        if \"data\" in data:\n",
    "            batch_mean = float(np.mean(data[\"data\"]))\n",
    "            float(np.var(data[\"data\"]))\n",
    "\n",
    "            # Welford's online algorithm for running stats\n",
    "            n = new_state[\"samples_processed\"]\n",
    "            delta = batch_mean - new_state[\"running_mean\"]\n",
    "            new_state[\"running_mean\"] += delta / n\n",
    "            new_state[\"running_var\"] = (\n",
    "                (n - 1) * new_state[\"running_var\"]\n",
    "                + delta * (batch_mean - new_state[\"running_mean\"])\n",
    "            ) / n\n",
    "\n",
    "        return data, new_state\n",
    "\n",
    "    def process_batch(self, batch: list[dict], state: dict) -> tuple[np.ndarray, dict]:\n",
    "        \"\"\"Process a batch with state threading.\"\"\"\n",
    "        processed = []\n",
    "        current_state = state.copy()\n",
    "\n",
    "        for sample in batch:\n",
    "            transformed, current_state = self.apply_transforms(sample, current_state)\n",
    "            processed.append(transformed)\n",
    "\n",
    "        # Update batch count\n",
    "        current_state[\"batch_count\"] += 1\n",
    "\n",
    "        # Stack into batch\n",
    "        if processed and \"data\" in processed[0]:\n",
    "            batch_array = np.stack([p[\"data\"] for p in processed])\n",
    "            return batch_array, current_state\n",
    "\n",
    "        return np.array([]), current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "============================================================================\n",
    "DATARAX (STATEFUL) TRANSFORMATIONS with NNX\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulNormalizeTransform(nnx.Module):\n",
    "    \"\"\"Stateful normalization with running statistics.\"\"\"\n",
    "\n",
    "    def __init__(self, momentum: float = 0.1):\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Automatic state tracking with NNX\n",
    "        self.running_mean = nnx.BatchStat(0.0)\n",
    "        self.running_var = nnx.BatchStat(1.0)\n",
    "        self.samples_seen = nnx.Variable(0)\n",
    "\n",
    "    def __call__(self, features: dict, training: bool = True) -> dict:\n",
    "        \"\"\"Apply normalization with automatic state updates.\"\"\"\n",
    "        if \"data\" not in features:\n",
    "            return features\n",
    "\n",
    "        data = features[\"data\"]\n",
    "\n",
    "        sample_mean = jnp.mean(data)\n",
    "        sample_var = jnp.var(data)\n",
    "\n",
    "        if training:\n",
    "            # Update running statistics automatically\n",
    "            self.running_mean.value = float(\n",
    "                self.momentum * sample_mean + (1 - self.momentum) * self.running_mean.value\n",
    "            )\n",
    "            self.running_var.value = float(\n",
    "                self.momentum * sample_var + (1 - self.momentum) * self.running_var.value\n",
    "            )\n",
    "\n",
    "            # Use sample stats for normalization\n",
    "            normalized = (data - sample_mean) / jnp.sqrt(sample_var + 1e-8)\n",
    "        else:\n",
    "            # Use running stats\n",
    "            normalized = (data - self.running_mean.value) / jnp.sqrt(self.running_var.value + 1e-8)\n",
    "\n",
    "        self.samples_seen.value += 1\n",
    "        features[\"data\"] = normalized\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def stats(self) -> dict:\n",
    "        \"\"\"Get current statistics.\"\"\"\n",
    "        return {\n",
    "            \"running_mean\": float(self.running_mean.value),\n",
    "            \"running_var\": float(self.running_var.value),\n",
    "            \"samples_seen\": int(self.samples_seen.value),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulAugmentationTransform(nnx.Module):\n",
    "    \"\"\"Stateful augmentation with internal RNG.\"\"\"\n",
    "\n",
    "    def __init__(self, noise_scale: float = 0.1):\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "        # Internal RNG management\n",
    "        self.rngs = nnx.Rngs(42)\n",
    "        self.augment_count = nnx.Variable(0)\n",
    "\n",
    "    def __call__(self, features: dict) -> dict:\n",
    "        \"\"\"Apply augmentation with automatic RNG handling.\"\"\"\n",
    "        if \"data\" not in features:\n",
    "            return features\n",
    "\n",
    "        # Automatic RNG management\n",
    "        key = self.rngs.noise()\n",
    "        noise = jax.random.normal(key, features[\"data\"].shape) * self.noise_scale\n",
    "\n",
    "        features[\"data\"] = features[\"data\"] + noise\n",
    "        self.augment_count.value += 1\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableTransform(nnx.Module):\n",
    "    \"\"\"Learnable transformation - only possible with stateful approach.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        # Learnable parameters\n",
    "        key = jax.random.key(0)\n",
    "        self.weight = nnx.Param(jax.random.normal(key, (input_dim, output_dim)) * 0.01)\n",
    "        self.bias = nnx.Param(jnp.zeros(output_dim))\n",
    "\n",
    "        # Batch normalization components\n",
    "        self.bn_scale = nnx.Param(jnp.ones(output_dim))\n",
    "        self.bn_shift = nnx.Param(jnp.zeros(output_dim))\n",
    "        self.bn_mean = nnx.BatchStat(jnp.zeros(output_dim))\n",
    "        self.bn_var = nnx.BatchStat(jnp.ones(output_dim))\n",
    "\n",
    "        # Track usage\n",
    "        self.forward_passes = nnx.Variable(0)\n",
    "\n",
    "    def __call__(self, features: dict, training: bool = True) -> dict:\n",
    "        \"\"\"Apply learnable transformation.\"\"\"\n",
    "        if \"data\" not in features:\n",
    "            return features\n",
    "\n",
    "        # Ensure correct shape\n",
    "        data = features[\"data\"]\n",
    "\n",
    "        # Handle different data shapes\n",
    "        if data.ndim == 1:\n",
    "            # Single sample - reshape to (1, features)\n",
    "            data = data.reshape(1, -1)\n",
    "            expected_features = data.shape[1]\n",
    "        elif data.ndim == 2:\n",
    "            # Batch - check if features match\n",
    "            expected_features = data.shape[1]\n",
    "        else:\n",
    "            # Flatten if needed\n",
    "            data = data.reshape(1, -1)\n",
    "            expected_features = data.shape[1]\n",
    "\n",
    "        # Check if input dimension matches expected\n",
    "        if expected_features != self.weight.value.shape[0]:\n",
    "            # Reshape weight to match input features\n",
    "            actual_features = self.weight.value.shape[0]\n",
    "            if expected_features < actual_features:\n",
    "                # Pad with zeros if input is smaller\n",
    "                padding = actual_features - expected_features\n",
    "                data = jnp.pad(data, ((0, 0), (0, padding)), mode=\"constant\")\n",
    "            elif expected_features > actual_features:\n",
    "                # Truncate if input is larger\n",
    "                data = data[:, :actual_features]\n",
    "            expected_features = self.weight.value.shape[0]\n",
    "\n",
    "        # Linear transformation\n",
    "        output = jnp.dot(data, self.weight.value) + self.bias.value\n",
    "\n",
    "        # Batch normalization\n",
    "        if training:\n",
    "            batch_mean = jnp.mean(output, axis=0, keepdims=True)\n",
    "            batch_var = jnp.var(output, axis=0, keepdims=True)\n",
    "\n",
    "            # Update running stats\n",
    "            self.bn_mean.value = 0.9 * self.bn_mean.value + 0.1 * batch_mean.squeeze()\n",
    "            self.bn_var.value = 0.9 * self.bn_var.value + 0.1 * batch_var.squeeze()\n",
    "\n",
    "            # Normalize\n",
    "            output = (output - batch_mean) / jnp.sqrt(batch_var + 1e-8)\n",
    "        else:\n",
    "            output = (output - self.bn_mean.value) / jnp.sqrt(self.bn_var.value + 1e-8)\n",
    "\n",
    "        output = output * self.bn_scale.value + self.bn_shift.value\n",
    "        output = jax.nn.relu(output)\n",
    "\n",
    "        self.forward_passes.value += 1\n",
    "\n",
    "        features[\"data\"] = output.squeeze() if output.shape[0] == 1 else output\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def num_parameters(self) -> int:\n",
    "        \"\"\"Count learnable parameters.\"\"\"\n",
    "        return (\n",
    "            self.weight.value.size\n",
    "            + self.bias.value.size\n",
    "            + self.bn_scale.value.size\n",
    "            + self.bn_shift.value.size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class StatefulTransformPipeline(nnx.Module):\n",
    "    \"\"\"Stateful transformation pipeline with automatic state management.\"\"\"\n",
    "\n",
    "    def __init__(self, transforms: list[nnx.Module]):\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Pipeline statistics\n",
    "        self.samples_processed = nnx.Variable(0)\n",
    "        self.batch_count = nnx.Variable(0)\n",
    "        self.total_time = nnx.Variable(0.0)\n",
    "\n",
    "    def __call__(self, batch: list[dict], training: bool = True) -> jax.Array:\n",
    "        \"\"\"Process batch with automatic state updates.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        processed = []\n",
    "        for sample in batch:\n",
    "            # Apply transforms in sequence\n",
    "            for transform in self.transforms:\n",
    "                if hasattr(transform, \"__call__\") and callable(transform):\n",
    "                    if isinstance(transform, StatefulNormalizeTransform | LearnableTransform):\n",
    "                        sample = transform(sample, training=training)\n",
    "                    else:\n",
    "                        sample = transform(sample)\n",
    "\n",
    "            processed.append(sample[\"data\"])\n",
    "\n",
    "        # Update statistics\n",
    "        self.samples_processed.value += len(batch)\n",
    "        self.batch_count.value += 1\n",
    "        self.total_time.value += time.time() - start_time\n",
    "\n",
    "        # Stack into batch\n",
    "        return jnp.stack(processed)\n",
    "\n",
    "    @property\n",
    "    def stats(self) -> dict:\n",
    "        \"\"\"Get pipeline statistics.\"\"\"\n",
    "        stats = {\n",
    "            \"samples_processed\": int(self.samples_processed.value),\n",
    "            \"batch_count\": int(self.batch_count.value),\n",
    "            \"avg_time_per_batch\": float(self.total_time.value / max(self.batch_count.value, 1)),\n",
    "        }\n",
    "\n",
    "        # Add transform-specific stats\n",
    "        for i, transform in enumerate(self.transforms):\n",
    "            if hasattr(transform, \"stats\"):\n",
    "                stats_attr = getattr(transform, \"stats\", None)\n",
    "                if stats_attr is not None:\n",
    "                    if callable(stats_attr):\n",
    "                        stats[f\"transform_{i}\"] = stats_attr()\n",
    "                    else:\n",
    "                        stats[f\"transform_{i}\"] = stats_attr\n",
    "            elif hasattr(transform, \"num_parameters\"):\n",
    "                num_params_attr = getattr(transform, \"num_parameters\", None)\n",
    "                if num_params_attr is not None:\n",
    "                    if callable(num_params_attr):\n",
    "                        stats[f\"transform_{i}_params\"] = num_params_attr()\n",
    "                    else:\n",
    "                        stats[f\"transform_{i}_params\"] = num_params_attr\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "============================================================================\n",
    "PRESSURE TESTING FUNCTIONS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(num_samples: int, feature_dim: int) -> list[dict]:\n",
    "    \"\"\"Create test data for transformation pipelines.\"\"\"\n",
    "    data = []\n",
    "    for i in range(num_samples):\n",
    "        data.append(\n",
    "            {\n",
    "                \"data\": np.random.randn(feature_dim).astype(np.float32),\n",
    "                \"label\": np.random.randint(0, 10),\n",
    "            }\n",
    "        )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_transform_performance(\n",
    "    pipeline_name: str,\n",
    "    pipeline: Any,\n",
    "    data: list[dict],\n",
    "    batch_size: int = 32,\n",
    "    is_stateful: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"Measure transformation pipeline performance.\"\"\"\n",
    "\n",
    "    print(f\"\\n{pipeline_name} Performance Test:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    num_batches = len(data) // batch_size\n",
    "    times = []\n",
    "    memory_before = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "\n",
    "    # Initialize state variable for type checker\n",
    "    state = {\"samples_processed\": 0}\n",
    "\n",
    "    # Process batches\n",
    "    if is_stateful:\n",
    "        # Warmup\n",
    "        for i in range(min(5, num_batches)):\n",
    "            batch = data[i * batch_size : (i + 1) * batch_size]\n",
    "            _ = pipeline(batch, training=True)\n",
    "\n",
    "        # Measure\n",
    "        gc.collect()\n",
    "        for i in range(num_batches):\n",
    "            batch = data[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            start = time.time()\n",
    "            result = pipeline(batch, training=True)\n",
    "            elapsed = time.time() - start\n",
    "\n",
    "            times.append(elapsed)\n",
    "\n",
    "            # Force computation\n",
    "            _ = jnp.mean(result)\n",
    "    else:\n",
    "        # Grain-style with state threading\n",
    "        state = pipeline.state.copy()\n",
    "\n",
    "        # Warmup\n",
    "        for i in range(min(5, num_batches)):\n",
    "            batch = data[i * batch_size : (i + 1) * batch_size]\n",
    "            _, state = pipeline.process_batch(batch, state)\n",
    "\n",
    "        # Measure\n",
    "        gc.collect()\n",
    "        for i in range(num_batches):\n",
    "            batch = data[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            start = time.time()\n",
    "            result, state = pipeline.process_batch(batch, state)\n",
    "            elapsed = time.time() - start\n",
    "\n",
    "            times.append(elapsed)\n",
    "\n",
    "            # Force computation\n",
    "            _ = np.mean(result)\n",
    "\n",
    "    memory_after = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "\n",
    "    # Calculate statistics\n",
    "    times = np.array(times[5:])  # Skip warmup\n",
    "\n",
    "    results = {\n",
    "        \"mean_time_ms\": np.mean(times) * 1000,\n",
    "        \"std_time_ms\": np.std(times) * 1000,\n",
    "        \"p50_time_ms\": np.percentile(times, 50) * 1000,\n",
    "        \"p95_time_ms\": np.percentile(times, 95) * 1000,\n",
    "        \"p99_time_ms\": np.percentile(times, 99) * 1000,\n",
    "        \"total_batches\": len(times),\n",
    "        \"memory_delta_mb\": memory_after - memory_before,\n",
    "        \"throughput_batches_per_sec\": 1 / np.mean(times),\n",
    "    }\n",
    "\n",
    "    print(f\"  Mean batch time: {results['mean_time_ms']:.3f} ms\")\n",
    "    print(f\"  Std deviation: {results['std_time_ms']:.3f} ms\")\n",
    "    print(f\"  P95: {results['p95_time_ms']:.3f} ms\")\n",
    "    print(f\"  Throughput: {results['throughput_batches_per_sec']:.1f} batches/sec\")\n",
    "    print(f\"  Memory delta: {results['memory_delta_mb']:.1f} MB\")\n",
    "\n",
    "    if is_stateful:\n",
    "        stats = pipeline.stats\n",
    "        print(f\"  Pipeline stats: {stats.get('samples_processed', 'N/A')} samples processed\")\n",
    "    else:\n",
    "        print(f\"  State management: Manual ({state['samples_processed']} samples)\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_learnable_transforms():\n",
    "    \"\"\"Demonstrate learnable transformations (only possible with stateful).\"\"\"\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LEARNABLE TRANSFORMATIONS (Datarax Exclusive)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create learnable pipeline\n",
    "    input_dim = 784\n",
    "    hidden_dim = 256\n",
    "    output_dim = 128\n",
    "\n",
    "    pipeline = StatefulTransformPipeline(\n",
    "        [\n",
    "            StatefulNormalizeTransform(),\n",
    "            LearnableTransform(input_dim, hidden_dim),\n",
    "            LearnableTransform(hidden_dim, output_dim),\n",
    "            StatefulAugmentationTransform(noise_scale=0.05),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"\\nLearnable Pipeline Configuration:\")\n",
    "    print(f\"  Input dimension: {input_dim}\")\n",
    "    print(f\"  Hidden dimension: {hidden_dim}\")\n",
    "    print(f\"  Output dimension: {output_dim}\")\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = 0\n",
    "    for transform in pipeline.transforms:\n",
    "        if hasattr(transform, \"num_parameters\"):\n",
    "            num_params_attr = getattr(transform, \"num_parameters\", None)\n",
    "            if num_params_attr is not None:\n",
    "                if callable(num_params_attr):\n",
    "                    params_val = num_params_attr()\n",
    "                else:\n",
    "                    params_val = num_params_attr\n",
    "\n",
    "                # Ensure it's a valid number\n",
    "                if isinstance(params_val, int | float | np.integer | np.floating):\n",
    "                    params = int(params_val)\n",
    "                    total_params += params\n",
    "                else:\n",
    "                    params = 0\n",
    "                print(f\"  {transform.__class__.__name__}: {params:,} parameters\")\n",
    "\n",
    "    print(f\"  Total learnable parameters: {total_params:,}\")\n",
    "\n",
    "    # Create optimizer for learnable parameters\n",
    "    optimizer = nnx.Optimizer(pipeline, optax.adam(1e-3), wrt=nnx.Param)\n",
    "\n",
    "    # Training step function\n",
    "    @nnx.jit\n",
    "    def train_step(pipeline, batch, optimizer):\n",
    "        def loss_fn(pipeline):\n",
    "            output = pipeline(batch, training=True)\n",
    "            return jnp.mean(output**2)  # Dummy loss\n",
    "\n",
    "        loss, grads = nnx.value_and_grad(loss_fn)(pipeline)\n",
    "        optimizer.update(grads)\n",
    "        return loss\n",
    "\n",
    "    # Simulate training\n",
    "    print(\"\\nTraining Learnable Pipeline:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    data = create_test_data(1000, input_dim)\n",
    "    batch_size = 32\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(3):\n",
    "        epoch_losses = []\n",
    "\n",
    "        for i in range(0, len(data) - batch_size, batch_size):\n",
    "            batch = data[i : i + batch_size]\n",
    "            loss = train_step(pipeline, batch, optimizer)\n",
    "            epoch_losses.append(float(loss))\n",
    "\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"  Epoch {epoch + 1}: loss = {avg_loss:.4f}\")\n",
    "\n",
    "    # Show learning occurred\n",
    "    improvement = (losses[0] - losses[-1]) / losses[0] * 100\n",
    "    print(f\"\\nLoss reduction: {improvement:.1f}%\")\n",
    "    print(\"✓ Learnable transformations successfully trained!\")\n",
    "\n",
    "    print(\"\\nGrain equivalent:\")\n",
    "    print(\"  ❌ Not possible - Grain transformations must be stateless\")\n",
    "    print(\"  ❌ Cannot have learnable parameters\")\n",
    "    print(\"  ❌ Cannot use automatic differentiation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_checkpoint_complexity():\n",
    "    \"\"\"Compare checkpoint complexity between approaches.\"\"\"\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CHECKPOINT COMPLEXITY COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Create pipelines\n",
    "    grain_pipeline = GrainTransformPipeline(\n",
    "        [GrainNormalizeTransform(), GrainAugmentationTransform(), GrainBatchTransform()]\n",
    "    )\n",
    "\n",
    "    workshop_pipeline = StatefulTransformPipeline(\n",
    "        [\n",
    "            StatefulNormalizeTransform(),\n",
    "            StatefulAugmentationTransform(noise_scale=0.1),\n",
    "            LearnableTransform(100, 50),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Process some data to accumulate state\n",
    "    data = create_test_data(500, 100)\n",
    "\n",
    "    # Grain processing\n",
    "    grain_state = grain_pipeline.state.copy()\n",
    "    for i in range(10):\n",
    "        batch = data[i * 32 : (i + 1) * 32]\n",
    "        _, grain_state = grain_pipeline.process_batch(batch, grain_state)\n",
    "\n",
    "    # Workshop processing\n",
    "    for i in range(10):\n",
    "        batch = data[i * 32 : (i + 1) * 32]\n",
    "        _ = workshop_pipeline(batch)\n",
    "\n",
    "    print(\"\\n1. GRAIN CHECKPOINT COMPLEXITY:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"  Manual state collection required:\")\n",
    "    print(f\"    - Pipeline state dict: {len(grain_state)} entries\")\n",
    "    print(\"    - Each transform state: Manual tracking\")\n",
    "    print(\"    - RNG states: External management\")\n",
    "    print(\"    - No automatic discovery\")\n",
    "\n",
    "    print(\"\\n2. DATARAX CHECKPOINT COMPLEXITY:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Get all state automatically\n",
    "    graphdef, state = nnx.split(workshop_pipeline)\n",
    "    state_leaves = jax.tree.leaves(state)\n",
    "\n",
    "    print(\"  Automatic state collection with NNX:\")\n",
    "    print(f\"    - Total state variables: {len(state_leaves)}\")\n",
    "    print(\"    - Includes all nested module states\")\n",
    "    print(\"    - Automatic parameter discovery\")\n",
    "    print(\"    - One-line checkpoint: nnx.split()\")\n",
    "\n",
    "    # Show state structure\n",
    "    print(\"\\n  State structure (automatic):\")\n",
    "    for transform in workshop_pipeline.transforms:\n",
    "        if hasattr(transform, \"__class__\"):\n",
    "            print(f\"    - {transform.__class__.__name__}: Fully tracked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_transform_comparison():\n",
    "    \"\"\"Run full transformation comparison.\"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FULL TRANSFORMATION PIPELINE COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    test_configs = [\n",
    "        {\"samples\": 5000, \"features\": 784, \"batch\": 32, \"desc\": \"Small (MNIST)\"},\n",
    "        {\"samples\": 10000, \"features\": 2048, \"batch\": 64, \"desc\": \"Medium (ResNet)\"},\n",
    "        {\"samples\": 20000, \"features\": 4096, \"batch\": 128, \"desc\": \"Large (ViT)\"},\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for config in test_configs:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"TEST: {config['desc']}\")\n",
    "        print(\n",
    "            f\"Samples: {config['samples']:,}, Features: {config['features']}, \"\n",
    "            f\"Batch: {config['batch']}\"\n",
    "        )\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Create test data\n",
    "        data = create_test_data(config[\"samples\"], config[\"features\"])\n",
    "\n",
    "        # Create pipelines\n",
    "        grain_pipeline = GrainTransformPipeline(\n",
    "            [\n",
    "                GrainNormalizeTransform(mean=0.5, std=0.5),\n",
    "                GrainAugmentationTransform(noise_scale=0.1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        workshop_pipeline = StatefulTransformPipeline(\n",
    "            [\n",
    "                StatefulNormalizeTransform(momentum=0.1),\n",
    "                StatefulAugmentationTransform(noise_scale=0.1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # If features match, add learnable transform\n",
    "        if config[\"features\"] in [784, 2048, 4096]:\n",
    "            output_dim = config[\"features\"] // 8\n",
    "            workshop_pipeline.transforms.append(LearnableTransform(config[\"features\"], output_dim))\n",
    "\n",
    "        # Measure performance\n",
    "        grain_results = measure_transform_performance(\n",
    "            \"Grain Pipeline\", grain_pipeline, data, config[\"batch\"], is_stateful=False\n",
    "        )\n",
    "\n",
    "        workshop_results = measure_transform_performance(\n",
    "            \"Workshop Pipeline\", workshop_pipeline, data, config[\"batch\"], is_stateful=True\n",
    "        )\n",
    "\n",
    "        # Calculate improvements\n",
    "        speedup = grain_results[\"mean_time_ms\"] / workshop_results[\"mean_time_ms\"]\n",
    "        memory_improvement = 1 - (\n",
    "            workshop_results[\"memory_delta_mb\"] / max(grain_results[\"memory_delta_mb\"], 0.1)\n",
    "        )\n",
    "\n",
    "        print(\"\\nIMPROVEMENTS:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"  Processing speedup: {speedup:.2f}x\")\n",
    "        print(f\"  Memory efficiency: {memory_improvement * 100:.1f}% better\")\n",
    "        print(\"  State management: Automatic vs Manual\")\n",
    "\n",
    "        # Check if learnable\n",
    "        has_learnable = any(hasattr(t, \"num_parameters\") for t in workshop_pipeline.transforms)\n",
    "        if has_learnable:\n",
    "            print(\"  Learnable parameters: ✓ Supported (Workshop only)\")\n",
    "\n",
    "        all_results.append(\n",
    "            {\n",
    "                \"config\": config,\n",
    "                \"grain\": grain_results,\n",
    "                \"workshop\": workshop_results,\n",
    "                \"improvements\": {\n",
    "                    \"speedup\": speedup,\n",
    "                    \"memory\": memory_improvement,\n",
    "                    \"has_learnable\": has_learnable,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ENHANCED TRANSFORMATION PIPELINE COMPARISON\")\n",
    "    print(\"All metrics from actual execution\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Run full comparison\n",
    "    results = run_comprehensive_transform_comparison()\n",
    "\n",
    "    # Demonstrate unique capabilities\n",
    "    demonstrate_learnable_transforms()\n",
    "\n",
    "    # Compare checkpoint complexity\n",
    "    compare_checkpoint_complexity()\n",
    "\n",
    "    # Final summary\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FINAL SUMMARY - MEASURED ADVANTAGES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_speedup = np.mean([r[\"improvements\"][\"speedup\"] for r in results])\n",
    "    avg_memory = np.mean([r[\"improvements\"][\"memory\"] for r in results])\n",
    "\n",
    "    print(f\"\\n✓ Average processing speedup: {avg_speedup:.2f}x\")\n",
    "    print(f\"✓ Average memory improvement: {avg_memory * 100:.1f}%\")\n",
    "    print(\"✓ Learnable transformations: Datarax only\")\n",
    "    print(\"✓ Automatic state management: Datarax only\")\n",
    "    print(\"✓ Gradient computation: Datarax only\")\n",
    "    print(\"✓ Built-in batch statistics: Datarax only\")\n",
    "    print(\"✓ Internal RNG management: Datarax only\")\n",
    "    print(\"✓ One-line checkpointing: Datarax only\")\n",
    "\n",
    "    print(\"\\nConclusion: Datarax's stateful approach enables\")\n",
    "    print(\"capabilities that are impossible with Grain's stateless design\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
