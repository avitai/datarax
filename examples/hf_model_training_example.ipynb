{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3fc703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example demonstrating HuggingFace Datasets integration with model training.\n",
    "\n",
    "This example shows how to use Datarax with HuggingFace Datasets to create a\n",
    "data pipeline for model training, complete with checkpointing and evaluation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datarax.core import Pipeline\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.sources import HfDataSourceConfig, HFSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f02d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple text classifier model\n",
    "class TextClassifier(nnx.Module):\n",
    "    \"\"\"Simple text classifier with embedding layer and MLP layers.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, *, rngs: nnx.Rngs):\n",
    "        \"\"\"Initialize the text classifier.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
    "        self.dense1 = nnx.Linear(in_features=embed_dim, out_features=hidden_dim, rngs=rngs)\n",
    "        # Create Dropout layer\n",
    "        self.dropout = nnx.Dropout(rate=0.1)\n",
    "        self.dense2 = nnx.Linear(in_features=hidden_dim, out_features=num_classes, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        \"\"\"Forward pass of the text classifier.\"\"\"\n",
    "        x = self.embedding(x)\n",
    "        # Average over token dimension\n",
    "        x = jnp.mean(x, axis=1)\n",
    "        x = nnx.relu(self.dense1(x))\n",
    "        if training:\n",
    "            # Apply dropout during training\n",
    "            dropout_rng = nnx.Rngs(dropout=jax.random.key(0))\n",
    "            x = self.dropout(x, deterministic=not training, rngs=dropout_rng)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaef12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_glue_sst2(examples, key=None):\n",
    "    \"\"\"Tokenizer for SST-2 sentiment analysis dataset.\n",
    "\n",
    "    Args:\n",
    "        examples: Input examples dictionary\n",
    "        key: Optional RNG key (unused for deterministic tokenization)\n",
    "\n",
    "    Returns:\n",
    "        Tokenized examples\n",
    "    \"\"\"\n",
    "    # Simple vocabulary for demo purposes\n",
    "    # In real applications, use a proper tokenizer like SentencePiece or WordPiece\n",
    "    vocab = {}\n",
    "    # Add special tokens\n",
    "    vocab[\"<pad>\"] = 0\n",
    "    vocab[\"<unk>\"] = 1\n",
    "\n",
    "    # Add common words (simplified for demo)\n",
    "    common_words = [\n",
    "        \"the\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"and\",\n",
    "        \"is\",\n",
    "        \"was\",\n",
    "        \"it\",\n",
    "        \"to\",\n",
    "        \"of\",\n",
    "        \"in\",\n",
    "        \"movie\",\n",
    "        \"film\",\n",
    "        \"great\",\n",
    "        \"good\",\n",
    "        \"bad\",\n",
    "        \"terrible\",\n",
    "        \"excellent\",\n",
    "        \"poor\",\n",
    "        \"amazing\",\n",
    "        \"awful\",\n",
    "        \"wonderful\",\n",
    "        \"horrible\",\n",
    "        \"best\",\n",
    "        \"worst\",\n",
    "        \"like\",\n",
    "        \"love\",\n",
    "        \"hate\",\n",
    "        \"enjoy\",\n",
    "        \"boring\",\n",
    "        \"exciting\",\n",
    "        \"interesting\",\n",
    "        \"dull\",\n",
    "        \"fun\",\n",
    "        \"not\",\n",
    "        \"very\",\n",
    "        \"really\",\n",
    "        \"quite\",\n",
    "        \"so\",\n",
    "        \"much\",\n",
    "        \"this\",\n",
    "    ]\n",
    "\n",
    "    for i, word in enumerate(common_words):\n",
    "        vocab[word] = i + 2  # Start after special tokens\n",
    "\n",
    "    # Ensure inputs are lists\n",
    "    if isinstance(examples[\"sentence\"], list):\n",
    "        sentences = examples[\"sentence\"]\n",
    "    else:\n",
    "        sentences = [examples[\"sentence\"]]\n",
    "\n",
    "    # Get labels\n",
    "    if \"label\" in examples:\n",
    "        if isinstance(examples[\"label\"], list):\n",
    "            labels = examples[\"label\"]\n",
    "        else:\n",
    "            labels = [examples[\"label\"]]\n",
    "    else:\n",
    "        # Default labels if not found (should not happen)\n",
    "        labels = [0] * len(sentences)\n",
    "\n",
    "    # Tokenize each sentence\n",
    "    tokenized_inputs = []\n",
    "    for sentence in sentences:\n",
    "        # Convert to lowercase, split by space, and map to vocab ids\n",
    "        tokens = []\n",
    "        for word in sentence.lower().split()[:50]:  # Limit to 50 words\n",
    "            tokens.append(vocab.get(word, vocab[\"<unk>\"]))\n",
    "\n",
    "        # Truncate or pad to fixed length (30 tokens)\n",
    "        if len(tokens) > 30:\n",
    "            tokens = tokens[:30]\n",
    "        else:\n",
    "            tokens = tokens + [vocab[\"<pad>\"]] * (30 - len(tokens))\n",
    "\n",
    "        tokenized_inputs.append(tokens)\n",
    "\n",
    "    # Return tokenized inputs with labels\n",
    "    return {\"tokens\": tokenized_inputs, \"label\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b246a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels):\n",
    "    \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "    one_hot = jax.nn.one_hot(labels, num_classes=2)\n",
    "    return optax.softmax_cross_entropy(logits, one_hot).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, labels):\n",
    "    \"\"\"Compute accuracy.\"\"\"\n",
    "    predictions = jnp.argmax(logits, axis=1)\n",
    "    return jnp.mean(predictions == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the example.\"\"\"\n",
    "    print(\"Datarax HuggingFace Datasets Integration Example\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    # Check for datasets package\n",
    "    try:\n",
    "        import datasets  # noqa: F401\n",
    "    except ImportError:\n",
    "        print(\"Error: HuggingFace datasets package not installed.\")\n",
    "        print(\"Install with: pip install datasets\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nLoading SST-2 dataset...\")\n",
    "\n",
    "    # Create data source for SST-2 dataset using config-based API\n",
    "    train_config = HfDataSourceConfig(\n",
    "        name=\"glue\",\n",
    "        split=\"train[:sst2]\",  # SST-2 subset of GLUE\n",
    "        streaming=False,\n",
    "        shuffle=True,\n",
    "        stochastic=True,\n",
    "        stream_name=\"shuffle\",\n",
    "    )\n",
    "    train_source = HFSource(train_config, rngs=nnx.Rngs(0, shuffle=0))\n",
    "\n",
    "    val_config = HfDataSourceConfig(\n",
    "        name=\"glue\",\n",
    "        split=\"validation[:sst2]\",\n",
    "        streaming=False,\n",
    "    )\n",
    "    val_source = HFSource(val_config, rngs=nnx.Rngs(1))\n",
    "\n",
    "    print(\"Creating data streams...\")\n",
    "\n",
    "    # Create ElementOperator for tokenization\n",
    "    tokenizer_config = ElementOperatorConfig(stochastic=False)\n",
    "    tokenizer = ElementOperator(tokenizer_config, fn=tokenize_glue_sst2, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Create data streams with transformations using the fluent API\n",
    "    train_stream = Pipeline(train_source).map(tokenizer).batch(batch_size=32)\n",
    "\n",
    "    val_stream = Pipeline(val_source).map(tokenizer).batch(batch_size=64)\n",
    "\n",
    "    print(\"Initializing model...\")\n",
    "\n",
    "    # Model hyperparameters\n",
    "    vocab_size = 42  # Size of our toy vocabulary\n",
    "    embed_dim = 64\n",
    "    hidden_dim = 128\n",
    "    num_classes = 2  # Binary sentiment classification\n",
    "\n",
    "    # Create model\n",
    "    param_key = jax.random.key(0)\n",
    "    model_rngs = nnx.Rngs(params=param_key)\n",
    "\n",
    "    model = TextClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=num_classes,\n",
    "        rngs=model_rngs,\n",
    "    )\n",
    "\n",
    "    # Create optimizer with nnx\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n",
    "\n",
    "    # Create metrics\n",
    "    metrics = nnx.MultiMetric(\n",
    "        accuracy=nnx.metrics.Accuracy(),\n",
    "        loss=nnx.metrics.Average(\"loss\"),\n",
    "    )\n",
    "\n",
    "    # Define the loss function\n",
    "    def loss_fn(model, batch):\n",
    "        tokens = jnp.array(batch[\"tokens\"])\n",
    "        labels = jnp.array(batch[\"label\"])\n",
    "        logits = model(tokens, training=True)\n",
    "        loss = compute_loss(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "    @nnx.jit\n",
    "    def train_step(model, optimizer, metrics, batch):\n",
    "        \"\"\"Single training step using nnx and optax.\"\"\"\n",
    "        # Use value_and_grad to compute loss and gradients\n",
    "        grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, logits), grads = grad_fn(model, batch)\n",
    "\n",
    "        # Update metrics (in-place)\n",
    "        metrics.update(loss=loss, logits=logits, labels=jnp.array(batch[\"label\"]))\n",
    "\n",
    "        # Update optimizer (which updates the model)\n",
    "        optimizer.update(model, grads)\n",
    "\n",
    "    @nnx.jit\n",
    "    def eval_step(model, metrics, batch):\n",
    "        \"\"\"Evaluate the model on a batch.\"\"\"\n",
    "        tokens = jnp.array(batch[\"tokens\"])\n",
    "        labels = jnp.array(batch[\"label\"])\n",
    "\n",
    "        logits = model(tokens, training=False)\n",
    "        loss = compute_loss(logits, labels)\n",
    "\n",
    "        # Update metrics (in-place)\n",
    "        metrics.update(loss=loss, logits=logits, labels=labels)\n",
    "\n",
    "    # Use repository-based directory for checkpointing instead of a temporary one\n",
    "    temp_dir = os.path.join(\n",
    "        os.path.abspath(os.path.dirname(__file__)), \"..\", \"temp\", \"hf_model_training\"\n",
    "    )\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    ckpt_dir = os.path.abspath(temp_dir)\n",
    "    print(f\"Using directory: {ckpt_dir}\")\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 3\n",
    "    steps_per_epoch = 10  # Limit for example purposes\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Reset metrics for training\n",
    "        metrics.reset()\n",
    "\n",
    "        # Training phase\n",
    "        print(\"Training...\")\n",
    "\n",
    "        # Process a fixed number of batches per epoch\n",
    "        batch_count = 0\n",
    "        for batch in train_stream:\n",
    "            train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "            batch_count += 1\n",
    "            if batch_count % 2 == 0:\n",
    "                # Print intermediate metrics\n",
    "                metric_values = metrics.compute()\n",
    "                print(\n",
    "                    f\"  Step {batch_count}/{steps_per_epoch}, \"\n",
    "                    f\"Loss: {metric_values['loss']:.4f}, \"\n",
    "                    f\"Accuracy: {metric_values['accuracy']:.4f}\"\n",
    "                )\n",
    "\n",
    "            if batch_count >= steps_per_epoch:\n",
    "                break\n",
    "\n",
    "        # Print training metrics\n",
    "        train_metrics = metrics.compute()\n",
    "        print(\n",
    "            f\"  Training summary - Loss: {train_metrics['loss']:.4f}, \"\n",
    "            f\"Accuracy: {train_metrics['accuracy']:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Reset metrics for validation\n",
    "        metrics.reset()\n",
    "\n",
    "        # Validation phase\n",
    "        print(\"Validating...\")\n",
    "\n",
    "        # Use a fixed number of batches for validation\n",
    "        val_batch_count = 0\n",
    "        for batch in val_stream:\n",
    "            eval_step(model, metrics, batch)\n",
    "            val_batch_count += 1\n",
    "            if val_batch_count >= 5:  # Limit for example purposes\n",
    "                break\n",
    "\n",
    "        # Print validation results\n",
    "        val_metrics = metrics.compute()\n",
    "        print(\n",
    "            f\"  Validation summary - Loss: {val_metrics['loss']:.4f}, \"\n",
    "            f\"Accuracy: {val_metrics['accuracy']:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nExample completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a10b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
