{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# DDSP: Differentiable Digital Signal Processing\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Advanced |\n",
    "| **Runtime** | ~3 hrs (GPU, full) / ~15 min (GPU, quick) |\n",
    "| **Prerequisites** | JAX, Flax NNX, audio/DSP basics, custom operator patterns |\n",
    "| **Memory** | ~6 GB VRAM (GPU, full) / ~4 GB VRAM (GPU, quick) |\n",
    "| **Devices** | GPU recommended, CPU supported |\n",
    "| **Dataset** | NSynth gansynth_subset (~1 GB, auto-downloaded via TFDS) |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example re-implements the core architecture from **DDSP: Differentiable\n",
    "Digital Signal Processing** (Engel et al., ICLR 2020) using datarax's\n",
    "extensibility features. We create **3 custom operators** for audio synthesis\n",
    "that extend `OperatorModule` directly — proving that datarax's operator\n",
    "system works for any domain, not just images.\n",
    "\n",
    "**Key insight**: DDSP shows that classical DSP operations (oscillators,\n",
    "filters, reverb) can be made differentiable and trained end-to-end, requiring\n",
    "100x less training data than neural audio models. Datarax's operator system\n",
    "makes this natural — just subclass `OperatorModule`, add `nnx.Param`, and\n",
    "the DAG executor handles the rest.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this example, you will be able to:\n",
    "\n",
    "1. **Create** custom `OperatorModule` subclasses for non-image domains (audio)\n",
    "2. **Implement** differentiable DSP primitives (harmonic synth, noise filter, reverb)\n",
    "3. **Compose** parallel + sequential pipelines using `CompositeOperatorModule`\n",
    "4. **Train** an audio synthesis model using multi-scale spectral loss on real data\n",
    "5. **Understand** how datarax's extensibility enables any-domain differentiable pipelines\n",
    "\n",
    "## Reference\n",
    "\n",
    "- Paper: Engel et al., \"DDSP: Differentiable Digital Signal Processing\" (ICLR 2020)\n",
    "  — [arXiv:2001.04643](https://arxiv.org/abs/2001.04643)\n",
    "- Code: [github.com/magenta/ddsp](https://github.com/magenta/ddsp) (TensorFlow)\n",
    "- JAX ref: [github.com/PapayaResearch/synthax](https://github.com/PapayaResearch/synthax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup & Prerequisites\n",
    "\n",
    "### Required Knowledge\n",
    "- [Custom Operators](../../core/02_operators_tutorial.py) — OperatorModule pattern\n",
    "- [DAG Pipelines](../dag/01_dag_fundamentals_guide.py) — Parallel, Merge nodes\n",
    "- Basic audio/DSP concepts (sample rate, FFT, harmonics)\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Install datarax with data dependencies (includes tensorflow-datasets)\n",
    "uv pip install \"datarax[data]\"\n",
    "\n",
    "# No additional audio libraries needed — all DSP is in pure JAX\n",
    "```\n",
    "\n",
    "**Estimated Time:** ~3 hrs on GPU (full, 10K samples) / ~15 min on GPU (quick mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "from datarax import from_source\n",
    "from datarax.core.config import OperatorConfig\n",
    "from datarax.core.element_batch import Batch\n",
    "from datarax.core.operator import OperatorModule\n",
    "from datarax.operators import (\n",
    "    CompositeOperatorModule,\n",
    "    CompositeOperatorConfig,\n",
    "    CompositionStrategy,\n",
    ")\n",
    "from datarax.sources import MemorySource, MemorySourceConfig\n",
    "from datarax.operators.modality.audio import LoudnessOperator, LoudnessConfig\n",
    "\n",
    "\n",
    "def exp_sigmoid(x, exponent=10.0, max_value=2.0, threshold=1e-7):\n",
    "    \"\"\"Exponentiated Sigmoid pointwise nonlinearity (DDSP paper, Engel et al. 2020).\n",
    "\n",
    "    Attempt at bounds: [threshold, max_value] with logarithmic response.\n",
    "    Used for amplitude activations in both harmonic and noise synthesis.\n",
    "\n",
    "    Reference: ddsp/core.py — ``exp_sigmoid``\n",
    "    \"\"\"\n",
    "    return max_value * jax.nn.sigmoid(x) ** jnp.log(exponent) + threshold\n",
    "\n",
    "\n",
    "from datarax.operators.modality.audio.crepe_model import load_crepe_weights\n",
    "from datarax.operators.modality.audio.f0_operator import CrepeF0Operator, CrepeF0Config\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory for saved figures\n",
    "OUTPUT_DIR = Path(\"docs/assets/images/examples\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_specgram(ax, audio, sample_rate=16000):\n",
    "    \"\"\"Plot spectrogram on axes with standard DDSP visualization settings.\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "        ax.specgram(\n",
    "            audio,\n",
    "            Fs=sample_rate,\n",
    "            NFFT=1024,\n",
    "            noverlap=768,\n",
    "            cmap=\"magma\",\n",
    "            vmin=-80,\n",
    "            vmax=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Core Concepts\n",
    "\n",
    "### DDSP Architecture\n",
    "\n",
    "DDSP's key innovation: replace opaque neural audio generation with\n",
    "**differentiable classical DSP**. The architecture:\n",
    "\n",
    "1. **Decoder**: Maps audio features (f0, loudness) → synthesis parameters\n",
    "2. **Harmonic Synth**: Additive synthesis with phase accumulation\n",
    "3. **Noise Synth**: Filtered white noise with learned frequency response\n",
    "4. **Reverb**: Trainable FIR impulse response for room acoustics\n",
    "5. **Loss**: Multi-scale spectral comparison with ground truth\n",
    "\n",
    "```\n",
    "                       DDSP Autoencoder\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                                                             │\n",
    "│  Audio Features (f0, loudness) → [Decoder]                  │\n",
    "│                                    (GRU + MLP)              │\n",
    "│                                       │                     │\n",
    "│                                  ┌────┴────┐                │\n",
    "│                                  ▼         ▼                │\n",
    "│                         [Harmonic    [Filtered               │\n",
    "│                          Synth]       Noise]   ← Parallel   │\n",
    "│                            │            │                    │\n",
    "│                            └─────┬──────┘                    │\n",
    "│                                  ▼                           │\n",
    "│                     WEIGHTED_PARALLEL (sum)                   │\n",
    "│                                  │                           │\n",
    "│                                  ▼                           │\n",
    "│                             [Reverb]      ← Trainable IR     │\n",
    "│                                  │                           │\n",
    "│                                  ▼                           │\n",
    "│                      Resynthesized Audio                      │\n",
    "│                                  │                           │\n",
    "│                                  ▼                           │\n",
    "│                    Multi-Scale Spectral Loss                  │\n",
    "│                                  │                           │\n",
    "│                    jax.grad → update decoder + operators      │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Why Custom Operators?\n",
    "\n",
    "Datarax's image operators (`BrightnessOperator`, etc.) extend `ModalityOperator`\n",
    "which provides image-specific helpers (`_extract_field`, `_apply_clip_range`).\n",
    "For audio, we extend `OperatorModule` directly — there's no audio base class\n",
    "(yet), which is exactly the point: **datarax is extensible to any domain**.\n",
    "\n",
    "Each custom operator follows the same pattern:\n",
    "1. Create a companion `Config` dataclass extending `OperatorConfig`\n",
    "2. Add `nnx.Param` for learnable parameters in `__init__`\n",
    "3. Implement `apply()` with the standard signature\n",
    "\n",
    "### Amplitude Activation: ``exp_sigmoid``\n",
    "\n",
    "The reference DDSP uses an exponentiated sigmoid nonlinearity for amplitude\n",
    "outputs (both harmonic and noise). This bounds values to ``[1e-7, 2.0]`` with\n",
    "a logarithmic response, giving stable gradients across the dynamic range:\n",
    "\n",
    "$$\n",
    "\\\\text{exp\\_sigmoid}(x) = 2.0 \\\\cdot \\\\sigma(x)^{\\\\ln 10} + 10^{-7}\n",
    "$$\n",
    "\n",
    "This replaces ``softplus`` (unbounded) and ``softmax`` (zero-sum competition\n",
    "between harmonics), which caused amplitude collapse in early training.\n",
    "\n",
    "### Multi-Scale Spectral Loss\n",
    "\n",
    "DDSP uses spectral loss instead of waveform MSE because:\n",
    "- Waveform MSE penalizes phase differences (which humans can't hear)\n",
    "- Spectral loss compares frequency content across multiple time scales\n",
    "- FFT sizes [64, 128, 256, 512, 1024, 2048] capture both fine detail and global structure\n",
    "\n",
    "$$\n",
    "\\\\mathcal{L} = \\\\sum_{s \\\\in \\\\text{scales}} \\\\left(\n",
    "  \\\\|\\\\hat{S}_s - S_s\\\\|_1 + \\\\alpha \\\\|\\\\log \\\\hat{S}_s - \\\\log S_s\\\\|_1\n",
    "\\\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Implementation\n",
    "\n",
    "### Step 1: Load NSynth Dataset + Extract Features via Datarax Operators\n",
    "\n",
    "We load raw instrument recordings from the NSynth dataset and extract\n",
    "audio features **using datarax's own audio operators** — the same\n",
    "`OperatorModule` pattern used for synthesis later in this guide:\n",
    "\n",
    "1. **`LoudnessOperator`** (pure JAX, learnable weights):\n",
    "   STFT → power spectrum → A-weighted loudness in dB.\n",
    "   Frequency weights are `nnx.Param`, initialized from IEC 61672.\n",
    "\n",
    "2. **`CrepeF0Operator`** (Flax NNX CREPE port):\n",
    "   Frames audio → normalizes → runs CREPE CNN → decodes pitch.\n",
    "   All weights are `nnx.Param` — enable fine-tuning during training.\n",
    "\n",
    "Each sample produces:\n",
    "- `audio`: (64000,) float32 — 4 seconds at 16 kHz\n",
    "- `f0_hz`: (1000,) — CREPE pitch estimates at 250 Hz frame rate\n",
    "- `loudness`: (1000,) — A-weighted loudness in dB\n",
    "\n",
    "**Why datarax operators instead of crepe + librosa?**\n",
    "- Same `apply(data, state, metadata)` contract as the synthesis operators below\n",
    "- Pure JAX — vmap/JIT/grad compatible, GPU-accelerated\n",
    "- No external Python dependencies (crepe, librosa) needed at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load NSynth dataset\n",
    "SAMPLE_RATE = 16000\n",
    "AUDIO_LENGTH = 64000  # 4 seconds at 16 kHz\n",
    "N_FRAMES = 1000  # Feature frames at 250 Hz frame rate\n",
    "FRAME_RATE = 250  # Hz\n",
    "N_HARMONICS = 100  # Paper uses 100 harmonics\n",
    "N_NOISE_BANDS = 65  # Number of frequency bins for noise filter\n",
    "\n",
    "# === Training Configuration ===\n",
    "# All mode-dependent settings in one place. QUICK_MODE=True for fast demos\n",
    "# (~15 min GPU), False for full training (~3 hrs GPU, ~31K steps).\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainConfig:\n",
    "    \"\"\"Immutable training configuration — all mode-dependent settings.\"\"\"\n",
    "\n",
    "    n_train: int\n",
    "    n_test: int\n",
    "    num_epochs: int\n",
    "    batch_size: int\n",
    "    loss_fft_sizes: tuple[int, ...]\n",
    "\n",
    "\n",
    "QUICK_CONFIG = TrainConfig(\n",
    "    n_train=500,\n",
    "    n_test=100,\n",
    "    num_epochs=5,\n",
    "    batch_size=8,\n",
    "    # Fewer FFT scales reduces XLA compilation time significantly\n",
    "    # (each scale adds a separate STFT + gradient computation to the XLA graph)\n",
    "    loss_fft_sizes=(256, 1024, 2048),\n",
    ")\n",
    "\n",
    "FULL_CONFIG = TrainConfig(\n",
    "    n_train=10000,\n",
    "    n_test=500,\n",
    "    num_epochs=100,\n",
    "    batch_size=32,\n",
    "    loss_fft_sizes=(64, 128, 256, 512, 1024, 2048),\n",
    ")\n",
    "\n",
    "QUICK_MODE = False\n",
    "cfg = QUICK_CONFIG if QUICK_MODE else FULL_CONFIG\n",
    "\n",
    "\n",
    "def load_nsynth(\n",
    "    n_train: int = 10000,\n",
    "    n_test: int = 500,\n",
    ") -> tuple[dict, dict]:\n",
    "    \"\"\"Load NSynth gansynth_subset and extract features with datarax operators.\n",
    "\n",
    "    Downloads via tensorflow_datasets on first run (~1 GB). Computes f0 with\n",
    "    datarax's CrepeF0Operator (Flax NNX CREPE port) and loudness with\n",
    "    LoudnessOperator (pure JAX A-weighted STFT). Results are cached to disk.\n",
    "\n",
    "    Args:\n",
    "        n_train: Number of training samples to use.\n",
    "        n_test: Number of test samples to use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_data, test_data) dicts with keys:\n",
    "            audio: (N, 64000) float32\n",
    "            f0: (N, 1000) float32 — MIDI-normalized f0 in [0,1]\n",
    "            loudness: (N, 1000) float32 — dB-range normalized loudness in [0,1]\n",
    "            f0_hz: (N, 1000) float32 — raw f0 in Hz\n",
    "    \"\"\"\n",
    "    import os as _os\n",
    "    import glob\n",
    "    import csv\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Prevent TF from claiming GPU memory (only JAX needs it)\n",
    "    tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "    # ---- Fast path: bypass Beam entirely ----\n",
    "    # The TFDS gansynth_subset.f0_and_loudness config runs CREPE (a CNN) on\n",
    "    # every audio clip via Apache Beam, which takes 30+ minutes even with\n",
    "    # multi-processing. Instead, we:\n",
    "    #   1. Read raw NSynth TFRecords directly (already downloaded)\n",
    "    #   2. Filter to GANSynth subset (acoustic instruments, MIDI pitch [24,84])\n",
    "    #   3. Compute f0 and loudness with datarax's audio operators — pure JAX,\n",
    "    #      GPU-accelerated, only for the samples we need (not all ~290K)\n",
    "    data_dir = _os.environ.get(\"TFDS_DATA_DIR\", None)\n",
    "    if data_dir is None:\n",
    "        data_dir = _os.path.join(_os.path.expanduser(\"~\"), \"tensorflow_datasets\")\n",
    "\n",
    "    # Step 1: Download raw NSynth data if needed (uses TFDS downloader)\n",
    "    import tensorflow_datasets as tfds\n",
    "\n",
    "    tfds.builder(\"nsynth/gansynth_subset\", data_dir=data_dir)\n",
    "    dl_manager = tfds.download.DownloadManager(\n",
    "        download_dir=_os.path.join(data_dir, \"downloads\"),\n",
    "        extract_dir=_os.path.join(data_dir, \"downloads\", \"extracted\"),\n",
    "    )\n",
    "    dl_urls = {\n",
    "        \"examples\": {\n",
    "            \"train\": \"http://download.magenta.tensorflow.org/datasets/nsynth/nsynth-train.tfrecord.tar\",\n",
    "        },\n",
    "        \"gansynth_splits\": \"http://download.magenta.tensorflow.org/datasets/nsynth/nsynth-gansynth_splits.csv\",\n",
    "    }\n",
    "    dl_paths = dl_manager.download_and_extract(dl_urls)\n",
    "\n",
    "    # Step 2: Load GANSynth split IDs (acoustic instruments, pitch [24,84])\n",
    "    gansynth_train_ids = set()\n",
    "    with tf.io.gfile.GFile(dl_paths[\"gansynth_splits\"]) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row[\"split\"] == \"train\":\n",
    "                gansynth_train_ids.add(row[\"id\"])\n",
    "    print(f\"  GANSynth train subset: {len(gansynth_train_ids)} note IDs\")\n",
    "\n",
    "    # Step 3: Read raw TFRecords, filter to GANSynth subset, collect samples\n",
    "    train_dir = dl_paths[\"examples\"][\"train\"]\n",
    "    if _os.path.isdir(train_dir):\n",
    "        tfrecord_files = sorted(glob.glob(_os.path.join(train_dir, \"*.tfrecord*\")))\n",
    "    else:\n",
    "        tfrecord_files = [train_dir]\n",
    "\n",
    "    # Parse raw NSynth TFRecord format\n",
    "    feature_spec = {\n",
    "        \"audio\": tf.io.FixedLenFeature([64000], tf.float32),\n",
    "        \"note_str\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    n_total = n_train + n_test\n",
    "    # Over-read to ensure enough samples after filtering + shuffling\n",
    "    n_read_target = n_total * 2\n",
    "\n",
    "    raw_ds = tf.data.TFRecordDataset(tfrecord_files, num_parallel_reads=8)\n",
    "    audio_list = []\n",
    "    note_ids = []\n",
    "    for raw_record in raw_ds:\n",
    "        parsed = tf.io.parse_single_example(raw_record, feature_spec)\n",
    "        note_id = parsed[\"note_str\"].numpy().decode(\"utf-8\")\n",
    "        if note_id in gansynth_train_ids:\n",
    "            audio_list.append(parsed[\"audio\"].numpy())\n",
    "            note_ids.append(note_id)\n",
    "            if len(audio_list) >= n_read_target:\n",
    "                break\n",
    "\n",
    "    print(f\"  Loaded {len(audio_list)} GANSynth samples from raw TFRecords\")\n",
    "    # Shuffle and trim to exact count\n",
    "    rng_load = np.random.RandomState(42)\n",
    "    load_indices = rng_load.permutation(len(audio_list))[:n_total]\n",
    "    audio_arr = np.stack([audio_list[i] for i in load_indices])\n",
    "\n",
    "    # Step 4: Compute f0 and loudness with datarax audio operators\n",
    "    # These use the same OperatorModule.apply() contract as the synthesis\n",
    "    # operators below — showing that datarax is extensible to any domain.\n",
    "    #\n",
    "    # Check disk cache first (feature extraction is a one-time cost)\n",
    "    cache_path = _os.path.join(data_dir, f\"nsynth_ddsp_cache_{n_total}.npz\")\n",
    "    if _os.path.exists(cache_path):\n",
    "        print(f\"  Loading cached features from {cache_path}\")\n",
    "        cached = np.load(cache_path)\n",
    "        audio_all = cached[\"audio\"]\n",
    "        f0_all = cached[\"f0_hz\"]\n",
    "        loudness_all = cached[\"loudness\"]\n",
    "    else:\n",
    "        # Create datarax feature extraction operators\n",
    "        rngs = nnx.Rngs(0)\n",
    "        loudness_op = LoudnessOperator(LoudnessConfig(), rngs=rngs)\n",
    "\n",
    "        f0_op = CrepeF0Operator(\n",
    "            CrepeF0Config(differentiable=False, batch_strategy=\"scan\"),\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        load_crepe_weights(f0_op.crepe_model)\n",
    "        f0_op.eval()\n",
    "\n",
    "        print(f\"  Extracting features with datarax operators for {len(audio_arr)} samples...\")\n",
    "        f0_list, loudness_list = [], []\n",
    "\n",
    "        # Batched extraction — call operators directly with Batch objects.\n",
    "        # __call__ → apply_batch → scan(apply), processing elements sequentially.\n",
    "        # CREPE uses jax.lax.scan internally for frame chunking AND\n",
    "        # batch_strategy=\"scan\" processes elements sequentially (O(1) memory).\n",
    "        extract_batch_size = 16\n",
    "\n",
    "        for start in range(0, len(audio_arr), extract_batch_size):\n",
    "            end = min(start + extract_batch_size, len(audio_arr))\n",
    "            audio_batch = jnp.array(audio_arr[start:end])\n",
    "            batch = Batch.from_parts(data={\"audio\": audio_batch}, states={})\n",
    "\n",
    "            loud_out = loudness_op(batch)\n",
    "            loudness_list.append(np.array(loud_out.get_data()[\"loudness\"]))\n",
    "\n",
    "            f0_out = f0_op(batch)\n",
    "            f0_list.append(np.array(f0_out.get_data()[\"f0_hz\"]))\n",
    "\n",
    "            if end % 50 == 0 or end == len(audio_arr):\n",
    "                print(f\"    Processed {end}/{len(audio_arr)} samples\")\n",
    "\n",
    "        audio_all = np.stack([audio_list[i] for i in load_indices])\n",
    "        f0_all = np.concatenate(f0_list, axis=0)\n",
    "        loudness_all = np.concatenate(loudness_list, axis=0)\n",
    "\n",
    "        # Cache to disk for subsequent runs\n",
    "        np.savez(cache_path, audio=audio_all, f0_hz=f0_all, loudness=loudness_all)\n",
    "        print(f\"  Cached features to {cache_path}\")\n",
    "\n",
    "    # Normalize audio to [-1, 1]\n",
    "    audio_max = np.max(np.abs(audio_all), axis=1, keepdims=True)\n",
    "    audio_all = audio_all / np.maximum(audio_max, 1e-8)\n",
    "\n",
    "    # Scale loudness to [0, 1] via fixed dB range (matching DDSP paper)\n",
    "    DB_RANGE = 80.0\n",
    "    loudness_norm = np.clip(loudness_all / DB_RANGE + 1.0, 0.0, 1.0)\n",
    "\n",
    "    # Scale f0 to [0, 1] via MIDI note normalization (perceptually uniform, matching DDSP paper)\n",
    "    f0_midi = 12.0 * np.log2(np.maximum(f0_all, 1e-5) / 440.0) + 69.0\n",
    "    f0_scaled = np.clip(f0_midi / 127.0, 0.0, 1.0)\n",
    "\n",
    "    # Shuffle with fixed seed and split\n",
    "    rng = np.random.RandomState(42)\n",
    "    n_total = len(audio_all)\n",
    "    indices = rng.permutation(n_total)\n",
    "\n",
    "    n_train = min(n_train, n_total - n_test)\n",
    "    train_idx = indices[:n_train]\n",
    "    test_idx = indices[n_train : n_train + n_test]\n",
    "\n",
    "    train_data = {\n",
    "        \"audio\": audio_all[train_idx],\n",
    "        \"f0\": f0_scaled[train_idx],\n",
    "        \"loudness\": loudness_norm[train_idx],\n",
    "        \"f0_hz\": f0_all[train_idx],  # Keep raw Hz for synthesis\n",
    "    }\n",
    "    test_data = {\n",
    "        \"audio\": audio_all[test_idx],\n",
    "        \"f0\": f0_scaled[test_idx],\n",
    "        \"loudness\": loudness_norm[test_idx],\n",
    "        \"f0_hz\": f0_all[test_idx],\n",
    "    }\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# Load NSynth data\n",
    "print(f\"Loading NSynth dataset ({cfg.n_train} train, {cfg.n_test} test)...\")\n",
    "train_data, test_data = load_nsynth(n_train=cfg.n_train, n_test=cfg.n_test)\n",
    "\n",
    "# Wrap in MemorySource\n",
    "train_source = MemorySource(MemorySourceConfig(), data=train_data, rngs=nnx.Rngs(0))\n",
    "test_source = MemorySource(MemorySourceConfig(), data=test_data, rngs=nnx.Rngs(1))\n",
    "\n",
    "print(\n",
    "    f\"Train: audio={train_data['audio'].shape}, \"\n",
    "    f\"f0={train_data['f0'].shape}, \"\n",
    "    f\"loudness={train_data['loudness'].shape}\"\n",
    ")\n",
    "print(\n",
    "    f\"Sample rate: {SAMPLE_RATE} Hz, Audio length: {AUDIO_LENGTH} samples \"\n",
    "    f\"({AUDIO_LENGTH / SAMPLE_RATE:.1f}s)\"\n",
    ")\n",
    "print(f\"Feature frames: {N_FRAMES} @ {FRAME_RATE} Hz frame rate\")\n",
    "# Expected output (QUICK_MODE=False):\n",
    "# Train: audio=(10000, 64000), f0=(10000, 1000), loudness=(10000, 1000)\n",
    "# Sample rate: 16000 Hz, Audio length: 64000 samples (4.0s)\n",
    "# Feature frames: 1000 @ 250 Hz frame rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample audio waveforms and their spectrograms\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "time_axis = np.arange(AUDIO_LENGTH) / SAMPLE_RATE\n",
    "\n",
    "for i in range(3):\n",
    "    audio = train_data[\"audio\"][i]\n",
    "    f0_val = float(train_data[\"f0_hz\"][i, N_FRAMES // 2])  # Mid-point f0\n",
    "\n",
    "    # Waveform (top row) — show first 4000 samples (~250ms)\n",
    "    n_show = 4000\n",
    "    axes[0, i].plot(time_axis[:n_show], audio[:n_show], color=\"steelblue\", linewidth=0.5)\n",
    "    axes[0, i].set_xlabel(\"Time (s)\")\n",
    "    axes[0, i].set_ylabel(\"Amplitude\")\n",
    "    axes[0, i].set_title(f\"Sample {i + 1}: f0 ~ {f0_val:.0f} Hz\")\n",
    "    axes[0, i].set_xlim(0, n_show / SAMPLE_RATE)\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "    # Spectrogram (bottom row)\n",
    "    plot_specgram(axes[1, i], audio)\n",
    "    axes[1, i].set_xlabel(\"Time (s)\")\n",
    "    axes[1, i].set_ylabel(\"Frequency (Hz)\")\n",
    "    axes[1, i].set_ylim(0, 4000)\n",
    "    axes[1, i].set_title(\"Spectrogram (harmonics visible)\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"NSynth Dataset — Real Instrument Waveforms and Spectrograms\\n\"\n",
    "    \"Each sample is a 4-second recording with pre-computed f0 and loudness\",\n",
    "    fontsize=12,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    OUTPUT_DIR / \"cv-ddsp-dataset-samples.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"white\",\n",
    ")\n",
    "plt.close()\n",
    "print(\"Saved: docs/assets/images/examples/cv-ddsp-dataset-samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Step 2: Custom Audio Operators (Extending OperatorModule)\n",
    "\n",
    "These operators extend `OperatorModule` directly — not `ModalityOperator` —\n",
    "because there's no audio-specific base class. This showcases datarax's\n",
    "extensibility: you can build operators for any data type.\n",
    "\n",
    "Each operator follows the standard contract:\n",
    "- `OperatorConfig` subclass for configuration\n",
    "- `nnx.Param` for learnable parameters\n",
    "- `apply(data, state, metadata, random_params, stats) → (data, state, metadata)`\n",
    "\n",
    "**Critical design choices**: The harmonic synthesizer uses **per-frame synthesis**\n",
    "with upsampling from frame rate (250 Hz) to sample rate (16 kHz) via linear\n",
    "interpolation, matching the DDSP paper (Engel et al. 2020). Phase accumulation\n",
    "(`cumsum`) on the upsampled f0 ensures smooth phase continuity — without it,\n",
    "frequency changes create phase discontinuities (audible clicks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Custom DDSP operators\n",
    "\n",
    "\n",
    "# --- Operator 1: Harmonic Synthesizer ---\n",
    "@dataclass\n",
    "class HarmonicSynthConfig(OperatorConfig):\n",
    "    \"\"\"Configuration for Harmonic Synthesizer.\n",
    "\n",
    "    Attributes:\n",
    "        n_harmonics: Number of harmonics to synthesize\n",
    "        n_frames: Number of input feature frames (for upsampling to sample rate)\n",
    "        sample_rate: Audio sample rate in Hz\n",
    "        audio_length: Number of output audio samples\n",
    "    \"\"\"\n",
    "\n",
    "    n_harmonics: int = field(default=N_HARMONICS, kw_only=True)\n",
    "    n_frames: int = field(default=N_FRAMES, kw_only=True)\n",
    "    sample_rate: int = field(default=SAMPLE_RATE, kw_only=True)\n",
    "    audio_length: int = field(default=AUDIO_LENGTH, kw_only=True)\n",
    "\n",
    "\n",
    "class HarmonicSynthOperator(OperatorModule):\n",
    "    \"\"\"Differentiable harmonic additive synthesizer with phase accumulation.\n",
    "\n",
    "    Generates audio as a sum of sinusoidal harmonics using continuous phase\n",
    "    accumulation (matching the DDSP paper's Harmonic synth):\n",
    "\n",
    "        phase[t] = cumsum(2π * f0[t] / sample_rate)\n",
    "        audio = Σ amplitudes[k] * sin(k * phase)\n",
    "\n",
    "    Phase accumulation is critical for differentiable synthesis — it ensures\n",
    "    smooth phase continuity when f0 varies over time (unlike instantaneous\n",
    "    phase `sin(2π * k * f0 * t)` which creates artifacts).\n",
    "\n",
    "    Harmonics above the Nyquist frequency are filtered out.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: HarmonicSynthConfig, *, rngs: nnx.Rngs | None = None):\n",
    "        super().__init__(config, rngs=rngs)\n",
    "        self.config: HarmonicSynthConfig = config\n",
    "\n",
    "    def apply(\n",
    "        self,\n",
    "        data: dict[str, Any],\n",
    "        state: dict[str, Any],\n",
    "        metadata: dict[str, Any] | None,\n",
    "        random_params: Any = None,\n",
    "        stats: dict[str, Any] | None = None,\n",
    "    ) -> tuple[dict[str, Any], dict[str, Any], dict[str, Any] | None]:\n",
    "        \"\"\"Synthesize audio from per-frame harmonic amplitudes and f0.\n",
    "\n",
    "        Upsamples frame-rate controls to sample-rate via linear interpolation,\n",
    "        then performs additive synthesis with time-varying phase accumulation\n",
    "        (matching Engel et al. 2020, Section 3.1).\n",
    "\n",
    "        Expected data keys:\n",
    "            - 'amplitudes': (n_frames, n_harmonics) — per-frame harmonic amplitudes\n",
    "            - 'f0_hz': (n_frames,) — fundamental frequency in Hz per frame\n",
    "\n",
    "        Output data keys (added/updated):\n",
    "            - 'audio': (audio_length,) — synthesized waveform\n",
    "        \"\"\"\n",
    "        amplitudes = data[\"amplitudes\"]  # (n_frames, n_harmonics)\n",
    "        f0_hz = data[\"f0_hz\"]  # (n_frames,)\n",
    "        n_harmonics = self.config.n_harmonics\n",
    "        n_frames = self.config.n_frames\n",
    "        sr = self.config.sample_rate\n",
    "        length = self.config.audio_length\n",
    "\n",
    "        # Upsample f0 from frame rate → sample rate (linear interpolation)\n",
    "        frame_times = jnp.linspace(0, 1, n_frames)\n",
    "        sample_times = jnp.linspace(0, 1, length)\n",
    "        f0_upsampled = jnp.interp(sample_times, frame_times, f0_hz)  # (audio_length,)\n",
    "\n",
    "        # Upsample per-harmonic amplitudes: (n_frames, n_harmonics) → (audio_length, n_harmonics)\n",
    "        amp_upsampled = jax.vmap(\n",
    "            lambda amp_k: jnp.interp(sample_times, frame_times, amp_k),\n",
    "            in_axes=1,\n",
    "            out_axes=1,\n",
    "        )(amplitudes)  # (audio_length, n_harmonics)\n",
    "\n",
    "        # Phase accumulation with time-varying f0\n",
    "        phase_inc = 2.0 * jnp.pi * f0_upsampled / sr  # (audio_length,)\n",
    "        phase = jnp.cumsum(phase_inc)  # (audio_length,)\n",
    "\n",
    "        # Harmonic indices: 1, 2, ..., n_harmonics\n",
    "        harmonic_k = jnp.arange(1, n_harmonics + 1, dtype=jnp.float32)\n",
    "\n",
    "        # Nyquist filtering (per-sample, since f0 varies over time)\n",
    "        nyquist = sr / 2.0\n",
    "        valid_mask = (f0_upsampled[:, None] * harmonic_k[None, :]) < nyquist\n",
    "        amp_upsampled = amp_upsampled * valid_mask\n",
    "\n",
    "        # Sinusoid generation + weighted sum\n",
    "        harmonics = jnp.sin(harmonic_k[None, :] * phase[:, None])  # (audio_length, n_harmonics)\n",
    "        audio = jnp.sum(amp_upsampled * harmonics, axis=1)  # (audio_length,)\n",
    "\n",
    "        out_data = {**data, \"audio\": audio}\n",
    "        return out_data, state, metadata\n",
    "\n",
    "\n",
    "# --- Operator 2: Filtered Noise ---\n",
    "@dataclass\n",
    "class FilteredNoiseConfig(OperatorConfig):\n",
    "    \"\"\"Configuration for Filtered Noise synthesizer.\n",
    "\n",
    "    Attributes:\n",
    "        audio_length: Number of output audio samples\n",
    "        n_noise_bands: Number of frequency bands for noise filter\n",
    "    \"\"\"\n",
    "\n",
    "    audio_length: int = field(default=AUDIO_LENGTH, kw_only=True)\n",
    "    n_noise_bands: int = field(default=N_NOISE_BANDS, kw_only=True)\n",
    "\n",
    "\n",
    "class FilteredNoiseOperator(OperatorModule):\n",
    "    \"\"\"Differentiable filtered noise synthesizer.\n",
    "\n",
    "    Generates audio by filtering white noise in the frequency domain:\n",
    "        1. Generate white noise\n",
    "        2. FFT → multiply by learned frequency response → IFFT\n",
    "\n",
    "    The frequency magnitudes are provided as input (from decoder) and passed\n",
    "    through ``exp_sigmoid`` (bounded [1e-7, 2.0]) before interpolation,\n",
    "    matching the DDSP reference (ddsp/synths.py). Uses a fixed noise seed\n",
    "    for deterministic gradient computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: FilteredNoiseConfig, *, rngs: nnx.Rngs | None = None):\n",
    "        super().__init__(config, rngs=rngs)\n",
    "        self.config: FilteredNoiseConfig = config\n",
    "        # Fixed noise for deterministic gradients\n",
    "        noise = jax.random.normal(jax.random.key(0), (config.audio_length,))\n",
    "        self.fixed_noise = nnx.Variable(noise)\n",
    "\n",
    "    def apply(\n",
    "        self,\n",
    "        data: dict[str, Any],\n",
    "        state: dict[str, Any],\n",
    "        metadata: dict[str, Any] | None,\n",
    "        random_params: Any = None,\n",
    "        stats: dict[str, Any] | None = None,\n",
    "    ) -> tuple[dict[str, Any], dict[str, Any], dict[str, Any] | None]:\n",
    "        \"\"\"Filter noise using learned frequency magnitudes.\n",
    "\n",
    "        Expected data keys:\n",
    "            - 'noise_magnitudes': (n_frames, n_noise_bands) — per-frame filter shape\n",
    "\n",
    "        Output data keys (added/updated):\n",
    "            - 'audio': (audio_length,) — filtered noise waveform\n",
    "        \"\"\"\n",
    "        noise_magnitudes = data[\"noise_magnitudes\"]  # (n_frames, n_noise_bands)\n",
    "\n",
    "        # Average over time frames (simplification — paper uses per-frame overlap-add)\n",
    "        magnitudes = jnp.mean(noise_magnitudes, axis=0)  # (n_noise_bands,)\n",
    "\n",
    "        noise = self.fixed_noise[...]  # (audio_length,)\n",
    "\n",
    "        # FFT-based filtering\n",
    "        noise_fft = jnp.fft.rfft(noise)  # (audio_length//2 + 1,)\n",
    "        n_fft = noise_fft.shape[0]\n",
    "\n",
    "        # Interpolate magnitudes to match FFT size\n",
    "        x_interp = jnp.linspace(0, 1, n_fft)\n",
    "        x_orig = jnp.linspace(0, 1, magnitudes.shape[0])\n",
    "        filter_response = jnp.interp(x_interp, x_orig, exp_sigmoid(magnitudes))\n",
    "\n",
    "        # Apply filter and IFFT\n",
    "        filtered_fft = noise_fft * filter_response\n",
    "        audio = jnp.fft.irfft(filtered_fft, n=self.config.audio_length)\n",
    "\n",
    "        out_data = {**data, \"audio\": audio}\n",
    "        return out_data, state, metadata\n",
    "\n",
    "\n",
    "# --- Operator 3: Reverb ---\n",
    "@dataclass\n",
    "class ReverbConfig(OperatorConfig):\n",
    "    \"\"\"Configuration for trainable Reverb operator.\n",
    "\n",
    "    Attributes:\n",
    "        ir_length: Length of impulse response in samples\n",
    "        sample_rate: Audio sample rate in Hz\n",
    "    \"\"\"\n",
    "\n",
    "    ir_length: int = field(default=SAMPLE_RATE, kw_only=True)  # 1 second IR\n",
    "    sample_rate: int = field(default=SAMPLE_RATE, kw_only=True)\n",
    "\n",
    "\n",
    "class ReverbOperator(OperatorModule):\n",
    "    \"\"\"Differentiable reverb via trainable FIR impulse response.\n",
    "\n",
    "    Applies room acoustics by convolving the input audio with a learned\n",
    "    impulse response (IR). The IR is initialized with exponential decay\n",
    "    (approximating a simple room) and optimized end-to-end.\n",
    "\n",
    "    Uses FFT-based convolution for efficiency.\n",
    "\n",
    "    Matches DDSP paper's Reverb effect (ddsp/effects.py).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ReverbConfig, *, rngs: nnx.Rngs | None = None):\n",
    "        super().__init__(config, rngs=rngs)\n",
    "        self.config: ReverbConfig = config\n",
    "\n",
    "        # Learnable impulse response (init = exponential decay)\n",
    "        decay = jnp.exp(-jnp.arange(config.ir_length, dtype=jnp.float32) * 5.0 / config.ir_length)\n",
    "        self.impulse_response = nnx.Param(decay * 0.1)\n",
    "\n",
    "    def apply(\n",
    "        self,\n",
    "        data: dict[str, Any],\n",
    "        state: dict[str, Any],\n",
    "        metadata: dict[str, Any] | None,\n",
    "        random_params: Any = None,\n",
    "        stats: dict[str, Any] | None = None,\n",
    "    ) -> tuple[dict[str, Any], dict[str, Any], dict[str, Any] | None]:\n",
    "        \"\"\"Apply reverb to audio via FFT-based convolution.\n",
    "\n",
    "        Expected data keys:\n",
    "            - 'audio': (audio_length,) — input audio\n",
    "\n",
    "        Output data keys (updated):\n",
    "            - 'audio': (audio_length,) — reverbed audio (same length)\n",
    "        \"\"\"\n",
    "        audio = data[\"audio\"]  # (audio_length,)\n",
    "        ir = self.impulse_response[...]  # (ir_length,)\n",
    "\n",
    "        # FFT-based convolution\n",
    "        n_fft = audio.shape[0] + ir.shape[0] - 1\n",
    "        # Round up to next power of 2 for FFT efficiency\n",
    "        n_fft_padded = 1 << (n_fft - 1).bit_length()\n",
    "\n",
    "        audio_fft = jnp.fft.rfft(audio, n=n_fft_padded)\n",
    "        ir_fft = jnp.fft.rfft(ir, n=n_fft_padded)\n",
    "        convolved = jnp.fft.irfft(audio_fft * ir_fft, n=n_fft_padded)\n",
    "\n",
    "        # Trim to original length\n",
    "        reverbed = convolved[: audio.shape[0]]\n",
    "\n",
    "        out_data = {**data, \"audio\": reverbed}\n",
    "        return out_data, state, metadata\n",
    "\n",
    "\n",
    "# Verify operators\n",
    "print(\"Verifying DDSP operators...\")\n",
    "\n",
    "# Test harmonic synth (per-frame inputs)\n",
    "h_config = HarmonicSynthConfig()\n",
    "h_op = HarmonicSynthOperator(h_config)\n",
    "h_batch = Batch.from_parts(\n",
    "    data={\n",
    "        \"amplitudes\": jnp.ones((1, N_FRAMES, N_HARMONICS)) * 0.1,\n",
    "        \"f0_hz\": jnp.ones((1, N_FRAMES)) * 440.0,\n",
    "    },\n",
    "    states={},\n",
    ")\n",
    "h_result = h_op(h_batch)\n",
    "h_out = h_result.get_data()\n",
    "print(f\"  HarmonicSynth: output keys={list(h_out.keys())}, audio shape={h_out['audio'].shape}\")\n",
    "\n",
    "# Test filtered noise (per-frame input)\n",
    "n_config = FilteredNoiseConfig()\n",
    "n_op = FilteredNoiseOperator(n_config)\n",
    "n_batch = Batch.from_parts(\n",
    "    data={\"noise_magnitudes\": jnp.ones((1, N_FRAMES, N_NOISE_BANDS))},\n",
    "    states={},\n",
    ")\n",
    "n_result = n_op(n_batch)\n",
    "n_out = n_result.get_data()\n",
    "print(f\"  FilteredNoise: output keys={list(n_out.keys())}, audio shape={n_out['audio'].shape}\")\n",
    "\n",
    "# Test reverb\n",
    "r_config = ReverbConfig()\n",
    "r_op = ReverbOperator(r_config)\n",
    "r_batch = Batch.from_parts(\n",
    "    data={\"audio\": jnp.sin(jnp.linspace(0, 10, AUDIO_LENGTH))[None]},\n",
    "    states={},\n",
    ")\n",
    "r_result = r_op(r_batch)\n",
    "r_out = r_result.get_data()\n",
    "print(\n",
    "    f\"  Reverb: IR params={r_op.impulse_response[...].shape[0]}, audio shape={r_out['audio'].shape}\"\n",
    ")\n",
    "\n",
    "# Count total operator parameters\n",
    "total_op_params = sum(\n",
    "    p.size for op in [h_op, n_op, r_op] for p in jax.tree.leaves(nnx.state(op, nnx.Param))\n",
    ")\n",
    "print(f\"\\nTotal operator parameters: {total_op_params:,}\")\n",
    "# Expected output:\n",
    "#   HarmonicSynth: output keys=['amplitudes', 'f0_hz', 'audio'], audio shape=(1, 64000)\n",
    "#   FilteredNoise: output keys=['noise_magnitudes', 'audio'], audio shape=(1, 64000)\n",
    "#   Reverb: IR params=16000, audio shape=(1, 64000)\n",
    "#   Total operator parameters: 16,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Step 3: DDSP Decoder (Paper-Accurate Architecture)\n",
    "\n",
    "The paper's \"decoder\" maps audio features (f0, loudness) to synthesis\n",
    "parameters. It follows the architecture from Section 3.1:\n",
    "\n",
    "    f0 + loudness → Linear(2→512) → GRU(512) → MLP(512, 3 layers) → heads\n",
    "\n",
    "The MLP stack uses the Artifex pattern: `nnx.List` for dynamic layer\n",
    "collections with `LayerNorm` + `ReLU` activation at each layer.\n",
    "\n",
    "**Why \"decoder\", not \"encoder\"?** The paper calls this a decoder because\n",
    "it maps *extracted audio features* to *synthesis parameters* — the inverse\n",
    "direction of an encoder. Previous versions of this example incorrectly\n",
    "called it \"encoder\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: DDSP Decoder\n",
    "class DDSPDecoder(nnx.Module):\n",
    "    \"\"\"RNN-FC decoder predicting synth params from audio features.\n",
    "\n",
    "    Architecture (matching paper Section 3.1):\n",
    "        f0 + loudness → Linear(2→hidden) → GRU(hidden) →\n",
    "        MLP(hidden, n_layers) → output heads\n",
    "\n",
    "    Output activations use ``exp_sigmoid`` (bounded [1e-7, 2.0]) for amplitude\n",
    "    and harmonic distribution heads, matching the reference DDSP implementation.\n",
    "    Noise head bias is initialized to -5.0 so noise starts near-zero.\n",
    "\n",
    "    Uses nnx.List for the MLP layer collection (Artifex pattern) to\n",
    "    allow arbitrary depth without hardcoding layer count.\n",
    "\n",
    "    Args:\n",
    "        hidden_dim: Hidden dimension for GRU and MLP layers.\n",
    "        n_harmonics: Number of harmonic amplitudes to predict.\n",
    "        n_noise_bands: Number of noise filter frequency bands.\n",
    "        n_mlp_layers: Number of MLP layers after GRU.\n",
    "        rngs: Flax NNX random number generators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        hidden_dim: int = 512,\n",
    "        n_harmonics: int = N_HARMONICS,\n",
    "        n_noise_bands: int = N_NOISE_BANDS,\n",
    "        n_mlp_layers: int = 3,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_noise_bands = n_noise_bands\n",
    "\n",
    "        # Input projection: f0_scaled + loudness_normalized (2 features) → hidden\n",
    "        self.input_proj = nnx.Linear(2, hidden_dim, rngs=rngs)\n",
    "\n",
    "        # GRU for temporal modeling over feature frames\n",
    "        self.gru_cell = nnx.GRUCell(hidden_dim, hidden_dim, rngs=rngs)\n",
    "\n",
    "        # Learnable initial GRU state\n",
    "        self.init_state = nnx.Param(jnp.zeros(hidden_dim))\n",
    "\n",
    "        # MLP stack (Artifex pattern: nnx.List for dynamic layer collections)\n",
    "        self.mlp_layers = nnx.List([])\n",
    "        self.mlp_norms = nnx.List([])\n",
    "        for _ in range(n_mlp_layers):\n",
    "            self.mlp_layers.append(nnx.Linear(hidden_dim, hidden_dim, rngs=rngs))\n",
    "            self.mlp_norms.append(nnx.LayerNorm(hidden_dim, rngs=rngs))\n",
    "\n",
    "        # Output heads\n",
    "        self.amplitude_head = nnx.Linear(hidden_dim, 1, rngs=rngs)\n",
    "        self.harmonic_head = nnx.Linear(hidden_dim, n_harmonics, rngs=rngs)\n",
    "        # Initialize noise head bias to -5.0 so noise starts near-zero\n",
    "        # (matching DDSP paper's initial_bias=-5.0 for FilteredNoise)\n",
    "        self.noise_head = nnx.Linear(hidden_dim, n_noise_bands, rngs=rngs)\n",
    "        self.noise_head.bias.value = jnp.full(n_noise_bands, -5.0)\n",
    "\n",
    "    def __call__(self, f0: jax.Array, loudness: jax.Array) -> tuple[jax.Array, jax.Array]:\n",
    "        \"\"\"Decode audio features into per-frame synthesis parameters.\n",
    "\n",
    "        Args:\n",
    "            f0: Scaled fundamental frequency per frame (n_frames,)\n",
    "            loudness: Standardized loudness per frame (n_frames,)\n",
    "\n",
    "        Returns:\n",
    "            amplitudes: Per-frame harmonic amplitudes (n_frames, n_harmonics)\n",
    "            noise_mags: Per-frame noise filter magnitudes (n_frames, n_noise_bands)\n",
    "        \"\"\"\n",
    "        # Stack features: (n_frames, 2)\n",
    "        features = jnp.stack([f0, loudness], axis=-1)\n",
    "\n",
    "        # Project to hidden dimension\n",
    "        projected = self.input_proj(features)  # (n_frames, hidden)\n",
    "\n",
    "        # Run GRU over frames using lax.scan (compiles loop body once,\n",
    "        # avoiding 1000× unrolling that dominates XLA compilation time)\n",
    "        def gru_step(carry, x_t):\n",
    "            new_carry, output = self.gru_cell(carry, x_t)\n",
    "            return new_carry, output\n",
    "\n",
    "        _, hidden_seq = jax.lax.scan(gru_step, self.init_state[...], projected)\n",
    "        # hidden_seq: (n_frames, hidden)\n",
    "\n",
    "        # MLP stack with LayerNorm + ReLU\n",
    "        h = hidden_seq\n",
    "        for layer, norm in zip(self.mlp_layers, self.mlp_norms):\n",
    "            h = nnx.relu(norm(layer(h)))\n",
    "        # h is (n_frames, hidden) — NO averaging\n",
    "\n",
    "        # Per-frame output heads (using exp_sigmoid from DDSP paper)\n",
    "        overall_amp = exp_sigmoid(self.amplitude_head(h))  # (n_frames, 1)\n",
    "        harmonic_dist = exp_sigmoid(self.harmonic_head(h))  # (n_frames, n_harmonics)\n",
    "        noise_mags = self.noise_head(h)  # (n_frames, n_noise_bands)\n",
    "\n",
    "        # Normalize harmonic distribution and scale by overall amplitude\n",
    "        harmonic_dist = harmonic_dist / (jnp.sum(harmonic_dist, axis=-1, keepdims=True) + 1e-8)\n",
    "        amplitudes = overall_amp * harmonic_dist  # (n_frames, n_harmonics)\n",
    "        return amplitudes, noise_mags\n",
    "\n",
    "\n",
    "# Verify decoder\n",
    "decoder = DDSPDecoder(rngs=nnx.Rngs(0))\n",
    "test_f0 = jnp.ones(N_FRAMES) * 0.543  # ~440 Hz (MIDI 69 / 127)\n",
    "test_loudness = jnp.ones(N_FRAMES) * 0.5  # Moderate loudness (-40 dB)\n",
    "test_amps, test_noise = decoder(test_f0, test_loudness)\n",
    "n_dec_params = sum(p.size for p in jax.tree.leaves(nnx.state(decoder, nnx.Param)))\n",
    "print(f\"Decoder output: amplitudes={test_amps.shape}, noise_mags={test_noise.shape}\")\n",
    "print(f\"Decoder parameters: {n_dec_params:,}\")\n",
    "# Expected output:\n",
    "# Decoder output: amplitudes=(1000, 100), noise_mags=(1000, 65)\n",
    "# Decoder parameters: 2,452,646"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Step 4: Multi-Scale Spectral Loss\n",
    "\n",
    "The loss function compares predicted and target audio in the frequency domain\n",
    "at multiple FFT scales. This is more perceptually meaningful than waveform MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Multi-scale spectral loss\n",
    "def stft_magnitude(audio: jax.Array, fft_size: int, hop_size: int | None = None) -> jax.Array:\n",
    "    \"\"\"Compute STFT magnitude spectrogram.\n",
    "\n",
    "    Args:\n",
    "        audio: Input waveform (n_samples,)\n",
    "        fft_size: FFT window size\n",
    "        hop_size: Hop between frames (default: fft_size // 4)\n",
    "\n",
    "    Returns:\n",
    "        Magnitude spectrogram (n_frames, fft_size // 2 + 1)\n",
    "    \"\"\"\n",
    "    if hop_size is None:\n",
    "        hop_size = fft_size // 4\n",
    "\n",
    "    # Hann window\n",
    "    window = jnp.hanning(fft_size)\n",
    "\n",
    "    # Frame the signal using dynamic_slice via vmap (single XLA op,\n",
    "    # avoids unrolling ~100+ Python slices per FFT scale)\n",
    "    n_frames = max(1, (audio.shape[0] - fft_size) // hop_size + 1)\n",
    "    starts = jnp.arange(n_frames) * hop_size\n",
    "    frames = jax.vmap(lambda s: jax.lax.dynamic_slice(audio, (s,), (fft_size,)))(starts)\n",
    "    frames = frames * window  # (n_frames, fft_size)\n",
    "\n",
    "    # FFT and magnitude\n",
    "    spectra = jnp.fft.rfft(frames)  # (n_frames, fft_size//2 + 1)\n",
    "    return jnp.abs(spectra)\n",
    "\n",
    "\n",
    "def multi_scale_spectral_loss(\n",
    "    pred_audio: jax.Array,\n",
    "    target_audio: jax.Array,\n",
    "    fft_sizes: tuple[int, ...] = (64, 128, 256, 512, 1024, 2048),\n",
    "    alpha: float = 1.0,\n",
    ") -> jax.Array:\n",
    "    \"\"\"Multi-scale spectral loss (matching DDSP paper).\n",
    "\n",
    "    Computes L1 + alpha * L1(log) loss across multiple FFT scales.\n",
    "    Handles both single (n_samples,) and batched (B, n_samples) inputs.\n",
    "\n",
    "    Args:\n",
    "        pred_audio: Predicted waveform (n_samples,) or (B, n_samples)\n",
    "        target_audio: Target waveform (n_samples,) or (B, n_samples)\n",
    "        fft_sizes: Tuple of FFT window sizes\n",
    "        alpha: Weight for log-magnitude term\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value (mean over batch if batched)\n",
    "    \"\"\"\n",
    "    # Batched inputs: vmap over batch dim, return mean\n",
    "    if pred_audio.ndim == 2:\n",
    "        per_sample = jax.vmap(\n",
    "            lambda p, t: multi_scale_spectral_loss(p, t, fft_sizes, alpha),\n",
    "        )(pred_audio, target_audio)\n",
    "        return jnp.mean(per_sample)\n",
    "\n",
    "    total_loss = jnp.array(0.0)\n",
    "\n",
    "    for fft_size in fft_sizes:\n",
    "        if fft_size > pred_audio.shape[0]:\n",
    "            continue\n",
    "\n",
    "        pred_mag = stft_magnitude(pred_audio, fft_size)\n",
    "        target_mag = stft_magnitude(target_audio, fft_size)\n",
    "\n",
    "        # L1 on linear magnitude\n",
    "        l1_loss = jnp.mean(jnp.abs(pred_mag - target_mag))\n",
    "\n",
    "        # L1 on log magnitude (matching DDSP paper — uses L1 for both terms)\n",
    "        pred_log = jnp.log(pred_mag + 1e-7)\n",
    "        target_log = jnp.log(target_mag + 1e-7)\n",
    "        log_loss = jnp.mean(jnp.abs(pred_log - target_log))\n",
    "\n",
    "        total_loss = total_loss + l1_loss + alpha * log_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Verify loss function\n",
    "test_pred = jnp.sin(jnp.linspace(0, 100, AUDIO_LENGTH))\n",
    "test_target = jnp.sin(jnp.linspace(0, 100.5, AUDIO_LENGTH))  # Slightly different\n",
    "loss_val = multi_scale_spectral_loss(test_pred, test_target)\n",
    "print(f\"Spectral loss (similar signals): {float(loss_val):.4f}\")\n",
    "\n",
    "test_noise_sig = jax.random.normal(jax.random.key(0), (AUDIO_LENGTH,)) * 0.1\n",
    "loss_val_diff = multi_scale_spectral_loss(test_pred, test_noise_sig)\n",
    "print(f\"Spectral loss (signal vs noise): {float(loss_val_diff):.4f}\")\n",
    "print(\"(Higher loss for dissimilar signals — as expected)\")\n",
    "# Expected output:\n",
    "# Spectral loss (similar signals): 4.0301\n",
    "# Spectral loss (signal vs noise): 68.8026\n",
    "# (Higher loss for dissimilar signals — as expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Step 5: DDSP Synthesis via Composite\n",
    "\n",
    "Instead of manually calling each operator's `.apply()` and explicit `jax.vmap()`,\n",
    "we compose the synthesis pipeline using `CompositeOperatorModule`:\n",
    "\n",
    "- **`WEIGHTED_PARALLEL([1.0, 0.1])`**: Runs HarmonicSynth and FilteredNoise on the\n",
    "  same input dict, then computes `1.0 * harmonic_audio + 0.1 * noise_audio`\n",
    "- **`SEQUENTIAL`**: Chains the parallel mix into Reverb\n",
    "\n",
    "```\n",
    "synth_composite = SEQUENTIAL([\n",
    "    WEIGHTED_PARALLEL([HarmonicSynth, FilteredNoise], weights=[1.0, 0.1]),\n",
    "    Reverb,\n",
    "])\n",
    "```\n",
    "\n",
    "Both operators use `{**data, \"audio\": audio}` passthrough, so non-audio keys\n",
    "get weighted-summed but Reverb ignores them — only reading the `audio` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Composite-based DDSP synthesis pipeline\n",
    "def create_synth_composite(\n",
    "    harmonic_synth: HarmonicSynthOperator,\n",
    "    noise_synth: FilteredNoiseOperator,\n",
    "    reverb: ReverbOperator,\n",
    ") -> CompositeOperatorModule:\n",
    "    \"\"\"Create DDSP synthesis composite: (Harmonic | Noise) >> Reverb.\n",
    "\n",
    "    Architecture:\n",
    "        SEQUENTIAL([\n",
    "            WEIGHTED_PARALLEL([HarmonicSynth, FilteredNoise], weights=[1.0, 0.1]),\n",
    "            Reverb,\n",
    "        ])\n",
    "    \"\"\"\n",
    "    synth_parallel = CompositeOperatorModule(\n",
    "        CompositeOperatorConfig(\n",
    "            strategy=CompositionStrategy.WEIGHTED_PARALLEL,\n",
    "            operators=[harmonic_synth, noise_synth],\n",
    "            weights=[1.0, 0.1],\n",
    "        )\n",
    "    )\n",
    "    return CompositeOperatorModule(\n",
    "        CompositeOperatorConfig(\n",
    "            strategy=CompositionStrategy.SEQUENTIAL,\n",
    "            operators=[synth_parallel, reverb],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def synthesize_batch(\n",
    "    decoder: DDSPDecoder,\n",
    "    synth_composite: CompositeOperatorModule,\n",
    "    f0_batch: jax.Array,\n",
    "    loudness_batch: jax.Array,\n",
    "    f0_hz_batch: jax.Array,\n",
    ") -> jax.Array:\n",
    "    \"\"\"Batch DDSP forward pass: decode → composite synthesis.\n",
    "\n",
    "    Uses jax.vmap(decoder) for batched decoding, then delegates to\n",
    "    the composite's native Batch processing (which vmaps internally).\n",
    "\n",
    "    Args:\n",
    "        decoder: RNN-FC decoder\n",
    "        synth_composite: Nested composite (WEIGHTED_PARALLEL >> Reverb)\n",
    "        f0_batch: Scaled f0 per sample (B, n_frames)\n",
    "        loudness_batch: Standardized loudness per sample (B, n_frames)\n",
    "        f0_hz_batch: Raw f0 in Hz per sample (B, n_frames)\n",
    "\n",
    "    Returns:\n",
    "        Synthesized audio batch (B, audio_length)\n",
    "    \"\"\"\n",
    "    # Batched decoding — now returns per-frame params\n",
    "    amps_batch, noise_batch = jax.vmap(decoder)(f0_batch, loudness_batch)\n",
    "    # amps_batch: (B, n_frames, n_harmonics), noise_batch: (B, n_frames, n_noise_bands)\n",
    "\n",
    "    # Pass full time-varying f0 — NO averaging\n",
    "    batch = Batch.from_parts(\n",
    "        data={\n",
    "            \"amplitudes\": amps_batch,  # (B, n_frames, n_harmonics)\n",
    "            \"f0_hz\": f0_hz_batch,  # (B, n_frames)\n",
    "            \"noise_magnitudes\": noise_batch,  # (B, n_frames, n_noise_bands)\n",
    "        },\n",
    "        states={},\n",
    "    )\n",
    "    result = synth_composite(batch)\n",
    "    return result.get_data()[\"audio\"]\n",
    "\n",
    "\n",
    "# Create composite and test synthesis\n",
    "synth_composite = create_synth_composite(h_op, n_op, r_op)\n",
    "test_f0_hz = jnp.ones((1, N_FRAMES)) * 440.0\n",
    "test_synth = synthesize_batch(\n",
    "    decoder,\n",
    "    synth_composite,\n",
    "    test_f0[None],\n",
    "    test_loudness[None],\n",
    "    test_f0_hz,\n",
    ")\n",
    "print(f\"Synthesized audio shape: {test_synth.shape}\")\n",
    "print(f\"Audio range: [{float(test_synth[0].min()):.4f}, {float(test_synth[0].max()):.4f}]\")\n",
    "# Expected output (values vary):\n",
    "# Synthesized audio shape: (1, 64000)\n",
    "# Audio range: [-0.0116, 0.0109]  (small before training — exp_sigmoid starts near-zero)\n",
    "\n",
    "\n",
    "def evaluate_spectral_loss(\n",
    "    decoder: DDSPDecoder,\n",
    "    synth_composite: CompositeOperatorModule,\n",
    "    source: MemorySource,\n",
    "    batch_size: int = 4,\n",
    ") -> float:\n",
    "    \"\"\"Compute average spectral loss over a dataset.\"\"\"\n",
    "    pipeline = from_source(source, batch_size=batch_size)\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for batch in pipeline:\n",
    "        pred = synthesize_batch(\n",
    "            decoder,\n",
    "            synth_composite,\n",
    "            batch[\"f0\"],\n",
    "            batch[\"loudness\"],\n",
    "            batch[\"f0_hz\"],\n",
    "        )\n",
    "        total_loss += float(\n",
    "            multi_scale_spectral_loss(pred, batch[\"audio\"], fft_sizes=cfg.loss_fft_sizes)\n",
    "        )\n",
    "        num_batches += 1\n",
    "    return total_loss / max(num_batches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Step 6: Training Loop\n",
    "\n",
    "Train the DDSP model using multi-scale spectral loss with exponential LR decay\n",
    "and gradient clipping (global norm 3.0), matching the DDSP reference.\n",
    "\n",
    "Gradients flow through: loss → reverb → noise synth + harmonic synth → decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Training\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def ddsp_train_step(\n",
    "    all_params: tuple,\n",
    "    optimizer: nnx.Optimizer,\n",
    "    target_audio: jax.Array,\n",
    "    f0: jax.Array,\n",
    "    loudness: jax.Array,\n",
    "    f0_hz: jax.Array,\n",
    ") -> jax.Array:\n",
    "    \"\"\"JIT-compiled DDSP training step.\"\"\"\n",
    "\n",
    "    def loss_fn(params: tuple) -> jax.Array:\n",
    "        dec, synth_comp = params\n",
    "        pred_audio = synthesize_batch(dec, synth_comp, f0, loudness, f0_hz)\n",
    "        return multi_scale_spectral_loss(pred_audio, target_audio, fft_sizes=cfg.loss_fft_sizes)\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(all_params)\n",
    "    optimizer.update(all_params, grads)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_ddsp(\n",
    "    train_source: MemorySource,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 16,\n",
    "    peak_lr: float = 1e-3,\n",
    "    decay_rate: float = 0.98,\n",
    "    decay_steps: int = 10000,\n",
    ") -> tuple[DDSPDecoder, CompositeOperatorModule, list[float]]:\n",
    "    \"\"\"Train DDSP model on audio data with LR scheduling.\n",
    "\n",
    "    Uses exponential decay matching the DDSP reference:\n",
    "        lr = peak_lr * decay_rate^(step / decay_steps)\n",
    "\n",
    "    Gradient clipping (global norm 3.0) stabilizes training and prevents\n",
    "    the loss spikes observed in unclipped training.\n",
    "\n",
    "    Returns (decoder, synth_composite, loss_history).\n",
    "    \"\"\"\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Create model components\n",
    "    decoder = DDSPDecoder(rngs=rngs)\n",
    "    harmonic_synth = HarmonicSynthOperator(HarmonicSynthConfig())\n",
    "    noise_synth = FilteredNoiseOperator(FilteredNoiseConfig())\n",
    "    reverb = ReverbOperator(ReverbConfig())\n",
    "\n",
    "    # Compose synthesis pipeline\n",
    "    synth_composite = create_synth_composite(harmonic_synth, noise_synth, reverb)\n",
    "\n",
    "    # LR schedule: exponential decay (matching DDSP reference, no warmup)\n",
    "    schedule = optax.exponential_decay(\n",
    "        init_value=peak_lr,\n",
    "        transition_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "    )\n",
    "\n",
    "    # Joint optimizer with gradient clipping (matching DDSP reference: global norm 3.0)\n",
    "    all_params = (decoder, synth_composite)\n",
    "    optimizer = nnx.Optimizer(\n",
    "        all_params,\n",
    "        optax.chain(\n",
    "            optax.clip_by_global_norm(3.0),\n",
    "            optax.adam(schedule),\n",
    "        ),\n",
    "        wrt=nnx.Param,\n",
    "    )\n",
    "\n",
    "    loss_history: list[float] = []\n",
    "\n",
    "    print(f\"Training DDSP: {num_epochs} epochs, batch_size={batch_size}, peak_lr={peak_lr}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        pipeline = from_source(train_source, batch_size=batch_size)\n",
    "        epoch_loss = 0.0\n",
    "        num_steps = 0\n",
    "\n",
    "        for batch in pipeline:\n",
    "            loss = ddsp_train_step(\n",
    "                all_params,\n",
    "                optimizer,\n",
    "                batch[\"audio\"],\n",
    "                batch[\"f0\"],\n",
    "                batch[\"loudness\"],\n",
    "                batch[\"f0_hz\"],\n",
    "            )\n",
    "            epoch_loss += float(loss)\n",
    "            num_steps += 1\n",
    "\n",
    "        avg_loss = epoch_loss / max(num_steps, 1)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"  Epoch {epoch + 1:2d}/{num_epochs} | Spectral Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return decoder, synth_composite, loss_history\n",
    "\n",
    "\n",
    "# Run training\n",
    "print(\"\\n=== DDSP Training ===\\n\")\n",
    "print(\"Note: First step triggers XLA compilation of the full forward+backward pass.\")\n",
    "print(\"This takes 1-3 minutes depending on GPU (lax.scan keeps the graph compact).\\n\")\n",
    "decoder, synth_composite, loss_history = train_ddsp(\n",
    "    train_source,\n",
    "    num_epochs=cfg.num_epochs,\n",
    "    batch_size=cfg.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss curve\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "epochs_range = list(range(1, len(loss_history) + 1))\n",
    "ax.plot(\n",
    "    epochs_range,\n",
    "    loss_history,\n",
    "    \"o-\",\n",
    "    color=\"steelblue\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    ")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Multi-Scale Spectral Loss\")\n",
    "ax.set_title(\"DDSP Training: Spectral Loss Over Epochs (NSynth)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate start and end values\n",
    "if len(loss_history) >= 2:\n",
    "    ax.annotate(\n",
    "        f\"{loss_history[0]:.2f}\",\n",
    "        (1, loss_history[0]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(10, 10),\n",
    "        fontsize=10,\n",
    "        arrowprops={\"arrowstyle\": \"->\", \"color\": \"gray\"},\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"{loss_history[-1]:.2f}\",\n",
    "        (len(loss_history), loss_history[-1]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(10, -15),\n",
    "        fontsize=10,\n",
    "        arrowprops={\"arrowstyle\": \"->\", \"color\": \"gray\"},\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    OUTPUT_DIR / \"perf-ddsp-training-curve.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"white\",\n",
    ")\n",
    "plt.close()\n",
    "print(\"Saved: docs/assets/images/examples/perf-ddsp-training-curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Step 7: Verify Gradient Flow Through All Operators\n",
    "\n",
    "Gradients must flow from the spectral loss back through reverb,\n",
    "noise synth, harmonic synth, and into the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Gradient flow verification\n",
    "print(\"\\n=== Gradient Flow Verification ===\")\n",
    "\n",
    "# Get a test batch\n",
    "verify_pipeline = from_source(test_source, batch_size=4)\n",
    "verify_batch = next(iter(verify_pipeline))\n",
    "target = verify_batch[\"audio\"]\n",
    "f0 = verify_batch[\"f0\"]\n",
    "loudness = verify_batch[\"loudness\"]\n",
    "f0_hz = verify_batch[\"f0_hz\"]\n",
    "\n",
    "all_params = (decoder, synth_composite)\n",
    "\n",
    "\n",
    "def verify_loss(params: tuple) -> jax.Array:\n",
    "    dec, synth_comp = params\n",
    "    pred = synthesize_batch(dec, synth_comp, f0, loudness, f0_hz)\n",
    "    return multi_scale_spectral_loss(pred, target)\n",
    "\n",
    "\n",
    "loss, grads = nnx.value_and_grad(verify_loss)(all_params)\n",
    "grad_leaves = jax.tree.leaves(grads)\n",
    "\n",
    "assert len(grad_leaves) > 0, \"No gradient leaves found\"\n",
    "assert any(jnp.sum(jnp.abs(g)) > 0 for g in grad_leaves), (\n",
    "    \"All gradients are zero — DDSP pipeline is NOT differentiable!\"\n",
    ")\n",
    "\n",
    "print(f\"Loss: {float(loss):.4f}\")\n",
    "print(f\"Gradient flow verified: {len(grad_leaves)} parameter groups\")\n",
    "\n",
    "# Check per-component gradients\n",
    "component_names = [\"Decoder\", \"SynthComposite\"]\n",
    "for name, component_grad in zip(component_names, grads):\n",
    "    leaves = jax.tree.leaves(component_grad)\n",
    "    total_grad_norm = sum(float(jnp.sum(jnp.abs(g))) for g in leaves)\n",
    "    n_params = sum(g.size for g in leaves)\n",
    "    status = \"RECEIVES GRADIENTS\" if total_grad_norm > 0 else \"NO GRADIENTS\"\n",
    "    print(f\"  {name:15s} | params: {n_params:6d} | |grad|: {total_grad_norm:.6f} | {status}\")\n",
    "\n",
    "print(\"\\nSUCCESS: All DDSP components receive gradients!\")\n",
    "# Expected output (values vary per training run):\n",
    "# Loss: 13.4107\n",
    "# Gradient flow verified: 25 parameter groups\n",
    "#   Decoder         | params: 2452646 | |grad|: 12439.834685 | RECEIVES GRADIENTS\n",
    "#   SynthComposite  | params:  16000 | |grad|: 700.408447 | RECEIVES GRADIENTS\n",
    "# SUCCESS: All DDSP components receive gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Step 8: Demonstrate Composite Pipeline Structure\n",
    "\n",
    "The trained `synth_composite` is a nested `CompositeOperatorModule`:\n",
    "`SEQUENTIAL([WEIGHTED_PARALLEL(...), Reverb])`. Let's process a demo batch\n",
    "through it to show the composite in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Demonstrate composite pipeline structure\n",
    "print(\"\\n=== Composite Pipeline Demonstration ===\")\n",
    "print(\"DDSP synthesis uses a nested CompositeOperatorModule:\")\n",
    "print(\"\"\"\n",
    "  synth_composite = SEQUENTIAL([\n",
    "      WEIGHTED_PARALLEL([HarmonicSynth, FilteredNoise], weights=[1.0, 0.1]),\n",
    "      Reverb,\n",
    "  ])\n",
    "\n",
    "  This replaces the manual pattern:\n",
    "      h_audio = harmonic_synth.apply(h_data)[\"audio\"]\n",
    "      n_audio = noise_synth.apply(n_data)[\"audio\"]\n",
    "      combined = h_audio + n_audio * 0.1\n",
    "      reverbed = reverb.apply({\"audio\": combined})[\"audio\"]\n",
    "\"\"\")\n",
    "\n",
    "# Process a batch through the actual trained composite (per-frame data)\n",
    "demo_amps = jnp.ones((1, N_FRAMES, N_HARMONICS)) * 0.1\n",
    "demo_f0 = jnp.ones((1, N_FRAMES)) * 440.0\n",
    "demo_noise_mags = jnp.ones((1, N_FRAMES, N_NOISE_BANDS)) * 0.5\n",
    "\n",
    "demo_batch = Batch.from_parts(\n",
    "    data={\n",
    "        \"amplitudes\": demo_amps,\n",
    "        \"f0_hz\": demo_f0,\n",
    "        \"noise_magnitudes\": demo_noise_mags,\n",
    "    },\n",
    "    states={},\n",
    ")\n",
    "result = synth_composite(demo_batch)\n",
    "result_audio = result.get_data()[\"audio\"]\n",
    "\n",
    "print(\n",
    "    f\"Input: amplitudes={demo_amps.shape}, f0_hz={demo_f0.shape}, \"\n",
    "    f\"noise_mags={demo_noise_mags.shape}\"\n",
    ")\n",
    "print(\n",
    "    f\"Output audio: shape={result_audio.shape}, \"\n",
    "    f\"rms={float(jnp.sqrt(jnp.mean(result_audio**2))):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "### Step 9: Evaluate Resynthesis Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Evaluate on test set\n",
    "print(\"\\n=== Resynthesis Evaluation ===\")\n",
    "\n",
    "avg_test_loss = evaluate_spectral_loss(decoder, synth_composite, test_source)\n",
    "print(f\"Average test spectral loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Compare with random synthesis (untrained)\n",
    "print(\"\\nComparison:\")\n",
    "random_decoder = DDSPDecoder(rngs=nnx.Rngs(999))\n",
    "random_composite = create_synth_composite(\n",
    "    HarmonicSynthOperator(HarmonicSynthConfig()),\n",
    "    FilteredNoiseOperator(FilteredNoiseConfig()),\n",
    "    ReverbOperator(ReverbConfig()),\n",
    ")\n",
    "\n",
    "rand_loss = evaluate_spectral_loss(random_decoder, random_composite, test_source)\n",
    "\n",
    "print(f\"  Random (untrained):  {rand_loss:.4f}\")\n",
    "print(f\"  Trained DDSP:        {avg_test_loss:.4f}\")\n",
    "improvement = ((rand_loss - avg_test_loss) / rand_loss) * 100\n",
    "print(f\"  Improvement:         {improvement:.1f}% lower spectral loss\")\n",
    "# Expected output (varies by training run, 10K samples):\n",
    "#   Random (untrained):  ~30\n",
    "#   Trained DDSP:        ~8-10\n",
    "#   Improvement:         ~65-70% lower spectral loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize resynthesis quality: target vs. synthesized waveforms and spectrograms\n",
    "vis_pipeline = from_source(test_source, batch_size=4)\n",
    "vis_batch = next(iter(vis_pipeline))\n",
    "vis_pred = synthesize_batch(\n",
    "    decoder,\n",
    "    synth_composite,\n",
    "    vis_batch[\"f0\"],\n",
    "    vis_batch[\"loudness\"],\n",
    "    vis_batch[\"f0_hz\"],\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "time_axis_full = np.arange(AUDIO_LENGTH) / SAMPLE_RATE\n",
    "\n",
    "for i in range(2):\n",
    "    target_audio = np.array(vis_batch[\"audio\"][i])\n",
    "    pred_audio = np.array(vis_pred[i])\n",
    "    f0_val = float(vis_batch[\"f0_hz\"][i, N_FRAMES // 2])\n",
    "\n",
    "    # Waveform comparison — show first 4000 samples for detail\n",
    "    n_show = 4000\n",
    "    t_show = time_axis_full[:n_show]\n",
    "\n",
    "    # Target waveform\n",
    "    axes[i * 2, 0].plot(t_show, target_audio[:n_show], color=\"steelblue\", linewidth=0.8)\n",
    "    axes[i * 2, 0].set_title(f\"Target Waveform (f0~{f0_val:.0f} Hz)\", fontsize=10)\n",
    "    axes[i * 2, 0].set_ylabel(\"Amplitude\")\n",
    "    axes[i * 2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Synthesized waveform\n",
    "    axes[i * 2, 1].plot(t_show, pred_audio[:n_show], color=\"darkorange\", linewidth=0.8)\n",
    "    axes[i * 2, 1].set_title(f\"Synthesized Waveform (f0~{f0_val:.0f} Hz)\", fontsize=10)\n",
    "    axes[i * 2, 1].set_ylabel(\"Amplitude\")\n",
    "    axes[i * 2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Target spectrogram\n",
    "    plot_specgram(axes[i * 2 + 1, 0], target_audio)\n",
    "    axes[i * 2 + 1, 0].set_ylabel(\"Frequency (Hz)\")\n",
    "    axes[i * 2 + 1, 0].set_ylim(0, 4000)\n",
    "    axes[i * 2 + 1, 0].set_title(\"Target Spectrogram\")\n",
    "\n",
    "    # Synthesized spectrogram\n",
    "    plot_specgram(axes[i * 2 + 1, 1], pred_audio)\n",
    "    axes[i * 2 + 1, 1].set_ylabel(\"Frequency (Hz)\")\n",
    "    axes[i * 2 + 1, 1].set_ylim(0, 4000)\n",
    "    axes[i * 2 + 1, 1].set_title(\"Synthesized Spectrogram\")\n",
    "\n",
    "# Only last row gets x labels\n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"DDSP Resynthesis: Target vs. Synthesized Audio (NSynth)\\n\"\n",
    "    f\"Spectral Loss: {avg_test_loss:.4f} (trained) vs {rand_loss:.4f} (random) — \"\n",
    "    f\"{improvement:.1f}% improvement\",\n",
    "    fontsize=12,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    OUTPUT_DIR / \"cv-ddsp-resynthesis-comparison.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"white\",\n",
    ")\n",
    "plt.close()\n",
    "print(\"Saved: docs/assets/images/examples/cv-ddsp-resynthesis-comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Step 10: Analyze Learned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Analyze what the model learned\n",
    "def analyze_ddsp(\n",
    "    decoder: DDSPDecoder,\n",
    "    synth_composite: CompositeOperatorModule,\n",
    ") -> None:\n",
    "    \"\"\"Print analysis of learned DDSP parameters.\"\"\"\n",
    "    print(\"\\n=== Learned DDSP Parameters ===\")\n",
    "\n",
    "    # Extract reverb from composite: SEQUENTIAL([WEIGHTED_PARALLEL(...), Reverb])\n",
    "    reverb = list(synth_composite.operators)[1]\n",
    "\n",
    "    # Decoder statistics\n",
    "    dec_params = sum(p.size for p in jax.tree.leaves(nnx.state(decoder, nnx.Param)))\n",
    "    print(f\"\\n1. Decoder: {dec_params:,} parameters\")\n",
    "    print(f\"   GRU hidden dim: {decoder.init_state[...].shape[0]}\")\n",
    "    print(f\"   MLP layers: {len(decoder.mlp_layers)}\")\n",
    "\n",
    "    # Reverb impulse response\n",
    "    ir = reverb.impulse_response[...]\n",
    "    ir_energy = float(jnp.sum(ir**2))\n",
    "    ir_peak = float(jnp.max(jnp.abs(ir)))\n",
    "    ir_length_ms = ir.shape[0] / SAMPLE_RATE * 1000\n",
    "\n",
    "    # Estimate RT60 (time for energy to decay by 60dB)\n",
    "    energy_cumsum = jnp.cumsum(ir**2)\n",
    "    total_energy = energy_cumsum[-1]\n",
    "    rt60_idx = jnp.searchsorted(energy_cumsum, total_energy * 0.999)\n",
    "    rt60_ms = float(rt60_idx) / SAMPLE_RATE * 1000\n",
    "\n",
    "    print(\"\\n2. Reverb Impulse Response:\")\n",
    "    print(f\"   Length: {ir_length_ms:.0f} ms ({ir.shape[0]} samples)\")\n",
    "    print(f\"   Peak amplitude: {ir_peak:.4f}\")\n",
    "    print(f\"   Total energy: {ir_energy:.4f}\")\n",
    "    print(f\"   Estimated RT60: {rt60_ms:.0f} ms\")\n",
    "\n",
    "    # Test synthesis with a known pitch\n",
    "    test_f0 = jnp.ones(N_FRAMES) * 0.543  # ~440 Hz (MIDI 69 / 127)\n",
    "    test_loud = jnp.ones(N_FRAMES) * 0.5  # Moderate loudness (-40 dB)\n",
    "    amps, noise_mags = decoder(test_f0, test_loud)\n",
    "    # amps: (n_frames, n_harmonics), noise_mags: (n_frames, n_noise_bands)\n",
    "\n",
    "    # Time-average for visualization (bar chart of harmonic strengths)\n",
    "    amps_avg = jnp.mean(amps, axis=0)  # (n_harmonics,)\n",
    "    noise_avg = jnp.mean(noise_mags, axis=0)  # (n_noise_bands,)\n",
    "\n",
    "    print(\"\\n3. Synthesis for A4 (440 Hz):\")\n",
    "    print(f\"   Top 5 harmonic amplitudes: {amps_avg[:5]}\")\n",
    "    print(\n",
    "        f\"   Amplitude decay rate: {float(amps_avg[0] / (amps_avg[4] + 1e-8)):.2f}x \"\n",
    "        f\"(fundamental vs 5th harmonic)\"\n",
    "    )\n",
    "    print(f\"   Noise magnitude range: [{float(noise_avg.min()):.3f}, {float(noise_avg.max()):.3f}]\")\n",
    "\n",
    "    # --- Visualization: Impulse response + harmonic amplitudes ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), layout=\"constrained\")\n",
    "\n",
    "    # 1. Learned impulse response\n",
    "    ir_np = np.array(ir)\n",
    "    ir_time = np.arange(len(ir_np)) / SAMPLE_RATE * 1000  # ms\n",
    "    # Show first 200ms for detail\n",
    "    show_ms = 200\n",
    "    show_idx = int(show_ms * SAMPLE_RATE / 1000)\n",
    "    axes[0].plot(ir_time[:show_idx], ir_np[:show_idx], color=\"steelblue\", linewidth=0.5)\n",
    "    axes[0].set_xlabel(\"Time (ms)\")\n",
    "    axes[0].set_ylabel(\"Amplitude\")\n",
    "    axes[0].set_title(f\"Learned Impulse Response (first {show_ms}ms)\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "\n",
    "    # 2. Harmonic amplitudes (bar chart) — time-averaged\n",
    "    amps_np = np.array(amps_avg)\n",
    "    n_show_harmonics = min(20, len(amps_np))\n",
    "    harmonic_nums = np.arange(1, n_show_harmonics + 1)\n",
    "    freqs = harmonic_nums * 440  # Hz\n",
    "    colors = plt.cm.plasma(np.linspace(0.2, 0.8, n_show_harmonics))\n",
    "    axes[1].bar(\n",
    "        harmonic_nums,\n",
    "        amps_np[:n_show_harmonics],\n",
    "        color=colors,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Harmonic Number (k)\")\n",
    "    axes[1].set_ylabel(\"Amplitude\")\n",
    "    axes[1].set_title(\"Learned Harmonic Amplitudes (A4 = 440 Hz)\")\n",
    "    axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    # Add frequency labels for first few harmonics\n",
    "    # Use relative offset so labels scale with amplitude range (avoids\n",
    "    # layout explosion when amps are near-zero in early training)\n",
    "    label_offset = max(float(amps_np[:n_show_harmonics].max()) * 0.08, 1e-8)\n",
    "    for idx in range(min(5, n_show_harmonics)):\n",
    "        axes[1].text(\n",
    "            idx + 1,\n",
    "            float(amps_np[idx]) + label_offset,\n",
    "            f\"{int(freqs[idx])}Hz\",\n",
    "            ha=\"center\",\n",
    "            fontsize=7,\n",
    "            rotation=45,\n",
    "        )\n",
    "\n",
    "    # 3. Noise filter frequency response — time-averaged\n",
    "    noise_np = np.array(exp_sigmoid(noise_avg))\n",
    "    freq_bins = np.linspace(0, SAMPLE_RATE / 2, len(noise_np))\n",
    "    axes[2].fill_between(freq_bins, noise_np, alpha=0.3, color=\"darkorange\")\n",
    "    axes[2].plot(freq_bins, noise_np, color=\"darkorange\", linewidth=1.5)\n",
    "    axes[2].set_xlabel(\"Frequency (Hz)\")\n",
    "    axes[2].set_ylabel(\"Magnitude\")\n",
    "    axes[2].set_title(\"Learned Noise Filter Response\")\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_xlim(0, SAMPLE_RATE / 2)\n",
    "\n",
    "    fig.suptitle(\"DDSP Learned Parameters — Audio Synthesis Analysis\", fontsize=13)\n",
    "    plt.savefig(\n",
    "        OUTPUT_DIR / \"cv-ddsp-learned-parameters.png\",\n",
    "        dpi=150,\n",
    "        facecolor=\"white\",\n",
    "    )\n",
    "    plt.close()\n",
    "    print(\"\\nSaved: docs/assets/images/examples/cv-ddsp-learned-parameters.png\")\n",
    "\n",
    "\n",
    "analyze_ddsp(decoder, synth_composite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory during training\n",
    "\n",
    "**Symptom**: `XlaRuntimeError: RESOURCE_EXHAUSTED` during `train_ddsp()`.\n",
    "\n",
    "**Cause**: Audio batches are large — each sample is 64,000 floats, and the\n",
    "spectral loss computes multiple FFTs per sample. With `batch_size=32`, a single\n",
    "batch uses ~8 MB of audio data, plus intermediate FFT buffers. Loading 10K\n",
    "samples also requires ~2.5 GB RAM for the dataset arrays.\n",
    "\n",
    "**Solution**: Reduce `batch_size` in `train_ddsp()` to 16 or 8, reduce the\n",
    "number of FFT scales in `cfg.loss_fft_sizes`, or use `QUICK_MODE = True` (500\n",
    "samples, 5 epochs).\n",
    "\n",
    "### NSynth dataset download fails or is slow\n",
    "\n",
    "**Symptom**: `load_nsynth()` hangs or fails with network errors.\n",
    "\n",
    "**Cause**: The NSynth TFRecord archive is ~1 GB. Downloads may fail on slow\n",
    "or unstable connections.\n",
    "\n",
    "**Solution**: Set `TFDS_DATA_DIR` to a directory with sufficient space on a\n",
    "fast drive: `export TFDS_DATA_DIR=/path/to/data`. If the download was partially\n",
    "completed, delete the `downloads/` subdirectory and retry.\n",
    "\n",
    "### Training is very slow on CPU\n",
    "\n",
    "**Symptom**: Each epoch takes 10+ minutes.\n",
    "\n",
    "**Cause**: The harmonic synthesizer uses phase accumulation with 100 harmonics\n",
    "per frame — heavy on FLOPs. XLA compilation also takes longer on CPU.\n",
    "\n",
    "**Solution**: Set `QUICK_MODE = True` to use fewer FFT scales, fewer samples\n",
    "(500 vs 10K), and fewer epochs (5 vs 100). GPU is strongly recommended for\n",
    "full training (10K samples, ~3 hrs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results & Evaluation\n",
    "\n",
    "### What We Achieved\n",
    "\n",
    "This example demonstrates datarax's **extensibility** to non-image domains.\n",
    "Three custom `OperatorModule` subclasses for audio DSP — with no changes to\n",
    "datarax's core library — enable a complete differentiable audio synthesis system\n",
    "trained on real NSynth instrument recordings.\n",
    "\n",
    "### Observed Results (Full Training, 10K samples, 100 epochs, ~31K steps)\n",
    "\n",
    "| Configuration | Spectral Loss | Notes |\n",
    "|---------------|---------------|-------|\n",
    "| Random (untrained) | ~30 | Random synth params on real audio |\n",
    "| **Trained DDSP (10K, 100 epochs)** | **~8-10** | ~31K steps matching paper's training budget |\n",
    "| Improvement | **~65-70%** | Relative to untrained baseline |\n",
    "| DDSP (paper, full NSynth) | ~1-3 | Full dataset (290K) + joint encoder-decoder |\n",
    "\n",
    "Training loss drops steadily over 100 epochs with no loss spikes (gradient\n",
    "clipping). The ~31K training steps match the paper's budget. The remaining gap\n",
    "is primarily due to using 10K samples (vs. 290K) and fixed CREPE features\n",
    "(vs. jointly-trained encoder).\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Any domain, same pattern**: Custom audio operators follow the exact same\n",
    "   `OperatorModule` pattern as image operators. No special audio infrastructure needed.\n",
    "\n",
    "2. **3 operators, complete pipeline**: HarmonicSynth + FilteredNoise + Reverb\n",
    "   = complete differentiable audio synthesis. Each has `nnx.Param` for\n",
    "   end-to-end gradient optimization.\n",
    "\n",
    "3. **Parallel + Sequential composition**: `CompositeOperatorModule` naturally\n",
    "   expresses DDSP's architecture: `WEIGHTED_PARALLEL` mixes harmonic + noise,\n",
    "   then `SEQUENTIAL` chains the mix into reverb — no manual `.apply()` calls.\n",
    "\n",
    "4. **Real data, real results**: Training on NSynth instrument recordings\n",
    "   produces recognizable instrument timbres, validating that the architecture\n",
    "   works on real audio (not just synthetic sine waves).\n",
    "\n",
    "5. **Paper-accurate architecture**: The DDSPDecoder uses GRU + MLP (nnx.List\n",
    "   pattern) with per-frame synthesis and phase accumulation in the harmonic\n",
    "   synth, matching the Engel et al. 2020 architecture — time-varying\n",
    "   amplitudes and f0 are upsampled from frame rate to sample rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps & Resources\n",
    "\n",
    "### Try These Experiments\n",
    "\n",
    "1. **Even larger dataset**: Increase `n_train = 30000` (half the GANSynth subset)\n",
    "   for closer-to-paper results (requires ~8 hrs GPU, ~18 GB RAM).\n",
    "\n",
    "2. **Per-frame noise synthesis**: Upgrade `FilteredNoiseOperator` to use\n",
    "   windowed overlap-add for per-frame noise filtering (currently time-averaged).\n",
    "\n",
    "3. **More effects**: Add a differentiable EQ (parametric equalizer) or\n",
    "   compressor operator to the chain.\n",
    "\n",
    "4. **Transfer to new instruments**: Train on strings, then fine-tune\n",
    "   on brass — does the reverb IR transfer?\n",
    "\n",
    "### Related Examples\n",
    "\n",
    "- [DADA Learned Augmentation](01_dada_learned_augmentation_guide.py) —\n",
    "  Differentiable augmentation search (operator library showcase)\n",
    "- [Learned ISP Guide](02_learned_isp_guide.py) — DAG-based differentiable\n",
    "  image processing\n",
    "- [Operators Tutorial](../../core/02_operators_tutorial.py) — Deep dive\n",
    "  into operator patterns\n",
    "\n",
    "### API Reference\n",
    "\n",
    "- [OperatorModule](../../../docs/core/operator.md) — Base class extended by custom operators\n",
    "- [OperatorConfig](../../../docs/core/config.md) — Configuration base class\n",
    "- [MergeBatchNode](../../../docs/dag/nodes.md) — Parallel merge node\n",
    "- [DAGExecutor](../../../docs/dag/dag_executor.md) — Pipeline executor\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [DDSP Paper (arXiv)](https://arxiv.org/abs/2001.04643) — Full paper\n",
    "- [Synthax (JAX DDSP)](https://github.com/PapayaResearch/synthax) — JAX implementation\n",
    "- [Magenta DDSP](https://github.com/magenta/ddsp) — Original TensorFlow implementation\n",
    "- [NSynth Dataset](https://magenta.tensorflow.org/datasets/nsynth) — Real audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"CLI entry point for full DDSP training pipeline.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DDSP: Differentiable Digital Signal Processing\")\n",
    "    print(\"  Dataset: NSynth (real instrument recordings)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load data (full dataset for CLI training — 10K samples, ~31K training steps)\n",
    "    print(\"\\n[1/4] Loading NSynth dataset...\")\n",
    "    data_train, data_test = load_nsynth(n_train=10000, n_test=500)\n",
    "    src_train = MemorySource(MemorySourceConfig(), data=data_train, rngs=nnx.Rngs(0))\n",
    "    src_test = MemorySource(MemorySourceConfig(), data=data_test, rngs=nnx.Rngs(1))\n",
    "    print(f\"  Train: {data_train['audio'].shape}, Test: {data_test['audio'].shape}\")\n",
    "\n",
    "    # Train\n",
    "    print(\"\\n[2/4] Training DDSP model...\")\n",
    "    dec, synth_comp, _ = train_ddsp(src_train, num_epochs=100, batch_size=32)\n",
    "\n",
    "    # Evaluate (reuses the extracted helper — no code duplication)\n",
    "    print(\"\\n[3/4] Evaluating resynthesis quality...\")\n",
    "    test_loss = evaluate_spectral_loss(dec, synth_comp, src_test)\n",
    "    print(f\"  Test spectral loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Analyze\n",
    "    print(\"\\n[4/4] Analyzing learned parameters...\")\n",
    "    analyze_ddsp(dec, synth_comp)\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DDSP training complete!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
