{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# DAG Pipeline Fundamentals Guide\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Advanced |\n",
    "| **Runtime** | ~45 min |\n",
    "| **Prerequisites** | Pipeline Tutorial, Operators Tutorial |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Master the Directed Acyclic Graph (DAG) pipeline architecture in Datarax.\n",
    "This guide covers explicit node construction, control flow, caching strategies,\n",
    "and building production-ready data pipelines with maximum performance.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this guide, you will be able to:\n",
    "\n",
    "1. Construct explicit DAG pipelines with node types\n",
    "2. Use operator-based composition (`>>` and `|` operators)\n",
    "3. Implement control flow patterns (Sequential, Parallel, Branch, Merge)\n",
    "4. Add caching for expensive transformations\n",
    "5. Build rebatch strategies for dynamic batch sizing\n",
    "6. Understand DAG execution and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Coming from PyTorch?\n",
    "\n",
    "| PyTorch | Datarax DAG |\n",
    "|---------|-------------|\n",
    "| `DataLoader(dataset)` | `from_source(source)` or `DataSourceNode(source)` |\n",
    "| `transforms.Compose` | `Sequential([...])` or `node1 >> node2` |\n",
    "| Multiple dataloaders | `Parallel([...])` or `node1 \\\\| node2` |\n",
    "| `collate_fn` | `BatchNode` with custom logic |\n",
    "| N/A (manual caching) | `Cache(node, cache_size=100)` |\n",
    "\n",
    "## Coming from TensorFlow?\n",
    "\n",
    "| TensorFlow tf.data | Datarax DAG |\n",
    "|--------------------|-------------|\n",
    "| `tf.data.Dataset` | `DataSourceNode` |\n",
    "| `dataset.map()` | `OperatorNode(operator)` |\n",
    "| `dataset.batch()` | `BatchNode(batch_size)` |\n",
    "| `dataset.cache()` | `Cache(node)` |\n",
    "| `dataset.prefetch()` | `PrefetchNode(buffer_size)` |\n",
    "| `dataset.shuffle()` | `ShuffleNode(source)` |\n",
    "\n",
    "## Coming from Google Grain?\n",
    "\n",
    "| Grain | Datarax DAG |\n",
    "|-------|-------------|\n",
    "| `grain.DataLoader` | `DAGExecutor` or `from_source()` |\n",
    "| `grain.MapTransform` | `OperatorNode(ElementOperator(...))` |\n",
    "| `grain.Batch` | `BatchNode(batch_size)` |\n",
    "| N/A | `Cache`, `Sequential`, `Parallel`, `Branch` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "uv pip install \"datarax[data]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "\n",
    "from datarax import from_source, DAGExecutor\n",
    "from datarax.dag.nodes import (\n",
    "    Node,\n",
    "    DataSourceNode,\n",
    "    BatchNode,\n",
    "    OperatorNode,\n",
    "    Identity,\n",
    "    Cache,\n",
    ")\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.operators.modality.image import (\n",
    "    BrightnessOperator,\n",
    "    BrightnessOperatorConfig,\n",
    ")\n",
    "from datarax.sources import MemorySource, MemorySourceConfig\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 1: DAG Node Hierarchy\n",
    "\n",
    "Datarax DAG nodes form a hierarchy with specific responsibilities:\n",
    "\n",
    "```\n",
    "Node (base)\n",
    "├── DataSourceNode     - Entry point, wraps data sources\n",
    "├── BatchNode          - Creates batches from elements\n",
    "├── OperatorNode       - Applies transformations\n",
    "├── ShuffleNode        - Shuffles data ordering\n",
    "├── PrefetchNode       - Background data loading\n",
    "├── SamplerNode        - Custom sampling strategies\n",
    "├── SharderNode        - Distributed sharding\n",
    "├── Cache              - LRU caching for expensive ops\n",
    "├── Sequential         - Chain nodes: out₁ → in₂\n",
    "├── Parallel           - Apply multiple nodes to same input\n",
    "├── Branch             - Route data conditionally\n",
    "├── Merge              - Combine parallel branches\n",
    "└── Identity           - Pass-through (useful for composition)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstrations\n",
    "np.random.seed(42)\n",
    "num_samples = 200\n",
    "data = {\n",
    "    \"image\": np.random.rand(num_samples, 32, 32, 3).astype(np.float32),\n",
    "    \"label\": np.random.randint(0, 10, (num_samples,)).astype(np.int32),\n",
    "}\n",
    "\n",
    "print(f\"Created dataset: {num_samples} samples\")\n",
    "print(f\"  image: {data['image'].shape}\")\n",
    "print(f\"  label: {data['label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 2: Simple Pipeline with from_source()\n",
    "\n",
    "The easiest way to build a pipeline is using the `from_source()` helper.\n",
    "It automatically creates the necessary nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using from_source() helper (recommended for simple cases)\n",
    "source = MemorySource(MemorySourceConfig(), data=data, rngs=nnx.Rngs(0))\n",
    "\n",
    "pipeline = from_source(source, batch_size=32)\n",
    "batch = next(iter(pipeline))\n",
    "\n",
    "print(\"Pipeline with from_source():\")\n",
    "print(f\"  Batch shape: {batch['image'].shape}\")\n",
    "print(f\"  Labels: {batch['label'][:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 3: Explicit DAG Construction\n",
    "\n",
    "For more control, construct nodes explicitly. This allows custom\n",
    "configurations and advanced patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Explicit node construction\n",
    "source2 = MemorySource(MemorySourceConfig(), data=data, rngs=nnx.Rngs(1))\n",
    "\n",
    "# Create nodes explicitly\n",
    "data_node = DataSourceNode(source2, name=\"ImageSource\")\n",
    "batch_node = BatchNode(batch_size=32, drop_remainder=False, name=\"Batcher\")\n",
    "\n",
    "# Compose using >> operator (creates Sequential)\n",
    "explicit_pipeline = data_node >> batch_node\n",
    "\n",
    "print(\"Explicit DAG construction:\")\n",
    "print(f\"  Pipeline: {explicit_pipeline}\")\n",
    "print(f\"  Type: {type(explicit_pipeline).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the explicit pipeline\n",
    "executor = DAGExecutor(explicit_pipeline, data_source=source2)\n",
    "\n",
    "batch_count = 0\n",
    "sample_count = 0\n",
    "for batch in executor:\n",
    "    batch_count += 1\n",
    "    sample_count += batch[\"image\"].shape[0]\n",
    "\n",
    "print(f\"  Processed {batch_count} batches, {sample_count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 4: Adding Operators to the Pipeline\n",
    "\n",
    "Use `OperatorNode` to wrap operators and add them to the DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a normalization operator\n",
    "def normalize_fn(element, key=None):\n",
    "    \"\"\"Normalize images to [0, 1] range.\"\"\"\n",
    "    del key  # Unused - deterministic\n",
    "    image = element.data[\"image\"]\n",
    "    normalized = (image - jnp.min(image)) / (jnp.max(image) - jnp.min(image) + 1e-8)\n",
    "    return element.update_data({\"image\": normalized})\n",
    "\n",
    "\n",
    "normalize_op = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False),\n",
    "    fn=normalize_fn,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "# Create brightness operator\n",
    "brightness_op = BrightnessOperator(\n",
    "    BrightnessOperatorConfig(\n",
    "        field_key=\"image\",\n",
    "        brightness_range=(-0.1, 0.1),\n",
    "        stochastic=True,\n",
    "        stream_name=\"brightness\",\n",
    "    ),\n",
    "    rngs=nnx.Rngs(brightness=100),\n",
    ")\n",
    "\n",
    "# Wrap in OperatorNodes\n",
    "normalize_node = OperatorNode(normalize_op, name=\"Normalize\")\n",
    "brightness_node = OperatorNode(brightness_op, name=\"Brightness\")\n",
    "\n",
    "print(\"Created operator nodes:\")\n",
    "print(f\"  - {normalize_node}\")\n",
    "print(f\"  - {brightness_node}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline with operators using >> operator\n",
    "source3 = MemorySource(MemorySourceConfig(), data=data, rngs=nnx.Rngs(2))\n",
    "data_node3 = DataSourceNode(source3, name=\"Source\")\n",
    "batch_node3 = BatchNode(batch_size=32, name=\"Batch\")\n",
    "\n",
    "# Chain: Source >> Batch >> Normalize >> Brightness\n",
    "pipeline_with_ops = data_node3 >> batch_node3 >> normalize_node >> brightness_node\n",
    "\n",
    "print()\n",
    "print(\"Pipeline with operators:\")\n",
    "print(f\"  {pipeline_with_ops}\")\n",
    "\n",
    "# Execute\n",
    "executor = DAGExecutor(pipeline_with_ops, data_source=source3)\n",
    "batch = next(iter(executor))\n",
    "\n",
    "print(f\"  Output range: [{batch['image'].min():.4f}, {batch['image'].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 5: Sequential and Parallel Composition\n",
    "\n",
    "DAG nodes support operator-based composition:\n",
    "\n",
    "- `>>` creates `Sequential`: `node1 >> node2 >> node3`\n",
    "- `|` creates `Parallel`: `node1 | node2 | node3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential composition - chain operators\n",
    "op1 = Identity(name=\"Op1\")\n",
    "op2 = Identity(name=\"Op2\")\n",
    "op3 = Identity(name=\"Op3\")\n",
    "\n",
    "# Using >> operator\n",
    "sequential = op1 >> op2 >> op3\n",
    "print(\"Sequential composition:\")\n",
    "print(f\"  {sequential}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel composition - multiple branches\n",
    "branch_a = Identity(name=\"BranchA\")\n",
    "branch_b = Identity(name=\"BranchB\")\n",
    "\n",
    "# Using | operator\n",
    "parallel = branch_a | branch_b\n",
    "print()\n",
    "print(\"Parallel composition:\")\n",
    "print(f\"  {parallel}\")\n",
    "print(\"  Returns list of outputs from each branch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined composition pattern: branch then merge\n",
    "# Useful for multi-view augmentation\n",
    "\n",
    "\n",
    "def create_augment_branch(name: str, delta: float, seed: int):\n",
    "    \"\"\"Create an augmentation branch.\"\"\"\n",
    "    op = BrightnessOperator(\n",
    "        BrightnessOperatorConfig(\n",
    "            field_key=\"image\",\n",
    "            brightness_range=(delta, delta),\n",
    "            stochastic=False,\n",
    "        ),\n",
    "        rngs=nnx.Rngs(seed),\n",
    "    )\n",
    "    return OperatorNode(op, name=name)\n",
    "\n",
    "\n",
    "# Create parallel augmentation branches\n",
    "bright_branch = create_augment_branch(\"Brighten\", 0.2, 1)\n",
    "dark_branch = create_augment_branch(\"Darken\", -0.2, 2)\n",
    "\n",
    "# Parallel branches\n",
    "multi_view = bright_branch | dark_branch\n",
    "\n",
    "print()\n",
    "print(\"Multi-view augmentation:\")\n",
    "print(f\"  {multi_view}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 6: Caching Expensive Operations\n",
    "\n",
    "Use `Cache` to store results of expensive transformations.\n",
    "Particularly useful for deterministic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate expensive operation\n",
    "def expensive_transform(element, key=None):\n",
    "    \"\"\"Simulate expensive transformation (e.g., feature extraction).\"\"\"\n",
    "    del key\n",
    "    image = element.data[\"image\"]\n",
    "    # Simulate computation with multiple operations\n",
    "    features = jnp.mean(image, axis=(0, 1))  # Simple pooling\n",
    "    features = jnp.sqrt(jnp.abs(features) + 1e-8)  # Non-linear transform\n",
    "    return element.update_data({\"features\": features, \"image\": image})\n",
    "\n",
    "\n",
    "expensive_op = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False),\n",
    "    fn=expensive_transform,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "# Wrap with caching\n",
    "cached_op = Cache(OperatorNode(expensive_op, name=\"ExpensiveOp\"), cache_size=100)\n",
    "\n",
    "print(\"Caching setup:\")\n",
    "print(f\"  Wrapped: {cached_op.node}\")\n",
    "print(f\"  Cache size: {cached_op.cache_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 7: Shuffle and Prefetch Nodes\n",
    "\n",
    "Add shuffling and prefetching for training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training-ready pipeline with shuffle and prefetch\n",
    "source4 = MemorySource(\n",
    "    MemorySourceConfig(shuffle=True, seed=42),  # Shuffled source\n",
    "    data=data,\n",
    "    rngs=nnx.Rngs(3),\n",
    ")\n",
    "\n",
    "# Build training pipeline\n",
    "training_pipeline = (\n",
    "    from_source(source4, batch_size=32)\n",
    "    .add(normalize_node)  # Normalize\n",
    "    .add(brightness_node)  # Augment\n",
    ")\n",
    "\n",
    "print(\"Training pipeline with shuffling:\")\n",
    "print(\"  Source → Shuffle → Batch → Normalize → Brightness\")\n",
    "\n",
    "# Process a few batches\n",
    "for i, batch in enumerate(training_pipeline):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"  Batch {i}: mean={batch['image'].mean():.4f}, std={batch['image'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 8: Building Production Pipelines\n",
    "\n",
    "Combine all concepts for a production-ready pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_production_pipeline(data, batch_size=32, shuffle=True):\n",
    "    \"\"\"Build a complete production pipeline.\n",
    "\n",
    "    Pipeline structure:\n",
    "        Source → Shuffle? → Batch → Normalize → Augment → Output\n",
    "\n",
    "    Args:\n",
    "        data: Dictionary of arrays\n",
    "        batch_size: Batch size\n",
    "        shuffle: Whether to shuffle\n",
    "\n",
    "    Returns:\n",
    "        Configured pipeline\n",
    "    \"\"\"\n",
    "    # Create source\n",
    "    source = MemorySource(\n",
    "        MemorySourceConfig(shuffle=shuffle, seed=42),\n",
    "        data=data,\n",
    "        rngs=nnx.Rngs(0),\n",
    "    )\n",
    "\n",
    "    # Create operators\n",
    "    norm_op = ElementOperator(\n",
    "        ElementOperatorConfig(stochastic=False),\n",
    "        fn=normalize_fn,\n",
    "        rngs=nnx.Rngs(0),\n",
    "    )\n",
    "\n",
    "    augment_op = BrightnessOperator(\n",
    "        BrightnessOperatorConfig(\n",
    "            field_key=\"image\",\n",
    "            brightness_range=(-0.15, 0.15),\n",
    "            stochastic=True,\n",
    "            stream_name=\"aug\",\n",
    "        ),\n",
    "        rngs=nnx.Rngs(aug=100),\n",
    "    )\n",
    "\n",
    "    # Build pipeline\n",
    "    pipeline = (\n",
    "        from_source(source, batch_size=batch_size)\n",
    "        .add(OperatorNode(norm_op, name=\"Normalize\"))\n",
    "        .add(OperatorNode(augment_op, name=\"Augment\"))\n",
    "    )\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# Create and run production pipeline\n",
    "prod_pipeline = build_production_pipeline(data, batch_size=64)\n",
    "\n",
    "print(\"Production pipeline:\")\n",
    "total_batches = 0\n",
    "total_samples = 0\n",
    "for batch in prod_pipeline:\n",
    "    total_batches += 1\n",
    "    total_samples += batch[\"image\"].shape[0]\n",
    "\n",
    "print(f\"  Batches: {total_batches}\")\n",
    "print(f\"  Samples: {total_samples}\")\n",
    "print(\"  Batch size: 64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 9: DAG Execution Patterns\n",
    "\n",
    "Understanding how DAG execution works helps optimize pipelines.\n",
    "\n",
    "### Execution Model\n",
    "\n",
    "1. **Pull-based**: Data is pulled through the graph on iteration\n",
    "2. **Lazy evaluation**: Nodes only execute when outputs are needed\n",
    "3. **State tracking**: NNX modules maintain state across iterations\n",
    "\n",
    "### Key Optimization Points\n",
    "\n",
    "| Optimization | Pattern |\n",
    "|--------------|---------|\n",
    "| Caching | Use `Cache` for deterministic expensive ops |\n",
    "| Batching | Batch early in the pipeline for vmap efficiency |\n",
    "| Operator fusion | Combine multiple light ops into single function |\n",
    "| Memory reuse | Use `drop_remainder=True` for fixed batch shapes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate lazy evaluation\n",
    "print(\"DAG Execution Demonstration:\")\n",
    "print()\n",
    "\n",
    "\n",
    "class TrackedNode(Node):\n",
    "    \"\"\"Node that tracks when it's executed.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        super().__init__(name=name)\n",
    "        self.call_count = nnx.Variable(0)\n",
    "\n",
    "    def __call__(self, data, *, key=None):\n",
    "        del key  # Unused\n",
    "        self.call_count.value += 1\n",
    "        return data\n",
    "\n",
    "\n",
    "# Create tracked pipeline\n",
    "tracked_a = TrackedNode(\"NodeA\")\n",
    "tracked_b = TrackedNode(\"NodeB\")\n",
    "tracked_seq = tracked_a >> tracked_b\n",
    "\n",
    "# Execute once\n",
    "_ = tracked_seq({\"test\": 1})\n",
    "\n",
    "print(\"After 1 execution:\")\n",
    "print(f\"  NodeA calls: {tracked_a.call_count.value}\")\n",
    "print(f\"  NodeB calls: {tracked_b.call_count.value}\")\n",
    "\n",
    "# Execute again\n",
    "_ = tracked_seq({\"test\": 2})\n",
    "_ = tracked_seq({\"test\": 3})\n",
    "\n",
    "print(\"After 3 executions:\")\n",
    "print(f\"  NodeA calls: {tracked_a.call_count.value}\")\n",
    "print(f\"  NodeB calls: {tracked_b.call_count.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "### DAG Node Types\n",
    "\n",
    "| Node | Purpose | Example |\n",
    "|------|---------|---------|\n",
    "| `DataSourceNode` | Pipeline entry point | `DataSourceNode(source)` |\n",
    "| `BatchNode` | Create batches | `BatchNode(batch_size=32)` |\n",
    "| `OperatorNode` | Apply transforms | `OperatorNode(operator)` |\n",
    "| `ShuffleNode` | Randomize order | Via `MemorySourceConfig(shuffle=True)` |\n",
    "| `Cache` | Store results | `Cache(node, cache_size=100)` |\n",
    "| `Sequential` | Chain operations | `node1 >> node2` |\n",
    "| `Parallel` | Multiple branches | `node1 \\\\| node2` |\n",
    "\n",
    "### Composition Operators\n",
    "\n",
    "| Operator | Creates | Usage |\n",
    "|----------|---------|-------|\n",
    "| `>>` | Sequential | `a >> b >> c` |\n",
    "| `\\\\|` | Parallel | `a \\\\| b \\\\| c` |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use `from_source()`** for simple pipelines\n",
    "2. **Explicit construction** for complex DAGs\n",
    "3. **Cache deterministic** expensive operations\n",
    "4. **Batch early** for vmap efficiency\n",
    "5. **Shuffle at source** level for memory efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- [Composition Strategies](../../core/08_composition_strategies_tutorial.ipynb) - Composition\n",
    "- [Sharding Guide](../distributed/02_sharding_guide.ipynb) - Distributed pipelines\n",
    "- [Performance Guide](../performance/01_optimization_guide.ipynb) - Optimization tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the DAG fundamentals guide.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DAG Pipeline Fundamentals Guide\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create data\n",
    "    np.random.seed(42)\n",
    "    data = {\n",
    "        \"image\": np.random.rand(100, 32, 32, 3).astype(np.float32),\n",
    "        \"label\": np.random.randint(0, 10, (100,)).astype(np.int32),\n",
    "    }\n",
    "\n",
    "    # Demo 1: Simple pipeline\n",
    "    print()\n",
    "    print(\"1. Simple Pipeline (from_source):\")\n",
    "    source = MemorySource(MemorySourceConfig(), data=data, rngs=nnx.Rngs(0))\n",
    "    pipeline = from_source(source, batch_size=16)\n",
    "    batch = next(iter(pipeline))\n",
    "    print(f\"   Batch shape: {batch['image'].shape}\")\n",
    "\n",
    "    # Demo 2: Operator composition\n",
    "    print()\n",
    "    print(\"2. Sequential Composition (>> operator):\")\n",
    "    op1 = Identity(name=\"A\")\n",
    "    op2 = Identity(name=\"B\")\n",
    "    seq = op1 >> op2\n",
    "    print(f\"   Pipeline: {seq}\")\n",
    "\n",
    "    # Demo 3: Parallel composition\n",
    "    print()\n",
    "    print(\"3. Parallel Composition (| operator):\")\n",
    "    branch_a = Identity(name=\"Left\")\n",
    "    branch_b = Identity(name=\"Right\")\n",
    "    par = branch_a | branch_b\n",
    "    print(f\"   Pipeline: {par}\")\n",
    "\n",
    "    # Demo 4: Production pipeline\n",
    "    print()\n",
    "    print(\"4. Production Pipeline:\")\n",
    "    prod = build_production_pipeline(data, batch_size=32)\n",
    "    count = sum(1 for _ in prod)\n",
    "    print(f\"   Processed {count} batches\")\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Guide completed successfully!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
