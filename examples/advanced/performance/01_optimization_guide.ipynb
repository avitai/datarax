{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Performance Optimization Guide\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Advanced |\n",
    "| **Runtime** | ~60 min |\n",
    "| **Prerequisites** | Pipeline Tutorial, Monitoring Quick Reference |\n",
    "| **Format** | Python + Jupyter |\n",
    "| **Memory** | ~2 GB RAM |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Master data pipeline performance optimization for Datarax. This guide covers\n",
    "profiling techniques, batch size tuning, operator optimization, and\n",
    "comprehensive benchmarking methodology.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this guide, you will be able to:\n",
    "\n",
    "1. Profile pipeline performance to identify bottlenecks\n",
    "2. Optimize batch size for your hardware\n",
    "3. Measure and improve operator throughput\n",
    "4. Compare different pipeline configurations\n",
    "5. Generate performance benchmarks and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "uv pip install \"datarax[tfds]\" matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Memory Configuration\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES_FOR_TF\"] = \"\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "# Core imports\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "\n",
    "# Datarax imports\n",
    "from datarax import from_source\n",
    "from datarax.dag.nodes import OperatorNode\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.operators.modality.image import (\n",
    "    BrightnessOperator,\n",
    "    BrightnessOperatorConfig,\n",
    "    ContrastOperator,\n",
    "    ContrastOperatorConfig,\n",
    "    NoiseOperator,\n",
    "    NoiseOperatorConfig,\n",
    "    RotationOperator,\n",
    "    RotationOperatorConfig,\n",
    ")\n",
    "from datarax.sources import MemorySource, MemorySourceConfig\n",
    "\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 1: Understanding Pipeline Performance\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "| Metric | Definition | Target |\n",
    "|--------|------------|--------|\n",
    "| **Throughput** | Samples/second | Maximize |\n",
    "| **Latency** | Time per batch | Minimize |\n",
    "| **Memory** | Peak RAM usage | Within limits |\n",
    "| **Utilization** | CPU/GPU usage | High |\n",
    "\n",
    "### Common Bottlenecks\n",
    "\n",
    "| Bottleneck | Symptom | Solution |\n",
    "|------------|---------|----------|\n",
    "| I/O bound | Low CPU usage | Increase prefetch |\n",
    "| CPU bound | High CPU, low throughput | JIT compile, vectorize |\n",
    "| Memory bound | OOM errors | Reduce batch size, stream |\n",
    "| GPU idle | Low GPU util | Increase batch size |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 2: Benchmarking Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineBenchmark:\n",
    "    \"\"\"Utility for benchmarking pipeline configurations.\"\"\"\n",
    "\n",
    "    def __init__(self, warmup_batches: int = 5, measure_batches: int = 50):\n",
    "        self.warmup_batches = warmup_batches\n",
    "        self.measure_batches = measure_batches\n",
    "        self.results = []\n",
    "\n",
    "    def benchmark(self, pipeline, name: str = \"Pipeline\") -> dict:\n",
    "        \"\"\"Benchmark a pipeline and return metrics.\"\"\"\n",
    "        # Warmup\n",
    "        warmup_count = 0\n",
    "        for batch in pipeline:\n",
    "            _ = batch[\"image\"].block_until_ready()\n",
    "            warmup_count += 1\n",
    "            if warmup_count >= self.warmup_batches:\n",
    "                break\n",
    "\n",
    "        # Need fresh pipeline after warmup\n",
    "        # (For real benchmarks, create new pipeline)\n",
    "\n",
    "        # Measurement\n",
    "        latencies = []\n",
    "        samples = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        start_total = time.time()\n",
    "        for batch in pipeline:\n",
    "            start_batch = time.time()\n",
    "            _ = batch[\"image\"].block_until_ready()\n",
    "            latencies.append(time.time() - start_batch)\n",
    "\n",
    "            samples += batch[\"image\"].shape[0]\n",
    "            batch_count += 1\n",
    "\n",
    "            if batch_count >= self.measure_batches + self.warmup_batches:\n",
    "                break\n",
    "\n",
    "        total_time = time.time() - start_total\n",
    "\n",
    "        # Compute metrics\n",
    "        measured_latencies = latencies[self.warmup_batches :]\n",
    "        result = {\n",
    "            \"name\": name,\n",
    "            \"total_samples\": samples,\n",
    "            \"total_time\": total_time,\n",
    "            \"throughput\": samples / total_time if total_time > 0 else 0,\n",
    "            \"avg_latency_ms\": np.mean(measured_latencies) * 1000 if measured_latencies else 0,\n",
    "            \"p50_latency_ms\": (\n",
    "                np.percentile(measured_latencies, 50) * 1000 if measured_latencies else 0\n",
    "            ),\n",
    "            \"p95_latency_ms\": (\n",
    "                np.percentile(measured_latencies, 95) * 1000 if measured_latencies else 0\n",
    "            ),\n",
    "            \"p99_latency_ms\": (\n",
    "                np.percentile(measured_latencies, 99) * 1000 if measured_latencies else 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "\n",
    "\n",
    "benchmark = PipelineBenchmark(warmup_batches=3, measure_batches=30)\n",
    "print(\"PipelineBenchmark utility created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 3: Batch Size Optimization\n",
    "\n",
    "Finding the optimal batch size balances throughput and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "NUM_SAMPLES = 5000\n",
    "IMAGE_SHAPE = (32, 32, 3)\n",
    "\n",
    "np.random.seed(42)\n",
    "test_data = {\n",
    "    \"image\": np.random.rand(NUM_SAMPLES, *IMAGE_SHAPE).astype(np.float32),\n",
    "    \"label\": np.random.randint(0, 10, (NUM_SAMPLES,)).astype(np.int32),\n",
    "}\n",
    "\n",
    "print(f\"Test data: {NUM_SAMPLES} samples, shape={IMAGE_SHAPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(element, key=None):  # noqa: ARG001\n",
    "    \"\"\"Simple normalization.\"\"\"\n",
    "    del key\n",
    "    image = element.data[\"image\"] / 255.0\n",
    "    return element.update_data({\"image\": image})\n",
    "\n",
    "\n",
    "def create_memory_pipeline(data, batch_size):\n",
    "    \"\"\"Create pipeline from memory data.\"\"\"\n",
    "    source = MemorySource(MemorySourceConfig(), data=data, rngs=nnx.Rngs(0))\n",
    "    prep = ElementOperator(ElementOperatorConfig(stochastic=False), fn=preprocess, rngs=nnx.Rngs(0))\n",
    "    return from_source(source, batch_size=batch_size).add(OperatorNode(prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different batch sizes with Datarax DAG pipeline\n",
    "batch_sizes = [8, 16, 32, 64, 128, 256, 512]\n",
    "batch_results = []\n",
    "\n",
    "print(\"\\nBatch Size Sweep (Datarax Pipeline):\")\n",
    "for bs in batch_sizes:\n",
    "    # Run multiple trials\n",
    "    throughputs = []\n",
    "    for trial in range(3):\n",
    "        pipeline = create_memory_pipeline(test_data, bs)\n",
    "\n",
    "        samples = 0\n",
    "        start = time.time()\n",
    "        for i, batch in enumerate(pipeline):\n",
    "            if i >= 50:\n",
    "                break\n",
    "            _ = batch[\"image\"].block_until_ready()\n",
    "            samples += batch[\"image\"].shape[0]\n",
    "        elapsed = time.time() - start\n",
    "        throughputs.append(samples / elapsed)\n",
    "\n",
    "    avg_tp = np.mean(throughputs)\n",
    "    batch_results.append({\"batch_size\": bs, \"throughput\": avg_tp, \"std\": np.std(throughputs)})\n",
    "    print(f\"  Batch {bs:4d}: {avg_tp:,.0f} samples/s (Â±{np.std(throughputs):.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"docs/assets/images/examples\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plot batch size sweep\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "bs_list = [r[\"batch_size\"] for r in batch_results]\n",
    "tp_list = [r[\"throughput\"] for r in batch_results]\n",
    "std_list = [r[\"std\"] for r in batch_results]\n",
    "\n",
    "# Throughput vs batch size\n",
    "ax1 = axes[0]\n",
    "ax1.errorbar(bs_list, tp_list, yerr=std_list, fmt=\"o-\", capsize=5, linewidth=2, markersize=8)\n",
    "ax1.set_xlabel(\"Batch Size\")\n",
    "ax1.set_ylabel(\"Throughput (samples/second)\")\n",
    "ax1.set_title(\"Throughput vs Batch Size\")\n",
    "ax1.set_xscale(\"log\", base=2)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark optimal\n",
    "optimal_idx = np.argmax(tp_list)\n",
    "ax1.axvline(x=bs_list[optimal_idx], color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "ax1.annotate(\n",
    "    f\"Optimal: {bs_list[optimal_idx]}\",\n",
    "    xy=(bs_list[optimal_idx], tp_list[optimal_idx]),\n",
    "    xytext=(bs_list[optimal_idx] * 1.5, tp_list[optimal_idx] * 0.95),\n",
    "    arrowprops=dict(arrowstyle=\"->\"),\n",
    ")\n",
    "\n",
    "# Throughput as bar chart with values labeled\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar([str(bs) for bs in bs_list], tp_list, color=\"steelblue\")\n",
    "ax2.set_xlabel(\"Batch Size\")\n",
    "ax2.set_ylabel(\"Throughput (samples/second)\")\n",
    "ax2.set_title(\"Throughput by Batch Size\")\n",
    "\n",
    "# Highlight optimal\n",
    "optimal_tp = max(tp_list)\n",
    "for bar, tp in zip(bars, tp_list):\n",
    "    color = \"darkgreen\" if tp == optimal_tp else \"black\"\n",
    "    x_pos = bar.get_x() + bar.get_width() / 2\n",
    "    ax2.text(x_pos, tp + 100, f\"{tp:,.0f}\", ha=\"center\", fontsize=8, color=color)\n",
    "\n",
    "ax2.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    output_dir / \"perf-batch-size-sweep.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\"\n",
    ")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'perf-batch-size-sweep.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 4: Operator Performance Comparison\n",
    "\n",
    "Compare throughput of different augmentation operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_operator_pipeline(data, operator, batch_size=64):\n",
    "    \"\"\"Create pipeline with specific operator.\"\"\"\n",
    "    source = MemorySource(MemorySourceConfig(), data=data, rngs=nnx.Rngs(0))\n",
    "    prep = ElementOperator(ElementOperatorConfig(stochastic=False), fn=preprocess, rngs=nnx.Rngs(0))\n",
    "\n",
    "    pipeline = from_source(source, batch_size=batch_size).add(OperatorNode(prep))\n",
    "\n",
    "    if operator is not None:\n",
    "        pipeline = pipeline.add(OperatorNode(operator))\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def benchmark_operator(name, operator, data, num_batches=30):\n",
    "    \"\"\"Benchmark a single operator.\"\"\"\n",
    "    pipeline = create_operator_pipeline(data, operator)\n",
    "\n",
    "    # Warmup\n",
    "    for i, batch in enumerate(pipeline):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        _ = batch[\"image\"].block_until_ready()\n",
    "\n",
    "    # Measure\n",
    "    pipeline = create_operator_pipeline(data, operator)\n",
    "    latencies = []\n",
    "    for i, batch in enumerate(pipeline):\n",
    "        if i >= num_batches + 5:\n",
    "            break\n",
    "        start = time.time()\n",
    "        _ = batch[\"image\"].block_until_ready()\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    measured = latencies[5:]\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"avg_ms\": np.mean(measured) * 1000,\n",
    "        \"p50_ms\": np.percentile(measured, 50) * 1000,\n",
    "        \"p95_ms\": np.percentile(measured, 95) * 1000,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark operators\n",
    "operators = {\n",
    "    \"Baseline\": None,\n",
    "    \"Brightness\": BrightnessOperator(\n",
    "        BrightnessOperatorConfig(\n",
    "            field_key=\"image\", brightness_range=(-0.2, 0.2), stochastic=True, stream_name=\"b\"\n",
    "        ),\n",
    "        rngs=nnx.Rngs(b=1),\n",
    "    ),\n",
    "    \"Contrast\": ContrastOperator(\n",
    "        ContrastOperatorConfig(\n",
    "            field_key=\"image\", contrast_range=(0.8, 1.2), stochastic=True, stream_name=\"c\"\n",
    "        ),\n",
    "        rngs=nnx.Rngs(c=2),\n",
    "    ),\n",
    "    \"Rotation\": RotationOperator(\n",
    "        RotationOperatorConfig(field_key=\"image\", angle_range=(-15, 15)),\n",
    "        rngs=nnx.Rngs(0),\n",
    "    ),\n",
    "    \"Noise\": NoiseOperator(\n",
    "        NoiseOperatorConfig(\n",
    "            field_key=\"image\", mode=\"gaussian\", noise_std=0.1, stochastic=True, stream_name=\"n\"\n",
    "        ),\n",
    "        rngs=nnx.Rngs(n=3),\n",
    "    ),\n",
    "}\n",
    "\n",
    "op_results = []\n",
    "print(\"\\nOperator Benchmarks:\")\n",
    "for name, op in operators.items():\n",
    "    result = benchmark_operator(name, op, test_data)\n",
    "    op_results.append(result)\n",
    "    print(f\"  {name:12s}: {result['avg_ms']:6.2f} ms (p95: {result['p95_ms']:.2f} ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot operator comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = [r[\"name\"] for r in op_results]\n",
    "avg_times = [r[\"avg_ms\"] for r in op_results]\n",
    "p95_times = [r[\"p95_ms\"] for r in op_results]\n",
    "\n",
    "# Average latency\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(names)))\n",
    "bars = ax1.barh(names, avg_times, color=colors)\n",
    "ax1.set_xlabel(\"Latency (ms)\")\n",
    "ax1.set_title(\"Average Operator Latency per Batch\")\n",
    "for bar, val in zip(bars, avg_times):\n",
    "    ax1.text(val + 0.5, bar.get_y() + bar.get_height() / 2, f\"{val:.1f}\", va=\"center\")\n",
    "\n",
    "# P95 vs Average\n",
    "ax2 = axes[1]\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "ax2.bar(x - width / 2, avg_times, width, label=\"Average\", color=\"steelblue\")\n",
    "ax2.bar(x + width / 2, p95_times, width, label=\"P95\", color=\"coral\")\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(names, rotation=45, ha=\"right\")\n",
    "ax2.set_ylabel(\"Latency (ms)\")\n",
    "ax2.set_title(\"Average vs P95 Latency\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    output_dir / \"perf-throughput-comparison.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\"\n",
    ")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'perf-throughput-comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 5: Latency Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect latency distributions\n",
    "latency_distributions = {}\n",
    "\n",
    "for name, op in operators.items():\n",
    "    pipeline = create_operator_pipeline(test_data, op)\n",
    "    latencies = []\n",
    "\n",
    "    for i, batch in enumerate(pipeline):\n",
    "        if i >= 100:\n",
    "            break\n",
    "        start = time.time()\n",
    "        _ = batch[\"image\"].block_until_ready()\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "\n",
    "    latency_distributions[name] = latencies[10:]  # Skip warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latency distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (name, latencies) in zip(axes, latency_distributions.items()):\n",
    "    ax.hist(latencies, bins=30, color=\"steelblue\", edgecolor=\"white\", alpha=0.7)\n",
    "    mean_lat = np.mean(latencies)\n",
    "    p95_lat = np.percentile(latencies, 95)\n",
    "    ax.axvline(x=mean_lat, color=\"red\", linestyle=\"--\", label=f\"Mean: {mean_lat:.2f}\")\n",
    "    ax.axvline(x=p95_lat, color=\"orange\", linestyle=\"--\", label=f\"P95: {p95_lat:.2f}\")\n",
    "    ax.set_xlabel(\"Latency (ms)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"{name} Latency Distribution\")\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Hide extra subplot\n",
    "if len(axes) > len(latency_distributions):\n",
    "    axes[-1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    output_dir / \"perf-latency-distribution.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\"\n",
    ")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'perf-latency-distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 6: Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate memory usage\n",
    "\n",
    "\n",
    "def estimate_batch_memory(batch_size, image_shape, dtype_bytes=4):\n",
    "    \"\"\"Estimate memory for a batch.\"\"\"\n",
    "    image_mem = batch_size * np.prod(image_shape) * dtype_bytes\n",
    "    label_mem = batch_size * 4  # int32\n",
    "    overhead = 1.2  # JAX/NumPy overhead factor\n",
    "    return (image_mem + label_mem) * overhead\n",
    "\n",
    "\n",
    "# Memory vs batch size\n",
    "memory_estimates = []\n",
    "for bs in batch_sizes:\n",
    "    mem = estimate_batch_memory(bs, IMAGE_SHAPE)\n",
    "    memory_estimates.append({\"batch_size\": bs, \"memory_mb\": mem / 1e6})\n",
    "    print(f\"Batch {bs}: ~{mem / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot memory profile\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Memory vs batch size\n",
    "ax1 = axes[0]\n",
    "mem_values = [m[\"memory_mb\"] for m in memory_estimates]\n",
    "ax1.plot(batch_sizes, mem_values, \"o-\", linewidth=2, markersize=8)\n",
    "ax1.set_xlabel(\"Batch Size\")\n",
    "ax1.set_ylabel(\"Estimated Memory (MB)\")\n",
    "ax1.set_title(\"Memory Usage vs Batch Size\")\n",
    "ax1.set_xscale(\"log\", base=2)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput per memory\n",
    "ax2 = axes[1]\n",
    "tp_per_mem = [tp / mem for tp, mem in zip(tp_list, mem_values)]\n",
    "ax2.bar([str(bs) for bs in batch_sizes], tp_per_mem, color=\"steelblue\")\n",
    "ax2.set_xlabel(\"Batch Size\")\n",
    "ax2.set_ylabel(\"Throughput per MB (samples/s/MB)\")\n",
    "ax2.set_title(\"Memory Efficiency\")\n",
    "ax2.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"perf-memory-profile.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'perf-memory-profile.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 7: Optimization Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate optimization report\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMIZATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best batch size\n",
    "best_bs_idx = np.argmax(tp_list)\n",
    "best_bs = batch_sizes[best_bs_idx]\n",
    "best_tp = tp_list[best_bs_idx]\n",
    "\n",
    "print(\"\\n1. BATCH SIZE OPTIMIZATION\")\n",
    "print(f\"   Optimal batch size: {best_bs}\")\n",
    "print(f\"   Peak throughput: {best_tp:,.0f} samples/s\")\n",
    "lower_bs = batch_sizes[max(0, best_bs_idx - 1)]\n",
    "upper_bs = batch_sizes[min(len(batch_sizes) - 1, best_bs_idx + 1)]\n",
    "print(f\"   Recommendation: Use batch sizes between {lower_bs} and {upper_bs}\")\n",
    "\n",
    "# Operator overhead\n",
    "baseline_time = next(r[\"avg_ms\"] for r in op_results if r[\"name\"] == \"Baseline\")\n",
    "print(\"\\n2. OPERATOR OVERHEAD\")\n",
    "print(f\"   Baseline latency: {baseline_time:.2f} ms\")\n",
    "for r in op_results:\n",
    "    if r[\"name\"] != \"Baseline\":\n",
    "        overhead = r[\"avg_ms\"] - baseline_time\n",
    "        overhead_pct = (overhead / baseline_time) * 100\n",
    "        print(f\"   {r['name']}: +{overhead:.2f} ms (+{overhead_pct:.0f}%)\")\n",
    "\n",
    "# Memory efficiency\n",
    "best_mem_eff_idx = np.argmax(tp_per_mem)\n",
    "print(\"\\n3. MEMORY EFFICIENCY\")\n",
    "print(f\"   Most efficient batch size: {batch_sizes[best_mem_eff_idx]}\")\n",
    "print(f\"   Throughput/MB: {tp_per_mem[best_mem_eff_idx]:.0f} samples/s/MB\")\n",
    "\n",
    "print(\"\\n4. GENERAL RECOMMENDATIONS\")\n",
    "print(\"   - Use JIT compilation for custom operators\")\n",
    "print(\"   - Minimize Python overhead in operator functions\")\n",
    "print(\"   - Prefer vectorized operations over loops\")\n",
    "print(\"   - Consider operator order (cheap before expensive)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "### Performance Tuning Checklist\n",
    "\n",
    "| Area | Action | Impact |\n",
    "|------|--------|--------|\n",
    "| Batch size | Find optimal via sweep | High |\n",
    "| Operators | Minimize custom logic | Medium |\n",
    "| I/O | Use streaming/prefetch | Medium |\n",
    "| JIT | Enable for operators | High |\n",
    "| Memory | Monitor and limit | Medium |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Batch size**: Optimal is typically 64-256 for most hardware\n",
    "2. **Latency**: P95 matters more than average for consistency\n",
    "3. **Memory**: Linear with batch size, monitor for OOM\n",
    "4. **Operators**: Rotation is typically most expensive\n",
    "5. **Baseline**: Always compare against no-augmentation baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Distributed**: [Sharding guide](../distributed/02_sharding_guide.ipynb)\n",
    "- **Checkpointing**: [Resumable training](../checkpointing/02_resumable_training_guide.ipynb)\n",
    "- **Full training**: [End-to-end CIFAR-10](../training/01_e2e_cifar10_guide.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the performance optimization guide.\"\"\"\n",
    "    print(\"Performance Optimization Guide\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Quick benchmark\n",
    "    np.random.seed(42)\n",
    "    data = {\n",
    "        \"image\": np.random.rand(1000, 32, 32, 3).astype(np.float32),\n",
    "        \"label\": np.random.randint(0, 10, (1000,)).astype(np.int32),\n",
    "    }\n",
    "\n",
    "    # Test different batch sizes\n",
    "    for bs in [32, 64, 128]:\n",
    "        pipeline = create_memory_pipeline(data, bs)\n",
    "        samples = 0\n",
    "        start = time.time()\n",
    "        for i, batch in enumerate(pipeline):\n",
    "            if i >= 20:\n",
    "                break\n",
    "            samples += batch[\"image\"].shape[0]\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Batch {bs}: {samples / elapsed:.0f} samples/s\")\n",
    "\n",
    "    print(\"Guide completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
