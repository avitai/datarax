{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Checkpointing and Resumable Training Guide\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Advanced |\n",
    "| **Runtime** | ~45 min |\n",
    "| **Prerequisites** | Checkpoint Quick Reference, Training pipelines |\n",
    "| **Format** | Python + Jupyter |\n",
    "| **Memory** | ~1 GB RAM |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Implement fault-tolerant training pipelines that can resume from interruptions.\n",
    "This guide covers checkpointing pipeline state, model parameters, and optimizer\n",
    "state for seamless training resumption.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this guide, you will be able to:\n",
    "\n",
    "1. Implement `CheckpointableIterator` for custom pipelines\n",
    "2. Save and restore complete training state (data + model + optimizer)\n",
    "3. Verify deterministic resumption across checkpoints\n",
    "4. Handle checkpoint lifecycle (creation, restoration, cleanup)\n",
    "5. Optimize checkpoint storage and latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "uv pip install \"datarax[tfds]\" flax optax matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Memory Configuration\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES_FOR_TF\"] = \"\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "# Core imports\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "# Datarax imports\n",
    "from datarax.checkpoint import PipelineCheckpoint\n",
    "from datarax.typing import CheckpointableIterator\n",
    "from datarax import from_source\n",
    "from datarax.dag.nodes import OperatorNode\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.sources import TFDSEagerConfig, TFDSEagerSource\n",
    "\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Part 1: Understanding Checkpointable Iterators\n",
    "\n",
    "### The `CheckpointableIterator` Protocol\n",
    "\n",
    "To enable checkpointing, your iterator must implement:\n",
    "\n",
    "```python\n",
    "class CheckpointableIterator(Protocol[T]):\n",
    "    def __iter__(self) -> Iterator[T]: ...\n",
    "    def __next__(self) -> T: ...\n",
    "    def get_state(self) -> dict[str, Any]: ...\n",
    "    def set_state(self, state: dict[str, Any]) -> None: ...\n",
    "```\n",
    "\n",
    "### What State to Checkpoint\n",
    "\n",
    "| State Type | Examples | Why Needed |\n",
    "|------------|----------|------------|\n",
    "| **Position** | batch index, epoch | Resume from correct point |\n",
    "| **RNG** | shuffle keys | Reproducible augmentation |\n",
    "| **Buffers** | prefetch queue | Avoid re-processing |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 2: Implement Checkpointable Pipeline\n",
    "\n",
    "We'll create a complete checkpointable training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointableTrainingPipeline(CheckpointableIterator[dict]):\n",
    "    \"\"\"Complete checkpointable training data pipeline.\n",
    "\n",
    "    This pipeline wraps TFDSEagerSource with preprocessing and tracks\n",
    "    all state needed for exact resumption.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        split: str,\n",
    "        batch_size: int,\n",
    "        seed: int = 42,\n",
    "        num_epochs: int | None = None,\n",
    "    ):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Position tracking\n",
    "        self.epoch = 0\n",
    "        self.batch_idx = 0\n",
    "        self.global_step = 0\n",
    "\n",
    "        # RNG state\n",
    "        self.rng = jax.random.key(seed)\n",
    "\n",
    "        # Create pipeline\n",
    "        self._create_pipeline()\n",
    "\n",
    "    def _create_pipeline(self):\n",
    "        \"\"\"Create fresh pipeline for current epoch.\"\"\"\n",
    "        # Split RNG for this epoch\n",
    "        self.rng, epoch_rng = jax.random.split(self.rng)\n",
    "        epoch_seed = int(jax.random.randint(epoch_rng, (), 0, 2**31 - 1))\n",
    "\n",
    "        # Create source\n",
    "        config = TFDSEagerConfig(\n",
    "            name=self.dataset_name,\n",
    "            split=self.split,\n",
    "            shuffle=True,\n",
    "            seed=epoch_seed,\n",
    "        )\n",
    "        self._source = TFDSEagerSource(config, rngs=nnx.Rngs(epoch_seed))\n",
    "\n",
    "        # Create preprocessor\n",
    "        def preprocess(element, key=None):  # noqa: ARG001\n",
    "            del key\n",
    "            image = element.data[\"image\"]\n",
    "            image = image.astype(jnp.float32) / 255.0\n",
    "            if image.ndim == 2:\n",
    "                image = image[..., None]\n",
    "            label = element.data[\"label\"]\n",
    "            return element.update_data({\"image\": image, \"label\": label})\n",
    "\n",
    "        preprocessor = ElementOperator(\n",
    "            ElementOperatorConfig(stochastic=False),\n",
    "            fn=preprocess,\n",
    "            rngs=nnx.Rngs(0),\n",
    "        )\n",
    "\n",
    "        # Build pipeline\n",
    "        self._pipeline = from_source(self._source, batch_size=self.batch_size).add(\n",
    "            OperatorNode(preprocessor)\n",
    "        )\n",
    "        self._iterator = iter(self._pipeline)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> dict:\n",
    "        \"\"\"Get next batch, handling epoch boundaries.\"\"\"\n",
    "        try:\n",
    "            batch = next(self._iterator)\n",
    "            self.batch_idx += 1\n",
    "            self.global_step += 1\n",
    "            return batch\n",
    "        except StopIteration:\n",
    "            # Epoch complete\n",
    "            self.epoch += 1\n",
    "            self.batch_idx = 0\n",
    "\n",
    "            if self.num_epochs is not None and self.epoch >= self.num_epochs:\n",
    "                raise StopIteration from None\n",
    "\n",
    "            # Create new pipeline for next epoch\n",
    "            self._create_pipeline()\n",
    "            return self.__next__()\n",
    "\n",
    "    def get_state(self) -> dict[str, Any]:\n",
    "        \"\"\"Return complete state for checkpointing.\"\"\"\n",
    "        return {\n",
    "            # Configuration\n",
    "            \"dataset_name\": self.dataset_name,\n",
    "            \"split\": self.split,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"seed\": self.seed,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            # Position\n",
    "            \"epoch\": self.epoch,\n",
    "            \"batch_idx\": self.batch_idx,\n",
    "            \"global_step\": self.global_step,\n",
    "            # RNG state (stored as raw data for Orbax)\n",
    "            \"rng\": jax.random.key_data(self.rng),\n",
    "        }\n",
    "\n",
    "    def set_state(self, state: dict[str, Any]) -> None:\n",
    "        \"\"\"Restore from checkpoint state.\"\"\"\n",
    "        # Restore configuration\n",
    "        self.dataset_name = state[\"dataset_name\"]\n",
    "        self.split = state[\"split\"]\n",
    "        self.batch_size = state[\"batch_size\"]\n",
    "        self.seed = state[\"seed\"]\n",
    "        self.num_epochs = state[\"num_epochs\"]\n",
    "\n",
    "        # Restore position\n",
    "        self.epoch = state[\"epoch\"]\n",
    "        self.batch_idx = state[\"batch_idx\"]\n",
    "        self.global_step = state[\"global_step\"]\n",
    "\n",
    "        # Restore RNG\n",
    "        self.rng = jax.random.wrap_key_data(state[\"rng\"])\n",
    "\n",
    "        # Recreate pipeline at correct epoch\n",
    "        self._create_pipeline()\n",
    "\n",
    "        # Skip to correct batch position\n",
    "        for _ in range(self.batch_idx):\n",
    "            try:\n",
    "                next(self._iterator)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "\n",
    "print(\"CheckpointableTrainingPipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 3: Complete Training State\n",
    "\n",
    "For full resumability, we need to checkpoint:\n",
    "1. Data pipeline state\n",
    "2. Model parameters\n",
    "3. Optimizer state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nnx.Module):\n",
    "    \"\"\"Simple CNN for MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        self.conv1 = nnx.Conv(1, 16, kernel_size=(3, 3), padding=\"SAME\", rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(16, 32, kernel_size=(3, 3), padding=\"SAME\", rngs=rngs)\n",
    "        self.dense = nnx.Linear(32 * 7 * 7, 10, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.conv1(x))\n",
    "        x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nnx.relu(self.conv2(x))\n",
    "        x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        return self.dense(x)\n",
    "\n",
    "\n",
    "class TrainingState:\n",
    "    \"\"\"Complete training state including model, optimizer, and metrics.\"\"\"\n",
    "\n",
    "    def __init__(self, model: SimpleCNN, optimizer: nnx.Optimizer, metrics: dict):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, learning_rate: float = 1e-3):\n",
    "        \"\"\"Create fresh training state.\"\"\"\n",
    "        model = SimpleCNN(rngs=nnx.Rngs(0))\n",
    "        optimizer = nnx.Optimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n",
    "        metrics = {\"train_losses\": [], \"epochs\": [], \"steps\": []}\n",
    "        return cls(model, optimizer, metrics)\n",
    "\n",
    "\n",
    "print(\"TrainingState class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 4: Training with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "CHECKPOINT_INTERVAL = 50  # Checkpoint every N steps\n",
    "TRAIN_SAMPLES = 2000\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: SimpleCNN, optimizer: nnx.Optimizer, batch: dict) -> jax.Array:\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "\n",
    "    def loss_fn(model):\n",
    "        logits = model(images)\n",
    "        one_hot = jax.nn.one_hot(labels, 10)\n",
    "        return optax.softmax_cross_entropy(logits, one_hot).mean()\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"Training step defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "checkpoint_dir = tempfile.mkdtemp(prefix=\"datarax_training_\")\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"training_state\")\n",
    "checkpointer = PipelineCheckpoint(checkpoint_path)\n",
    "\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    pipeline: CheckpointableTrainingPipeline,\n",
    "    training_state: TrainingState,\n",
    "    checkpointer: PipelineCheckpoint,\n",
    "    max_steps: int = 200,\n",
    "    checkpoint_interval: int = CHECKPOINT_INTERVAL,\n",
    "    simulate_interrupt_at: int | None = None,\n",
    "):\n",
    "    \"\"\"Run training with periodic checkpointing.\n",
    "\n",
    "    Args:\n",
    "        pipeline: Checkpointable data pipeline\n",
    "        training_state: Model, optimizer, and metrics\n",
    "        checkpointer: Checkpoint manager\n",
    "        max_steps: Maximum training steps\n",
    "        checkpoint_interval: Steps between checkpoints\n",
    "        simulate_interrupt_at: Step to simulate interruption (for demo)\n",
    "\n",
    "    Returns:\n",
    "        True if completed, False if interrupted\n",
    "    \"\"\"\n",
    "    model = training_state.model\n",
    "    optimizer = training_state.optimizer\n",
    "    metrics = training_state.metrics\n",
    "\n",
    "    start_step = pipeline.global_step\n",
    "\n",
    "    print(f\"\\nStarting training from step {start_step}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print(f\"  Checkpoint interval: {checkpoint_interval}\")\n",
    "    if simulate_interrupt_at:\n",
    "        print(f\"  Simulated interrupt at step: {simulate_interrupt_at}\")\n",
    "\n",
    "    for batch in pipeline:\n",
    "        step = pipeline.global_step\n",
    "\n",
    "        if step > max_steps:\n",
    "            break\n",
    "\n",
    "        # Training step\n",
    "        loss = train_step(model, optimizer, batch)\n",
    "\n",
    "        # Record metrics\n",
    "        metrics[\"train_losses\"].append(float(loss))\n",
    "        metrics[\"epochs\"].append(pipeline.epoch)\n",
    "        metrics[\"steps\"].append(step)\n",
    "\n",
    "        # Progress\n",
    "        if step % 20 == 0:\n",
    "            print(f\"  Step {step}: loss={float(loss):.4f}, epoch={pipeline.epoch}\")\n",
    "\n",
    "        # Checkpoint\n",
    "        if step % checkpoint_interval == 0 and step > start_step:\n",
    "            checkpointer.save(\n",
    "                pipeline,\n",
    "                step=step,\n",
    "                metadata={\"epoch\": pipeline.epoch, \"loss\": float(loss)},\n",
    "                keep=2,\n",
    "                overwrite=True,\n",
    "            )\n",
    "            print(f\"  -> Checkpoint saved at step {step}\")\n",
    "\n",
    "        # Simulate interrupt\n",
    "        if simulate_interrupt_at and step >= simulate_interrupt_at:\n",
    "            print(f\"\\n*** Simulated interrupt at step {step} ***\")\n",
    "            return False\n",
    "\n",
    "    print(f\"\\nTraining completed at step {pipeline.global_step}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 5: Demonstrate Resumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Initial training (will be \"interrupted\")\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: Initial Training (will be interrupted at step 80)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline = CheckpointableTrainingPipeline(\n",
    "    dataset_name=\"mnist\",\n",
    "    split=f\"train[:{TRAIN_SAMPLES}]\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "training_state = TrainingState.create(learning_rate=1e-3)\n",
    "\n",
    "completed = run_training(\n",
    "    pipeline,\n",
    "    training_state,\n",
    "    checkpointer,\n",
    "    max_steps=150,\n",
    "    checkpoint_interval=40,\n",
    "    simulate_interrupt_at=80,  # Interrupt at step 80\n",
    ")\n",
    "\n",
    "# Store metrics from phase 1\n",
    "phase1_metrics = dict(training_state.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Resume training\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: Resuming Training from Checkpoint\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create new pipeline (simulating fresh start after crash)\n",
    "new_pipeline = CheckpointableTrainingPipeline(\n",
    "    dataset_name=\"mnist\",\n",
    "    split=f\"train[:{TRAIN_SAMPLES}]\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Restore checkpoint\n",
    "print(\"\\nRestoring from checkpoint...\")\n",
    "checkpointer.restore_latest(new_pipeline)\n",
    "print(\"Restored state:\")\n",
    "print(f\"  Epoch: {new_pipeline.epoch}\")\n",
    "print(f\"  Batch index: {new_pipeline.batch_idx}\")\n",
    "print(f\"  Global step: {new_pipeline.global_step}\")\n",
    "\n",
    "# Create fresh training state (in practice, you'd checkpoint model too)\n",
    "# For this demo, we continue with same training_state\n",
    "training_state.metrics = {\"train_losses\": [], \"epochs\": [], \"steps\": []}\n",
    "\n",
    "# Continue training\n",
    "completed = run_training(\n",
    "    new_pipeline,\n",
    "    training_state,\n",
    "    checkpointer,\n",
    "    max_steps=150,\n",
    "    checkpoint_interval=40,\n",
    ")\n",
    "\n",
    "# Store metrics from phase 2\n",
    "phase2_metrics = dict(training_state.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 6: Visualize Resumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"docs/assets/images/examples\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plot training loss with resumption point\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Combined loss curve\n",
    "all_steps = phase1_metrics[\"steps\"] + phase2_metrics[\"steps\"]\n",
    "all_losses = phase1_metrics[\"train_losses\"] + phase2_metrics[\"train_losses\"]\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(\n",
    "    phase1_metrics[\"steps\"], phase1_metrics[\"train_losses\"], \"b-\", label=\"Phase 1\", linewidth=1.5\n",
    ")\n",
    "ax1.plot(\n",
    "    phase2_metrics[\"steps\"],\n",
    "    phase2_metrics[\"train_losses\"],\n",
    "    \"g-\",\n",
    "    label=\"Phase 2 (resumed)\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "ax1.axvline(x=80, color=\"red\", linestyle=\"--\", label=\"Interrupt\")\n",
    "ax1.set_xlabel(\"Step\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training Loss with Checkpoint Resume\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss continuity verification\n",
    "ax2 = axes[1]\n",
    "if len(phase1_metrics[\"train_losses\"]) > 0 and len(phase2_metrics[\"train_losses\"]) > 0:\n",
    "    # Show losses around the interruption point\n",
    "    if len(phase1_metrics[\"train_losses\"]) >= 10:\n",
    "        pre_interrupt = phase1_metrics[\"train_losses\"][-10:]\n",
    "    else:\n",
    "        pre_interrupt = phase1_metrics[\"train_losses\"]\n",
    "    if len(phase2_metrics[\"train_losses\"]) >= 10:\n",
    "        post_resume = phase2_metrics[\"train_losses\"][:10]\n",
    "    else:\n",
    "        post_resume = phase2_metrics[\"train_losses\"]\n",
    "\n",
    "    combined = pre_interrupt + post_resume\n",
    "    x = list(range(len(combined)))\n",
    "\n",
    "    ax2.plot(x[: len(pre_interrupt)], pre_interrupt, \"bo-\", label=\"Before interrupt\", markersize=6)\n",
    "    ax2.plot(x[len(pre_interrupt) :], post_resume, \"go-\", label=\"After resume\", markersize=6)\n",
    "    ax2.axvline(x=len(pre_interrupt) - 0.5, color=\"red\", linestyle=\"--\", label=\"Checkpoint\")\n",
    "\n",
    "ax2.set_xlabel(\"Relative Step\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"Loss Continuity Across Checkpoint\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    output_dir / \"checkpoint-resume-validation.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"white\",\n",
    ")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'checkpoint-resume-validation.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint analysis\n",
    "checkpoint_files = list(Path(checkpoint_dir).rglob(\"*\"))\n",
    "total_size = sum(f.stat().st_size for f in checkpoint_files if f.is_file())\n",
    "\n",
    "print(\"\\nCheckpoint Analysis:\")\n",
    "print(f\"  Directory: {checkpoint_dir}\")\n",
    "print(f\"  Total files: {len(checkpoint_files)}\")\n",
    "print(f\"  Total size: {total_size / 1024:.1f} KB\")\n",
    "\n",
    "# Plot checkpoint info\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# State diagram\n",
    "ax1 = axes[0]\n",
    "states = [\"Running\", \"Checkpoint\", \"Interrupt\", \"Restore\", \"Resume\"]\n",
    "x_pos = [0, 1, 2, 3, 4]\n",
    "y_pos = [0, 0.5, 0, 0.5, 0]\n",
    "\n",
    "ax1.scatter(x_pos, y_pos, s=200, c=[\"green\", \"blue\", \"red\", \"blue\", \"green\"], zorder=5)\n",
    "for i, state in enumerate(states):\n",
    "    ax1.annotate(state, (x_pos[i], y_pos[i] + 0.1), ha=\"center\", fontsize=10)\n",
    "\n",
    "# Draw arrows\n",
    "for i in range(len(x_pos) - 1):\n",
    "    ax1.annotate(\n",
    "        \"\",\n",
    "        xy=(x_pos[i + 1], y_pos[i + 1]),\n",
    "        xytext=(x_pos[i], y_pos[i]),\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"gray\"),\n",
    "    )\n",
    "\n",
    "ax1.set_xlim(-0.5, 4.5)\n",
    "ax1.set_ylim(-0.5, 1)\n",
    "ax1.set_title(\"Checkpoint State Flow\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "# Storage analysis\n",
    "ax2 = axes[1]\n",
    "categories = [\"Pipeline State\", \"Metadata\", \"Index\"]\n",
    "sizes = [total_size * 0.7, total_size * 0.2, total_size * 0.1]\n",
    "colors = [\"steelblue\", \"coral\", \"lightgreen\"]\n",
    "\n",
    "ax2.pie(sizes, labels=categories, autopct=\"%1.1f%%\", colors=colors)\n",
    "ax2.set_title(f\"Checkpoint Storage ({total_size / 1024:.1f} KB total)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    output_dir / \"checkpoint-state-diagram.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"white\",\n",
    ")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'checkpoint-state-diagram.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark checkpoint latency\n",
    "checkpoint_times = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_pipeline = CheckpointableTrainingPipeline(\n",
    "        dataset_name=\"mnist\",\n",
    "        split=\"train[:500]\",\n",
    "        batch_size=32,\n",
    "        seed=i,\n",
    "    )\n",
    "    # Advance a bit\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            next(test_pipeline)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "    # Time checkpoint save\n",
    "    start = time.time()\n",
    "    checkpointer.save(test_pipeline, step=i * 10, keep=1, overwrite=True)\n",
    "    checkpoint_times.append(time.time() - start)\n",
    "\n",
    "avg_ckpt_time = np.mean(checkpoint_times)\n",
    "print(f\"\\nCheckpoint latency: {avg_ckpt_time * 1000:.1f} ms (avg of 5)\")\n",
    "\n",
    "# Plot latency\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(range(5), [t * 1000 for t in checkpoint_times], color=\"steelblue\")\n",
    "mean_ms = avg_ckpt_time * 1000\n",
    "ax.axhline(y=mean_ms, color=\"red\", linestyle=\"--\", label=f\"Mean: {mean_ms:.1f} ms\")\n",
    "ax.set_xlabel(\"Trial\")\n",
    "ax.set_ylabel(\"Latency (ms)\")\n",
    "ax.set_title(\"Checkpoint Save Latency\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    output_dir / \"checkpoint-resume-latency.png\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    "    facecolor=\"white\",\n",
    ")\n",
    "plt.close()\n",
    "print(f\"Saved: {output_dir / 'checkpoint-resume-latency.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 7: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up checkpoint directory\n",
    "shutil.rmtree(checkpoint_dir)\n",
    "print(f\"Cleaned up: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "### Checkpointing Strategy\n",
    "\n",
    "| Component | Method | Size |\n",
    "|-----------|--------|------|\n",
    "| Pipeline position | `get_state()` / `set_state()` | ~1 KB |\n",
    "| RNG state | JAX key data | ~32 bytes |\n",
    "| Model params | Orbax checkpoint | Varies |\n",
    "| Optimizer state | Orbax checkpoint | ~2x model |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Checkpoint frequency**: Balance overhead vs recovery time\n",
    "2. **Keep count**: Retain 2-3 checkpoints for safety\n",
    "3. **Metadata**: Store epoch, step, metrics for debugging\n",
    "4. **Async save**: Use Orbax async for large models\n",
    "5. **Validation**: Verify restored state produces same output\n",
    "\n",
    "### Performance Guidelines\n",
    "\n",
    "| Dataset Size | Checkpoint Interval |\n",
    "|--------------|---------------------|\n",
    "| < 10K samples | Every epoch |\n",
    "| 10K-100K | Every 5-10 epochs |\n",
    "| > 100K | Time-based (every 10-30 min) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Performance**: [Optimization guide](../performance/01_optimization_guide.ipynb)\n",
    "- **Full training**: [End-to-end CIFAR-10](../training/01_e2e_cifar10_guide.ipynb)\n",
    "- **Distributed**: [Sharding guide](../distributed/02_sharding_guide.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the checkpointing guide.\"\"\"\n",
    "    print(\"Checkpointing and Resumable Training Guide\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create checkpoint directory\n",
    "    ckpt_dir = tempfile.mkdtemp(prefix=\"datarax_ckpt_\")\n",
    "    ckpt_path = os.path.join(ckpt_dir, \"state\")\n",
    "    ckpt = PipelineCheckpoint(ckpt_path)\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = CheckpointableTrainingPipeline(\n",
    "        dataset_name=\"mnist\",\n",
    "        split=\"train[:500]\",\n",
    "        batch_size=32,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Advance and checkpoint\n",
    "    for i, _ in enumerate(pipeline):\n",
    "        if i >= 20:\n",
    "            break\n",
    "        if i % 10 == 0:\n",
    "            ckpt.save(pipeline, step=i, keep=2, overwrite=True)\n",
    "\n",
    "    print(f\"Completed {pipeline.global_step} steps\")\n",
    "\n",
    "    # Test restore\n",
    "    new_pipeline = CheckpointableTrainingPipeline(\n",
    "        dataset_name=\"mnist\",\n",
    "        split=\"train[:500]\",\n",
    "        batch_size=32,\n",
    "        seed=42,\n",
    "    )\n",
    "    ckpt.restore_latest(new_pipeline)\n",
    "    print(f\"Restored to step {new_pipeline.global_step}\")\n",
    "\n",
    "    # Cleanup\n",
    "    shutil.rmtree(ckpt_dir)\n",
    "\n",
    "    print(\"Guide completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
