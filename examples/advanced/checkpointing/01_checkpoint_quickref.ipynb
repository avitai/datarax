{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Pipeline Checkpointing Quick Reference\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Intermediate |\n",
    "| **Runtime** | ~10 min |\n",
    "| **Prerequisites** | Basic Datarax pipeline, JAX fundamentals |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Save and restore data pipeline state to enable resumable processing.\n",
    "This is essential for long-running data jobs that may be interrupted\n",
    "and need to continue from where they left off.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this example, you will be able to:\n",
    "\n",
    "1. Create a `CheckpointableIterator` with proper state management\n",
    "2. Use `PipelineCheckpoint` to save/restore state\n",
    "3. Implement resumable data processing loops\n",
    "4. Handle interrupted jobs gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "# Install datarax\n",
    "uv pip install datarax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from typing import Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from datarax.checkpoint import PipelineCheckpoint\n",
    "from datarax.typing import CheckpointableIterator\n",
    "\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Create Checkpointable Iterator\n",
    "\n",
    "A `CheckpointableIterator` must implement:\n",
    "\n",
    "- `get_state()` - Return current iteration state\n",
    "- `set_state(state)` - Restore from saved state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePipeline(CheckpointableIterator[dict[str, jax.Array]]):\n",
    "    \"\"\"Data stream with checkpointing support.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: jax.Array,\n",
    "        batch_size: int = 10,\n",
    "        shuffle: bool = True,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.rng = jax.random.key(seed)\n",
    "        self.epoch = 0\n",
    "        self.position = 0\n",
    "        self.indices = self._create_indices()\n",
    "\n",
    "    def _create_indices(self) -> jax.Array:\n",
    "        \"\"\"Create iteration indices (shuffled or sequential).\"\"\"\n",
    "        indices = jnp.arange(len(self.data))\n",
    "        if self.shuffle:\n",
    "            self.rng, key = jax.random.split(self.rng)\n",
    "            indices = jax.random.permutation(key, indices)\n",
    "        return indices\n",
    "\n",
    "    def __iter__(self) -> \"SimplePipeline\":\n",
    "        return self\n",
    "\n",
    "    def iterator(self, pipeline_seed: int | None = None) -> \"SimplePipeline\":\n",
    "        \"\"\"Create iterator with optional new seed.\"\"\"\n",
    "        if pipeline_seed is not None:\n",
    "            self.rng = jax.random.key(pipeline_seed)\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> dict[str, jax.Array]:\n",
    "        \"\"\"Get next batch.\"\"\"\n",
    "        if self.position >= len(self.data):\n",
    "            self.epoch += 1\n",
    "            self.position = 0\n",
    "            self.indices = self._create_indices()\n",
    "            raise StopIteration\n",
    "\n",
    "        start = self.position\n",
    "        end = min(start + self.batch_size, len(self.data))\n",
    "        batch_indices = self.indices[start:end]\n",
    "        self.position = end\n",
    "\n",
    "        return {\"x\": self.data[batch_indices]}\n",
    "\n",
    "    def get_state(self) -> dict[str, Any]:\n",
    "        \"\"\"Return checkpoint state.\"\"\"\n",
    "        return {\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"shuffle\": self.shuffle,\n",
    "            \"seed\": self.seed,\n",
    "            \"rng\": jax.random.key_data(self.rng),  # Convert key to raw data\n",
    "            \"epoch\": self.epoch,\n",
    "            \"position\": self.position,\n",
    "            \"indices\": self.indices,\n",
    "        }\n",
    "\n",
    "    def set_state(self, state: dict[str, Any]) -> None:\n",
    "        \"\"\"Restore from checkpoint state.\"\"\"\n",
    "        self.batch_size = state[\"batch_size\"]\n",
    "        self.shuffle = state[\"shuffle\"]\n",
    "        self.seed = state[\"seed\"]\n",
    "        self.rng = jax.random.wrap_key_data(state[\"rng\"])  # Convert back to key\n",
    "        self.epoch = state[\"epoch\"]\n",
    "        self.position = state[\"position\"]\n",
    "        self.indices = state[\"indices\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return (len(self.data) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "# Create pipeline\n",
    "data = jnp.arange(50).reshape(50, 1).astype(jnp.float32)\n",
    "pipeline = SimplePipeline(data, batch_size=10, shuffle=True)\n",
    "print(f\"Pipeline: {len(pipeline)} batches, {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Set Up Checkpointing\n",
    "\n",
    "`PipelineCheckpoint` manages checkpoint files using Orbax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "checkpoint_dir = tempfile.mkdtemp(prefix=\"datarax_ckpt_\")\n",
    "checkpointer = PipelineCheckpoint(os.path.join(checkpoint_dir, \"pipeline_state\"))\n",
    "\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Process with Periodic Checkpoints\n",
    "\n",
    "Save checkpoints at regular intervals to enable resumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data with checkpointing\n",
    "step = 0\n",
    "for epoch in range(2):\n",
    "    print(f\"\\nEpoch {epoch}:\")\n",
    "    pipeline_iter = pipeline.iterator()\n",
    "\n",
    "    for batch_idx, batch in enumerate(pipeline_iter):\n",
    "        batch_mean = jnp.mean(batch[\"x\"]).item()\n",
    "        step += 1\n",
    "\n",
    "        print(f\"  Batch {batch_idx}: mean={batch_mean:.2f}\")\n",
    "\n",
    "        # Save checkpoint every 3 steps\n",
    "        if step % 3 == 0:\n",
    "            save_path = checkpointer.save(\n",
    "                pipeline,\n",
    "                step=step,\n",
    "                metadata={\"epoch\": epoch, \"batch\": batch_idx},\n",
    "                keep=2,  # Keep last 2 checkpoints\n",
    "                overwrite=True,\n",
    "            )\n",
    "            print(f\"  -> Saved checkpoint at step {step}\")\n",
    "\n",
    "print(f\"\\nProcessed {step} total steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Restore from Checkpoint\n",
    "\n",
    "Demonstrate resuming from a saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new pipeline (simulating restart)\n",
    "new_pipeline = SimplePipeline(data, batch_size=10, shuffle=True)\n",
    "print(f\"New pipeline state: epoch={new_pipeline.epoch}, position={new_pipeline.position}\")\n",
    "\n",
    "# Restore from checkpoint\n",
    "checkpointer.restore_latest(new_pipeline)\n",
    "print(f\"Restored state: epoch={new_pipeline.epoch}, position={new_pipeline.position}\")\n",
    "\n",
    "# Continue processing\n",
    "print(\"\\nContinuing from checkpoint:\")\n",
    "for batch_idx, batch in enumerate(new_pipeline):\n",
    "    batch_mean = jnp.mean(batch[\"x\"]).item()\n",
    "    print(f\"  Batch {batch_idx}: mean={batch_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 5: Cleanup\n",
    "\n",
    "Remove checkpoint files when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up checkpoint directory\n",
    "shutil.rmtree(checkpoint_dir)\n",
    "print(f\"Cleaned up: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| State Saved | RNG, position, epoch, indices |\n",
    "| Checkpoint Format | Orbax (efficient, async-capable) |\n",
    "| Retention | Configurable via `keep` parameter |\n",
    "| Metadata | Custom fields (epoch, batch, etc.) |\n",
    "\n",
    "Key benefits:\n",
    "\n",
    "- **Fault tolerance**: Resume interrupted jobs\n",
    "- **Incremental processing**: Process data in stages\n",
    "- **Reproducibility**: Exact state restoration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- **CLI interface**: Integrate checkpointing with command-line tools\n",
    "- **Distributed checkpoints**: Coordinate checkpoints across workers\n",
    "- **Custom handlers**: Implement handlers for special data types\n",
    "- **Pipeline monitoring**: [Monitoring](../monitoring/01_monitoring_quickref.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the checkpoint example.\"\"\"\n",
    "    print(\"Pipeline Checkpointing Example\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Setup\n",
    "    data = jnp.arange(50).reshape(50, 1).astype(jnp.float32)\n",
    "    pipeline = SimplePipeline(data, batch_size=10, shuffle=True)\n",
    "\n",
    "    checkpoint_dir = tempfile.mkdtemp(prefix=\"datarax_ckpt_\")\n",
    "    checkpointer = PipelineCheckpoint(os.path.join(checkpoint_dir, \"pipeline_state\"))\n",
    "\n",
    "    # Process with checkpoints\n",
    "    step = 0\n",
    "    for epoch in range(2):\n",
    "        for batch in pipeline.iterator():\n",
    "            step += 1\n",
    "            if step % 5 == 0:\n",
    "                checkpointer.save(pipeline, step=step, keep=2, overwrite=True)\n",
    "\n",
    "    # Test restoration\n",
    "    new_pipeline = SimplePipeline(data, batch_size=10, shuffle=True)\n",
    "    checkpointer.restore_latest(new_pipeline)\n",
    "\n",
    "    # Cleanup\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "    print(f\"Processed {step} steps with checkpointing\")\n",
    "    print(\"Example completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
