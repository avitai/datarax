{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test script for HuggingFace datasets with Datarax.\n",
    "\n",
    "This script tests various HuggingFace datasets with Datarax to ensure compatibility\n",
    "with different dataset types, structures, and sizes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ddcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax import nnx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cd3d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datarax.core import Pipeline\n",
    "from datarax.sources import HfDataSourceConfig, HFSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620415c4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"hf_datasets_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ada9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configurations to test\n",
    "DATASET_CONFIGS = [\n",
    "    # Text datasets\n",
    "    {\"path\": \"imdb\", \"split\": \"train\", \"streaming\": True},\n",
    "    {\"path\": \"squad\", \"split\": \"train\", \"streaming\": True},\n",
    "    # Image datasets\n",
    "    {\"path\": \"cifar10\", \"split\": \"train\", \"streaming\": True},\n",
    "    {\"path\": \"mnist\", \"split\": \"train\", \"streaming\": True},\n",
    "    # Other datasets\n",
    "    {\"path\": \"ag_news\", \"split\": \"train\", \"streaming\": True},\n",
    "    {\"path\": \"rotten_tomatoes\", \"split\": \"train\", \"streaming\": True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ef53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_compatibility_test(\n",
    "    path: str,\n",
    "    split: str,\n",
    "    streaming: bool = True,\n",
    "    name: str | None = None,\n",
    "    data_dir: str | None = None,\n",
    "    max_examples: int = 10,\n",
    "    analyze_structure: bool = True,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Test a specific HuggingFace dataset's compatibility with Datarax.\n",
    "\n",
    "    Args:\n",
    "        path: Path/name of the dataset\n",
    "        split: Split to test\n",
    "        streaming: Whether to use streaming mode\n",
    "        name: Optional dataset configuration name (for datasets with configs)\n",
    "        data_dir: Optional directory for cached datasets\n",
    "        max_examples: Maximum number of examples to process\n",
    "        analyze_structure: Whether to analyze dataset structure\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Testing dataset: {path} (name: {name}, split: {split})\")\n",
    "\n",
    "    try:\n",
    "        # Create data source config\n",
    "        config = HfDataSourceConfig(\n",
    "            name=path,\n",
    "            split=split,\n",
    "            streaming=streaming,\n",
    "            data_dir=data_dir,\n",
    "        )\n",
    "        source = HFSource(config, rngs=nnx.Rngs(0))\n",
    "\n",
    "        # Create data stream\n",
    "        stream = Pipeline(source)\n",
    "\n",
    "        # Check dataset size if possible\n",
    "        try:\n",
    "            dataset_size = len(source)\n",
    "            size_info = f\"Dataset size: {dataset_size}\"\n",
    "        except NotImplementedError:\n",
    "            size_info = \"Dataset size not available (streaming mode)\"\n",
    "\n",
    "        logger.info(size_info)\n",
    "\n",
    "        # Process examples\n",
    "        examples_processed = 0\n",
    "        data_types = {}\n",
    "        shapes = {}\n",
    "\n",
    "        example_iter = iter(stream.batch(batch_size=1).iterator())\n",
    "        for _ in tqdm(range(max_examples), desc=\"Processing examples\"):\n",
    "            element = next(example_iter)\n",
    "            examples_processed += 1\n",
    "\n",
    "            # Analyze structure if needed\n",
    "            if analyze_structure and examples_processed == 1:\n",
    "                # Log keys\n",
    "                logger.info(f\"Keys: {list(element.keys())}\")\n",
    "\n",
    "                # Analyze data types and shapes\n",
    "                for key, value in element.items():\n",
    "                    if hasattr(value, \"dtype\"):\n",
    "                        data_types[key] = str(value.dtype)\n",
    "                        shapes[key] = str(value.shape) if hasattr(value, \"shape\") else \"scalar\"\n",
    "                    else:\n",
    "                        data_types[key] = type(value).__name__\n",
    "                        shapes[key] = \"N/A\"\n",
    "\n",
    "        # Test complete\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(\n",
    "            f\"Test completed successfully. Processed {examples_processed} examples in \"\n",
    "            f\"{elapsed_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"dataset\": path,\n",
    "            \"name\": name,\n",
    "            \"split\": split,\n",
    "            \"examples_processed\": examples_processed,\n",
    "            \"data_types\": data_types,\n",
    "            \"shapes\": shapes,\n",
    "            \"elapsed_time\": elapsed_time,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        logger.error(f\"Test failed: {str(e)}\", exc_info=True)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"failure\",\n",
    "            \"dataset\": path,\n",
    "            \"name\": name,\n",
    "            \"split\": split,\n",
    "            \"error\": str(e),\n",
    "            \"elapsed_time\": time.time() - start_time,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb486d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset_tests():\n",
    "    \"\"\"Run tests on all configured datasets.\"\"\"\n",
    "    logger.info(f\"JAX devices: {jax.devices()}\")\n",
    "    logger.info(f\"Testing {len(DATASET_CONFIGS)} dataset configurations\")\n",
    "\n",
    "    results = []\n",
    "    for dataset_config in DATASET_CONFIGS:\n",
    "        result = dataset_compatibility_test(\n",
    "            path=dataset_config[\"path\"],\n",
    "            split=dataset_config[\"split\"],\n",
    "            streaming=dataset_config.get(\"streaming\", True),\n",
    "            name=dataset_config.get(\"name\"),\n",
    "            data_dir=\"./data/hf_cache\",\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    # Summarize results\n",
    "    logger.info(\"=== Test Results Summary ===\")\n",
    "    success_count = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "    logger.info(f\"Successful tests: {success_count}/{len(results)}\")\n",
    "\n",
    "    logger.info(\"Dataset compatibility details:\")\n",
    "    for result in results:\n",
    "        status = \"✅\" if result[\"status\"] == \"success\" else \"❌\"\n",
    "        dataset_name = f\"{result['dataset']}\"\n",
    "        if result.get(\"name\"):\n",
    "            dataset_name += f\"/{result['name']}\"\n",
    "\n",
    "        logger.info(f\"{status} {dataset_name} ({result['split']})\")\n",
    "        if result[\"status\"] == \"failure\":\n",
    "            logger.info(f\"  Error: {result['error']}\")\n",
    "        else:\n",
    "            logger.info(\n",
    "                f\"  Processed {result['examples_processed']} examples in \"\n",
    "                f\"{result['elapsed_time']:.2f}s\"\n",
    "            )\n",
    "            if result.get(\"data_types\"):\n",
    "                for key, dtype in result[\"data_types\"].items():\n",
    "                    shape = result[\"shapes\"].get(key, \"N/A\")\n",
    "                    logger.info(f\"  - {key}: {dtype} (shape: {shape})\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_dataset_tests()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
