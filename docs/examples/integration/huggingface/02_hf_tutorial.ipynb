{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71e8122",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# HuggingFace Datasets Tutorial\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Intermediate |\n",
    "| **Runtime** | ~30 min |\n",
    "| **Prerequisites** | HuggingFace Quick Reference, Pipeline Tutorial |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial provides a comprehensive guide to using HuggingFace Datasets with Datarax.\n",
    "You'll learn to work with different data modalities, configure advanced options, and\n",
    "build production-ready training pipelines.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. Load different dataset types (images, text, audio)\n",
    "2. Configure field filtering with include/exclude keys\n",
    "3. Set up shuffling with proper buffer configuration\n",
    "4. Build complete training pipelines with augmentation\n",
    "5. Handle streaming vs downloaded modes effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bae46",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "# Install datarax with data dependencies\n",
    "uv pip install \"datarax[data]\"\n",
    "```\n",
    "\n",
    "**Note**: Some datasets may require additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bda5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "from datarax import from_source\n",
    "from datarax.dag.nodes import OperatorNode\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.operators.composite_operator import (\n",
    "    CompositeOperatorConfig,\n",
    "    CompositeOperatorModule,\n",
    "    CompositionStrategy,\n",
    ")\n",
    "from datarax.sources import HfDataSourceConfig, HFSource\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f82024",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 1: Understanding HFSource Configuration\n",
    "\n",
    "`HfDataSourceConfig` provides extensive options for loading HuggingFace datasets.\n",
    "\n",
    "### Key Configuration Options\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `name` | Dataset identifier on HF Hub | Required |\n",
    "| `split` | Which split to use | Required |\n",
    "| `streaming` | Stream data on-the-fly | `False` |\n",
    "| `shuffle` | Enable shuffling | `False` |\n",
    "| `shuffle_buffer_size` | Buffer size for shuffling | `1000` |\n",
    "| `include_keys` | Only include these fields | `None` |\n",
    "| `exclude_keys` | Exclude these fields | `None` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b2378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Basic configuration for MNIST\n",
    "basic_config = HfDataSourceConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"train[:1000]\",  # Load first 1000 samples\n",
    "    streaming=False,  # Download full dataset\n",
    ")\n",
    "\n",
    "basic_source = HFSource(basic_config, rngs=nnx.Rngs(0))\n",
    "print(f\"Basic MNIST source: {len(basic_source)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048fb72",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 2: Field Filtering\n",
    "\n",
    "Use `include_keys` or `exclude_keys` to control which fields are returned.\n",
    "This is useful for:\n",
    "- Reducing memory usage\n",
    "- Excluding metadata you don't need\n",
    "- Simplifying downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cdc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only specific fields\n",
    "filtered_config = HfDataSourceConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"train[:500]\",\n",
    "    include_keys={\"image\", \"label\"},  # Only return these fields\n",
    ")\n",
    "\n",
    "filtered_source = HFSource(filtered_config, rngs=nnx.Rngs(1))\n",
    "\n",
    "# Check what fields are available\n",
    "pipeline = from_source(filtered_source, batch_size=1)\n",
    "batch = next(iter(pipeline))\n",
    "data = batch.get_data()\n",
    "\n",
    "print(\"Filtered fields:\")\n",
    "for key in data.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2710ef",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 3: Shuffling Configuration\n",
    "\n",
    "Shuffling is essential for training ML models. HFSource supports:\n",
    "- Buffer-based shuffling for streaming mode\n",
    "- Full shuffle for downloaded datasets\n",
    "- RNG-based reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa73170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure shuffling with custom buffer\n",
    "shuffle_config = HfDataSourceConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"train[:2000]\",\n",
    "    shuffle=True,\n",
    "    shuffle_buffer_size=500,  # Shuffle in chunks of 500\n",
    "    stochastic=True,\n",
    "    stream_name=\"data_shuffle\",\n",
    ")\n",
    "\n",
    "# Create source with explicit RNG for reproducibility\n",
    "shuffle_source = HFSource(\n",
    "    shuffle_config,\n",
    "    rngs=nnx.Rngs(42, data_shuffle=123),  # Named stream for shuffle\n",
    ")\n",
    "\n",
    "print(\"Shuffle configuration:\")\n",
    "print(f\"  Buffer size: {shuffle_config.shuffle_buffer_size}\")\n",
    "print(f\"  Stochastic: {shuffle_config.stochastic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0fe56",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 4: Streaming vs Downloaded Mode\n",
    "\n",
    "### Streaming Mode (`streaming=True`)\n",
    "- Data loaded on-the-fly from HuggingFace servers\n",
    "- No disk storage required\n",
    "- Ideal for large datasets\n",
    "- Cannot seek to specific indices\n",
    "- Dataset length may not be available\n",
    "\n",
    "### Downloaded Mode (`streaming=False`)\n",
    "- Full dataset downloaded and cached locally\n",
    "- Random access to any sample\n",
    "- Faster iteration after initial download\n",
    "- Requires disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d758633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare streaming vs downloaded\n",
    "print(\"Mode Comparison:\")\n",
    "\n",
    "# Streaming mode\n",
    "streaming_config = HfDataSourceConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "streaming_source = HFSource(streaming_config, rngs=nnx.Rngs(0))\n",
    "\n",
    "try:\n",
    "    print(f\"Streaming mode length: {len(streaming_source)}\")\n",
    "except (NotImplementedError, TypeError):\n",
    "    print(\"Streaming mode length: N/A (not available in streaming)\")\n",
    "\n",
    "# Downloaded mode (using subset)\n",
    "downloaded_config = HfDataSourceConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"train[:1000]\",\n",
    "    streaming=False,\n",
    ")\n",
    "downloaded_source = HFSource(downloaded_config, rngs=nnx.Rngs(0))\n",
    "print(f\"Downloaded mode length: {len(downloaded_source)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704b7d8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 5: Building Complete Training Pipeline\n",
    "\n",
    "Combine HFSource with operators for a production-ready pipeline.\n",
    "This example shows:\n",
    "- Data loading from HuggingFace\n",
    "- Normalization operator\n",
    "- Data augmentation (random flip)\n",
    "- Batched iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define operators\n",
    "def normalize_image(element, key=None):  # noqa: ARG001\n",
    "    \"\"\"Normalize image to [0, 1] and ensure proper shape.\"\"\"\n",
    "    del key  # Unused - deterministic\n",
    "    image = element.data.get(\"image\")\n",
    "    if image is not None and hasattr(image, \"dtype\"):\n",
    "        # Normalize to [0, 1]\n",
    "        normalized = image.astype(jnp.float32) / 255.0\n",
    "        # Add channel dimension if needed (for grayscale)\n",
    "        if normalized.ndim == 2:\n",
    "            normalized = normalized[..., None]\n",
    "        return element.update_data({\"image\": normalized})\n",
    "    return element\n",
    "\n",
    "\n",
    "def random_flip(element, key):\n",
    "    \"\"\"Randomly flip image horizontally.\"\"\"\n",
    "    flip_key, _ = jax.random.split(key)\n",
    "    should_flip = jax.random.bernoulli(flip_key, 0.5)\n",
    "\n",
    "    image = element.data.get(\"image\")\n",
    "    if image is not None:\n",
    "        flipped = jax.lax.cond(\n",
    "            should_flip,\n",
    "            lambda x: jnp.flip(x, axis=1),  # Flip width axis\n",
    "            lambda x: x,\n",
    "            image,\n",
    "        )\n",
    "        return element.update_data({\"image\": flipped})\n",
    "    return element\n",
    "\n",
    "\n",
    "# Create operators\n",
    "normalizer = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False),\n",
    "    fn=normalize_image,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "\n",
    "flipper = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=True, stream_name=\"flip\"),\n",
    "    fn=random_flip,\n",
    "    rngs=nnx.Rngs(flip=42),\n",
    ")\n",
    "\n",
    "# Create composite augmentation\n",
    "augmentation = CompositeOperatorModule(\n",
    "    CompositeOperatorConfig(\n",
    "        strategy=CompositionStrategy.SEQUENTIAL,\n",
    "        operators=[normalizer, flipper],\n",
    "        stochastic=True,\n",
    "        stream_name=\"augment\",\n",
    "    ),\n",
    "    rngs=nnx.Rngs(augment=999),\n",
    ")\n",
    "\n",
    "print(\"Created operators: normalizer, flipper, augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the complete pipeline\n",
    "train_config = HfDataSourceConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"train[:5000]\",\n",
    "    shuffle=True,\n",
    "    shuffle_buffer_size=1000,\n",
    "    include_keys={\"image\", \"label\"},\n",
    ")\n",
    "\n",
    "train_source = HFSource(train_config, rngs=nnx.Rngs(0))\n",
    "\n",
    "# Chain: Source -> Augmentation -> Output\n",
    "training_pipeline = from_source(train_source, batch_size=64).add(OperatorNode(augmentation))\n",
    "\n",
    "print(\"Training pipeline:\")\n",
    "print(\"  HFSource(mnist) -> Normalize -> RandomFlip -> Output\")\n",
    "print(\"  Batch size: 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f9e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data\n",
    "print(\"\\nProcessing training batches:\")\n",
    "stats = {\"batches\": 0, \"samples\": 0}\n",
    "\n",
    "for i, batch in enumerate(training_pipeline):\n",
    "    if i >= 5:  # Process 5 batches for demo\n",
    "        break\n",
    "\n",
    "    image_batch = batch[\"image\"]\n",
    "    label_batch = batch[\"label\"]\n",
    "\n",
    "    stats[\"batches\"] += 1\n",
    "    stats[\"samples\"] += image_batch.shape[0]\n",
    "\n",
    "    if i == 0:  # Print details for first batch\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  Image: shape={image_batch.shape}, dtype={image_batch.dtype}\")\n",
    "        img_min, img_max = float(image_batch.min()), float(image_batch.max())\n",
    "        print(f\"  Image range: [{img_min:.3f}, {img_max:.3f}]\")\n",
    "        print(f\"  Label: shape={label_batch.shape}\")\n",
    "\n",
    "print(f\"\\nProcessed {stats['batches']} batches, {stats['samples']} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6b080",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Part 6: Working with Different Datasets\n",
    "\n",
    "HuggingFace Hub hosts thousands of datasets across different modalities.\n",
    "\n",
    "### Common Dataset Examples\n",
    "\n",
    "| Dataset | Type | Example Config |\n",
    "|---------|------|----------------|\n",
    "| `mnist` | Image | `split=\"train\"` |\n",
    "| `cifar10` | Image | `split=\"train\"` |\n",
    "| `imdb` | Text | `split=\"train\"` |\n",
    "| `squad` | QA | `split=\"train\"` |\n",
    "| `librispeech_asr` | Audio | `split=\"train.clean.100\"` |\n",
    "\n",
    "### Dataset Discovery\n",
    "\n",
    "```python\n",
    "# List available datasets\n",
    "from datasets import list_datasets\n",
    "datasets = list_datasets()\n",
    "\n",
    "# Get dataset info\n",
    "from datasets import load_dataset_builder\n",
    "builder = load_dataset_builder(\"mnist\")\n",
    "print(builder.info)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55909654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Different split syntax\n",
    "print(\"Split syntax examples:\")\n",
    "print(\"  'train' - Full training set\")\n",
    "print(\"  'train[:1000]' - First 1000 samples\")\n",
    "print(\"  'train[1000:2000]' - Samples 1000-2000\")\n",
    "print(\"  'train[:10%]' - First 10% of data\")\n",
    "print(\"  'train+test' - Combined splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49291e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Feature | Configuration |\n",
    "|---------|--------------|\n",
    "| Field Filtering | `include_keys` / `exclude_keys` |\n",
    "| Shuffling | `shuffle=True`, `shuffle_buffer_size=N` |\n",
    "| Streaming | `streaming=True` for large datasets |\n",
    "| Reproducibility | Named RNG streams |\n",
    "| Pipeline | Source -> Operators -> Output |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Large datasets**: Use `streaming=True` to avoid memory issues\n",
    "2. **Training**: Always enable shuffling with appropriate buffer size\n",
    "3. **Reproducibility**: Use named RNG streams (`nnx.Rngs(name=seed)`)\n",
    "4. **Memory**: Use `include_keys` to filter unnecessary fields\n",
    "5. **Development**: Use split syntax like `train[:1000]` for quick iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee42d31",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Image augmentation**: See [Operators Tutorial](../../core/03_operators_tutorial.ipynb)\n",
    "- **TFDS alternative**: [TFDS Integration](../tfds/01_tfds_quickref.ipynb)\n",
    "- **Distributed training**: [Sharding Guide](../../advanced/distributed/01_sharding_quickref.ipynb)\n",
    "- **HuggingFace Hub**: Browse datasets at https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the HuggingFace tutorial.\"\"\"\n",
    "    print(\"HuggingFace Datasets Tutorial\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Create pipeline\n",
    "    config = HfDataSourceConfig(\n",
    "        name=\"mnist\",\n",
    "        split=\"train[:2000]\",\n",
    "        shuffle=True,\n",
    "        include_keys={\"image\", \"label\"},\n",
    "    )\n",
    "    source = HFSource(config, rngs=nnx.Rngs(0))\n",
    "\n",
    "    # Normalizer\n",
    "    def normalize(element, key=None):  # noqa: ARG001\n",
    "        del key\n",
    "        image = element.data.get(\"image\")\n",
    "        if image is not None:\n",
    "            normalized = image.astype(jnp.float32) / 255.0\n",
    "            if normalized.ndim == 2:\n",
    "                normalized = normalized[..., None]\n",
    "            return element.update_data({\"image\": normalized})\n",
    "        return element\n",
    "\n",
    "    normalizer = ElementOperator(\n",
    "        ElementOperatorConfig(stochastic=False),\n",
    "        fn=normalize,\n",
    "        rngs=nnx.Rngs(0),\n",
    "    )\n",
    "\n",
    "    pipeline = from_source(source, batch_size=64).add(OperatorNode(normalizer))\n",
    "\n",
    "    # Process\n",
    "    total = 0\n",
    "    for batch in pipeline:\n",
    "        total += batch[\"image\"].shape[0]\n",
    "        # Verify normalization\n",
    "        assert batch[\"image\"].min() >= 0.0\n",
    "        assert batch[\"image\"].max() <= 1.0\n",
    "\n",
    "    print(f\"Processed {total} samples\")\n",
    "    print(\"Tutorial completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
