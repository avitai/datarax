{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcdf78a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Sharded Pipeline Quick Reference\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Intermediate |\n",
    "| **Runtime** | ~5 min |\n",
    "| **Prerequisites** | Basic Datarax pipeline, JAX sharding concepts |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Distribute data processing across multiple JAX devices using Datarax sharding.\n",
    "This enables efficient utilization of multi-GPU setups for large-scale data\n",
    "pipelines, essential for training on large datasets.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this example, you will be able to:\n",
    "\n",
    "1. Create a JAX device mesh for multi-device execution\n",
    "2. Configure Datarax pipelines for sharded data distribution\n",
    "3. Verify data is properly distributed across devices\n",
    "4. Handle single-device fallback gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48802ce9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "# Install datarax\n",
    "uv pip install datarax\n",
    "```\n",
    "\n",
    "**Note**: Multi-GPU sharding requires at least 2 JAX devices.\n",
    "Single-device systems will run in fallback mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127bd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import jax\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "\n",
    "from datarax import from_source\n",
    "from datarax.dag.nodes import OperatorNode\n",
    "from datarax.operators import ElementOperator, ElementOperatorConfig\n",
    "from datarax.sources import MemorySource, MemorySourceConfig\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Device count: {len(jax.devices())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9b08a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Check Device Availability\n",
    "\n",
    "Sharding requires multiple JAX devices. The example gracefully\n",
    "handles single-device environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device availability\n",
    "devices = jax.devices()\n",
    "use_sharding = len(devices) >= 2\n",
    "\n",
    "if use_sharding:\n",
    "    print(f\"Multi-device mode: {len(devices)} devices available\")\n",
    "    print(f\"Devices: {[str(d) for d in devices]}\")\n",
    "else:\n",
    "    print(f\"Single-device mode: Only {len(devices)} device(s) found\")\n",
    "    print(\"Sharding demo will show concepts without actual distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fb1c9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Create Data and Pipeline\n",
    "\n",
    "Standard pipeline setup - the sharding is applied at the mesh level,\n",
    "not changing how you define sources or operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ef825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "num_samples = 1024\n",
    "data = {\n",
    "    \"image\": np.random.rand(num_samples, 32, 32, 3).astype(np.float32),\n",
    "    \"feature\": np.random.rand(num_samples, 128).astype(np.float32),\n",
    "    \"label\": np.random.randint(0, 10, (num_samples,)).astype(np.int32),\n",
    "}\n",
    "\n",
    "# Create source\n",
    "source_config = MemorySourceConfig()\n",
    "source = MemorySource(source_config, data=data, rngs=nnx.Rngs(0))\n",
    "\n",
    "print(f\"Data samples: {num_samples}\")\n",
    "print(f\"Image shape per sample: {data['image'].shape[1:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f148cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define normalization operator\n",
    "def normalize(element, key=None):\n",
    "    \"\"\"Normalize image to [0, 1] range.\"\"\"\n",
    "    return element.update_data({\"image\": element.data[\"image\"] / 255.0})\n",
    "\n",
    "\n",
    "normalizer = ElementOperator(\n",
    "    ElementOperatorConfig(stochastic=False), fn=normalize, rngs=nnx.Rngs(0)\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = from_source(source, batch_size=128).add(OperatorNode(normalizer))\n",
    "\n",
    "print(\"Pipeline created with batch_size=128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494f13c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Device Mesh Setup\n",
    "\n",
    "A JAX `Mesh` defines how devices are organized. Common patterns:\n",
    "- `(\"data\",)` - Data parallelism across all devices\n",
    "- `(\"data\", \"model\")` - 2D mesh for data + model parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b213da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create device mesh\n",
    "if use_sharding:\n",
    "    # Reshape devices into a mesh\n",
    "    # For data parallelism: all devices along \"data\" axis\n",
    "    device_mesh = np.array(devices).reshape(-1)\n",
    "    mesh = Mesh(device_mesh, axis_names=(\"data\",))\n",
    "    print(f\"Created mesh with {len(device_mesh)} devices along 'data' axis\")\n",
    "\n",
    "    # Define partition spec for batched data\n",
    "    # batch dimension sharded across \"data\" axis, others replicated\n",
    "    data_sharding = NamedSharding(mesh, PartitionSpec(\"data\", None, None, None))\n",
    "    label_sharding = NamedSharding(mesh, PartitionSpec(\"data\"))\n",
    "else:\n",
    "    mesh = None\n",
    "    print(\"Skipping mesh creation (single device)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5878fdc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Process with Sharding\n",
    "\n",
    "When running inside a mesh context, JAX operations automatically\n",
    "use the sharded execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process batches\n",
    "print(\"\\nProcessing batches:\")\n",
    "\n",
    "if use_sharding and mesh is not None:\n",
    "    with mesh:\n",
    "        for i, batch in enumerate(pipeline):\n",
    "            if i >= 2:\n",
    "                break\n",
    "\n",
    "            # Apply sharding to batch data\n",
    "            image_batch = jax.device_put(batch[\"image\"], data_sharding)\n",
    "            label_batch = jax.device_put(batch[\"label\"], label_sharding)\n",
    "\n",
    "            print(f\"Batch {i}:\")\n",
    "            print(f\"  Image shape: {image_batch.shape}\")\n",
    "            print(f\"  Image sharding: {image_batch.sharding}\")\n",
    "            print(f\"  Label shape: {label_batch.shape}\")\n",
    "else:\n",
    "    # Single device fallback\n",
    "    for i, batch in enumerate(pipeline):\n",
    "        if i >= 2:\n",
    "            break\n",
    "\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  Image shape: {batch['image'].shape}\")\n",
    "        print(f\"  Label shape: {batch['label'].shape}\")\n",
    "        print(\"  (Running on single device)\")\n",
    "\n",
    "# Expected output (multi-GPU):\n",
    "# Batch 0:\n",
    "#   Image shape: (128, 32, 32, 3)\n",
    "#   Image sharding: NamedSharding(mesh=..., spec=PartitionSpec('data',))\n",
    "#   Label shape: (128,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244664c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Feature | Value |\n",
    "|---------|-------|\n",
    "| Device Count | Depends on system |\n",
    "| Mesh Shape | (N,) for N devices |\n",
    "| Data Parallelism | Batch dimension sharded |\n",
    "| Fallback | Single-device execution |\n",
    "\n",
    "Sharding benefits:\n",
    "- **Memory efficiency**: Data distributed across device memories\n",
    "- **Throughput**: Parallel preprocessing on multiple devices\n",
    "- **Scalability**: Easily scales with more devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0920eb1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Advanced sharding**: Explore model parallelism for large models\n",
    "- **TPU sharding**: Configure meshes for TPU pod slices\n",
    "- **Pipeline parallelism**: Overlap data loading and computation\n",
    "- **Checkpointing**: [Checkpointing](../checkpointing/01_checkpoint_quickref.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the sharded pipeline example.\"\"\"\n",
    "    print(\"Sharded Pipeline Example\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Check devices\n",
    "    devices = jax.devices()\n",
    "    use_sharding = len(devices) >= 2\n",
    "    print(f\"Devices: {len(devices)}, Sharding: {use_sharding}\")\n",
    "\n",
    "    # Create data and pipeline\n",
    "    num_samples = 1024\n",
    "    data = {\n",
    "        \"image\": np.random.rand(num_samples, 32, 32, 3).astype(np.float32),\n",
    "        \"feature\": np.random.rand(num_samples, 128).astype(np.float32),\n",
    "        \"label\": np.random.randint(0, 10, (num_samples,)).astype(np.int32),\n",
    "    }\n",
    "\n",
    "    source = MemorySource(MemorySourceConfig(), data=data, rngs=nnx.Rngs(0))\n",
    "    normalizer = ElementOperator(\n",
    "        ElementOperatorConfig(stochastic=False), fn=normalize, rngs=nnx.Rngs(0)\n",
    "    )\n",
    "    pipeline = from_source(source, batch_size=128).add(OperatorNode(normalizer))\n",
    "\n",
    "    # Process batches\n",
    "    total_samples = 0\n",
    "    for i, batch in enumerate(pipeline):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        total_samples += batch[\"image\"].shape[0]\n",
    "\n",
    "    print(f\"Processed {total_samples} samples\")\n",
    "    print(\"Example completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
